[["index.html", "Are you ready for R? A Workbook for R for Political Science and Beyond Preface", " Are you ready for R? A Workbook for R for Political Science and Beyond Michael Strausz 2023-04-19 Preface I decided to learn R in late 2020 for a variety of reasons, including the reasons that I lay out in section 1.1: R is open source, free, and powerful. Early on in my studies, I decided that going forward I would also teach my introductory undergraduate course Scope and Methods of Political Science using R instead of SPSS (which I had used in the past), for those same reasons  it would be great for my students to get some experience with this powerful piece of software. As I reviewed the available workbooks, I did not find one that I was satisfied with. Carly Potz-Nielsen has written a terrific workbook with a colleague of hers which she shared with me. I considered using that, but ultimately, I decided to write my own workbook to fit the way that I have come to teach Scope and Methods. I also wanted to write an R workbook that uses the tidyverse package, because I think that is a great way to use R. I decided to make my workbook available for free online because that is consistent with the open source spirit of R. This workbook might be helpful to you if you are trying to teach yourself R, and you are also welcome to use it to teach students R in non-profit high schools and non-profit collegiate settings. Those wishing to use it for commercial reasons outside of non-profit educational uses should contact me for permission. Id like to thank my spouse Kate for coming up with the great title and for putting up with my (admittedly annoying) regular, enthusiastic discussion of the new things that I had learned to do and make in R over these last few years. "],["getting-started.html", "Chapter 1 Getting started with R 1.1 Why R? 1.2 Create a project for this class 1.3 Using the Console 1.4 Writing your first R script 1.5 What do to if the load command doesnt load the dataframe 1.6 Review of this chapters commands 1.7 Review exercises", " Chapter 1 Getting started with R 1.1 Why R? R is a powerful, customizable, open source piece of software that is extremely useful for political science analysis (among many, many other uses). R has a steep learning curve (in other words, it is hard, especially at first), but your instructor is here to help you, and we will all be learning this together. Here are some of the biggest advantages of R: R is free! In the past, I have taught this class with SPSS. As of this writing, the professional version of SPSS costs $99 per month for a subscription. Because R is open source, users are regularly writing custom programs to improve it, which they tend to share for free. Because R is open source, there are many, many great resources available for free online, and there is a thriving community of people that ask and answer questions about how to use R. R is more powerful than SPSS. It can conduct statistical analyses and generate graphics that SPSS is not capable of (including maps with GIS data). R is professional-grade software, extremely commonly used by professional analysts. Familiarity with R will look great on your resume. It will show potential employers that you have some serious data skills and are capable of learning to code. Learning R will help you learn to code in other pieces of software as well, which will be an important skill in the future. In this course, I have no expectation that you have any background in coding or in statistical analysis. So, if you have no such experience, please do not worry! You are not alone. I would be surprised if more than a few of you have experience with coding or statistical analysis, and I will teach the class as if none of you do. If some parts of the class are review for you, that wont hurt you either! 1.2 Create a project for this class Follow the following instructions to get your computer set up for this class. First, you will have to download and install two pieces of software on your computer. NOTE: You can install these programs on a Mac or a PC, but not on an iPad or a tablet that is not running Windows or the Mac OS. You can find the first piece of software, R, here: https://www.r-project.org/. Be sure to download a version of R that starts with a 4. (like 4.1.3 or 4.0.5). You can find the second piece of software, RStudio, here: https://www.rstudio.com/products/rstudio/. Be sure to download RStudio Desktop open source edition. Create a folder somewhere on your computer that you can easily find. If no place comes to mind use the Desktop. Title the folder Scope and Methods Labs. (or another title that you would prefer). Inside that folder, make three other folders. Call one Data, one Codebooks, and one R Scripts. Open RStudio on your computer. Click on the File menu at the top, and then select New Project. Select Existing Directory on the menu that comes up. Navigate to the main folder that you just created (which I asked you to call Scope and Methods Labs), and click Open. Select Create Project. You should see something like this on your screen: There is a lot on this screen! Please dont be intimidated. For now, just make sure that you see something like the circled portion on your screen. Go to this website. There, you will see three folders whose names match the three folders that you created earlier. Download the contents of each folder and move it to the corresponding folder that you made earlier. So, the Codebooks folder that you made should now have three files in it: anes2020 codebook.pdf, states2010 codebook.pdf, and world codebook.pdf. The Data file that you made should also have three files in it, and the R Scripts file that you made should have one file in it. Congratulations! You have set up R for this workbook (and for other uses too!) 1.3 Using the Console In order to use R we must use commands to tell R what to do. One way you can write commands is in the Console, here: At its most basic, you might think of R as a very, very fancy calculator. So, for example, I can type 45*21 in the Console (after the &gt;) and hit enter, and I have done my first R operation: What if we want to R remember that value? Then we can use this notation &lt;- to assign that value to an object. You will soon see that programming in R often involves creating objects, modifying objects, and analyzing objects. There are a few rules about object names. 1.3.1 Rules about naming objects Object names cannot begin with a number. Object names cannot have a space in them. R notices whether object names use capital or lower-case letters, so be careful with this. To avoid errors, copy and paste object names rather than typing them out. 1.3.2 Common practices when naming objects Common practices in naming objects are to use a . between words, a _ between words, or to capitalize each word after the first. So either: an.object, an_object, or anObject. You can choose which style you prefer. Lets make our first object, using this command (you can copy and paste this command into your Console): my.first.object&lt;-45*21 When you copy and paste that code into your Console, you should see something like this: Notice what happened when we created this object. R did not give us the answer to our math problem in the Console, but instead put the object that we created in the Environment pane. This kind of object, a single value, is called a scalar. You wont deal with these very frequently. 1.3.3 About vectors A more commonly used kind of object is a vector  a list of two or more values. What if I want to create a vector that includes the my.first.object scalar and another number? One way that I can build a vector is with the c() function. The c here stands for concatenate, which means bring together. Here is how I would use c() to build such a vector: my.second.object&lt;-c(my.first.object, 2) If you enter that into the Console, you will see in the Environment pane there are now two objects in your Environment: my.first object (a scalar with a value of 945) and my.second.object (a vector with two values: 945 and 2). But you actually dont need to remember these objects, right? No need to have it gunking up your Environment. So you can use the rm() command to remove them (rm() means remove). Just type this into the Console, and you will see the first object, the scalar, disappear: rm(my.first.object) hen type this into the Console and you will see the second object, the vector, disappear: rm(my.second.object) 1.4 Writing your first R script You will often want to write a series of commands in R that you will be able to refer back to and modify. In that case, instead of writing them in the Console, you can write them in a script file. Lets write our first R script! Go to the File menu at the top of your screen, select New File and then select R Script. Before starting to write it, save it in the R scripts section of the folders that you made (click File then Save As and then give it a name, like my first script). One thing to know about R is that R ignores hashtags (like #). Once R reads a hashtag, it stops reading the rest of that line. So, we can use # to insert our comments to remind us what we are doing with specific chunks of code. Try entering something like this at the top of your page: #This is my first R script Now we are going to enter a command. We are going to use the load() command to load one of the datasets that we will be using in this class. Now type: load(&quot;Data/anes2020.Rda&quot;) #This is how I am loading some data that I will use later. Be sure to include the  around the name and location of the dataset. The comment is not necessary, but it is there for your reference. So, at this point you should have two lines of script. To run this script, you must first select it, by highlighting it with your cursor. Then, you can either click the little run button circled below or you can hit Cntl-Enter in a PC or Command-Enter on a Mac. Once you run this script, you should see a new object in your environment: anes2020 (if that didnt work for you, see section 1.5. This is a dataframe  a collection of vectors that are combined to give us a lot of interesting information. In this case, this dataframe tells us about a survey of residents of the United States from 2020. We will look more at this in future weeks. Save your R script, and be sure that you save changes to this project. 1.5 What do to if the load command doesnt load the dataframe Click on the folder called Data in the bottom right of your screen. That should open up a folder with the three dataframes that you downloaded earlier. Click on anes2020. You will get a warning message like this: Click Yes, and anes2020 should appear in your environment. 1.6 Review of this chapters commands Command Purpose &lt;- To create an object. For example: this.object&lt;-1 assigns the number 1 to an object called this.object. c() Concatenate. To combine more than one data point into a vector. rm() Remove an object from your environment. # Tells R to stop reading that line, so a useful prefix to comment and reminders. load() Load an R-formatted dataframe. 1.7 Review exercises Lets practice some of the things that we learned in this chapter. You can number each of your answers with comments (beginning with the # sign). Create a new R script called Chapter 1 Exercises, and save it to the R scripts folder that you created earlier. Make the first line in the script a comment (using #) that includes your name and the phrase Chapter 1 Exercises. In 2021, TCU had 10,222 undergraduate students and 1,716 graduate students. Generate a scalar called tcu.students that is the total population of TCU students. NOTE: leave the commas out of the numbers when entering them into R. In 2020 SMU had 6,827 undergraduate students and 5,546 graduate students. Generate a scalar called smu.students that is the total population of SMU students. Use the c() command to generate a vector called dfw.area.students. This vector should have two values: the total number of students at TCU, and the total number of students at SMU. Remove all three of these objects you created using three different rm() commands. Use the load() command to load both states2010 (which you can find at Data/states2010.Rda) and world (which you can find at Data/world.Rda). If you didnt already load anes2020 when following along with this chapter, use the load() command to load that too). If the load command isnt working, follow the instructions at the end of section 1.5 to load all three dataframes. You will need these for future labs. "],["tidyverse.html", "Chapter 2 Welcome to the tidyverse 2.1 Installing packages 2.2 Exploring dataframes 2.3 Organizing your scripts 2.4 Welcome to the tidyverse 2.5 Review of this chapters commands 2.6 Review exercises", " Chapter 2 Welcome to the tidyverse Here is a review of existing methods. 2.1 Installing packages One of the nice things about R is that users can create their own libraries of commands. These can be extremely powerful and useful tools. There is an excellent set of tools that are in a library called the tidyverse. To install the tidyverse on your local computer, first, make sure that your computer is attached to the internet. Second, enter this command into the Console of RStudio and press Enter: install.packages(&quot;tidyverse&quot;) Once you hit enter, you should see some action in your Console (what it says will depend on what version of R you are using). Some of it might be in red, but dont worry about it. Now you have installed the tidyverse set of libraries of commands! We will regularly work with the tools that this library makes possible for us. 2.2 Exploring dataframes To begin exploring dataframes, open the project that you made in the last lab. There are two ways that you can do this: You can open RStudio, go to File, open project, and navigate to the file called Scope and Methods labs. You can navigate to the folders that you made last lab and double click the file called Scope and Methods Labs, like the circled file below: When the project opens up, you should see three objects in the Environment: anes2020, states2010, and world. If you dont see those files, follow the instructions in section 1.5, to load them. Those files are the dataframes that we will be doing most of our analysis of in this class. As I mentioned in the last chapter, a dataframe is essentially a group of vectors attached to one another, and is an extremely useful way of storing data for analysis. Here is a quick summary of those three dataframes: anes2020 is a snippet of the American National Election Survey, a survey that has been conducted at every election since 1948 by scholars based out of the University of Michigan and Stanford. This snippet that I created only looks at the 2020 election. You can find more information about ANES at their website, https://electionstudies.org/ states2010 is a dataframe with information about US states. It is a snippet of a large dataset produced by scholars at Michigan State University. You can find more information here: http://ippsr.msu.edu/public-policy/correlates-state-policy world is a snippet of a dataset about most of the countries of the world gathered from different sources by the great Harvard political scientist Pippa Norris. You can find the whole dataset here: https://www.pippanorris.com/data To begin, lets take a look at the anes2020 dataset using the View() command (note that View is capitalized, unlike many R commands). Enter this command into the Console and then hit Enter: View(anes2020) When you hit enter, the following should come up on your screen: This is a neat visualization of the ANES survey. Each row represents one case. Since ANES is a survey, each case is a single individual who took the survey. Each column represents a variable: something that the survey designers wanted to measure about the people taking the survey. Look at row #1. That describes a person who is 46 years old, divorced, with a bachelors degree, who never served in the military, etc. If you want to see what all of the variables mean, you can look at the file anes2020 codebook, which you should have downloaded in chapter 1.2 and put in your Codebooks folder. Now lets say that we want to know a bit more about the age variable. How old was the average person that took our survey? To answer, we need to learn two more R commands: the $ and the command summary. The $ command helps us identify single columns within a dataframe. So, if we just want to see the age column, you can type this in the Console: anes2020$age When you do this, you will see that R gives you the age of every single person who took the survey. Kind of interesting, but not all that useful. So, lets try something else. Type summary(anes2020$age) into the Console, and you get these results: summary(anes2020$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 18.00 37.00 52.00 51.59 66.00 80.00 348 This gives you a lot of useful information! You can see that the youngest age in the dataframe is 18 (likely because they were not surveying minors), and the oldest is 80. If you look in the codebook, you will see that the survey takers coded everyone 80 and over as 80. You will also see that the median age is 52 and the mean is 51.59. And, you will see that there are 348 NAs. NA is what R reports when it does not have data in a particular variable. So, there are 348 cases for whom we dont have data on their ages. The summary command is very useful with numeric data, like age. However, what if you have a variable like marital, which measures the marital status of individuals? The mean and median of this variable would not be meaningful, but we might want to know how many married, single, divorced, etc. people there are in our dataframe. To get this information, we can use the table command, like this: table(anes2020$marital) That command generates this output ## ## divorced married never married separated widowed ## 1221 4322 1951 163 567 In other words, our dataframe has 1221 divorced people, 4322 married people, 1951 never married people, etc. Useful stuff! 2.3 Organizing your scripts As you use R you will generate a lot of script files, and if you are not careful you might find yourself forgetting what some of your scripts are for. This is where the # characters can be really helpful. You can leave notes to yourself. This is a nice way to add headers to R script files: #################################### # Your name # Project, Purpose of R file # Date started : Date last modified #################################### Then, when you open a script file, you can see right away when you made it and what it is for. You can customize this header as is helpful to you. 2.4 Welcome to the tidyverse Perhaps the code package that has had the biggest impact on how people do R in the last few years had been the tidyverse. To take advantage of this great library which you installed in section 2.1, you first have to activate it. You will have to do this step every time that you open R, so I often put it at the top of my script files. To follow along, open a new script file and save it in your scripts folder as chapter 2 practice. Copy and paste this onto the page: #################################### # Your name # 20093 Chapter 2, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) Now select all the text on this page, run it, and then save it. You should see something like this in your Console: Warning: package &#39;tidyverse&#39; was built under R version 4.1.2 v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.3 v dplyr 1.0.7 v tidyr 1.1.3 v stringr 1.4.0 v readr 2.0.0 v forcats 0.5.1 Dont worry about the error messages. The list of packages are the libraries of commands that come with the tidyverse library. The two that we will be using the most are ggplot2, which is terrific for graphing, and dplyr, which is great for managing data. Since you have installed the tidyverse, you will not have to install those packages separately  you already have them. Now that we have access to the tidyverse, we will use a few of the commands that are available to us through the dplyr package to get a better look at some of our data. Before we can do that, however, we need to learn about the somewhat strange feature of dplyr, the pipe. The pipe is this set of characters: %&gt;% You can always type those three characters to build a pipe, but you can also type Ctrl-Shift-M on a Windows machine or Cmd + Shift + M on a Mac. The pipe means and then. Lets try an example, using the glimpse() command that is available in the tidyverse. Try entering this command into your script file: anes2020 %&gt;% glimpse() #taking a different kind of look at our dataframe Highlight that text and run that command. You should see a lot of output in the Console. If you scroll up a bit in the Console, you should see something like this (this is just the first 12 variables in this dataframe; you will have access to more on your screen): ## Rows: 8,280 ## Columns: 12 ## $ age &lt;int&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55, 30, 38, 41, 66, 54, 55, 62, 80, 31, 80, 24, 55, 59, 73, 72,~ ## $ marital &lt;chr&gt; &quot;divorced&quot;, &quot;married&quot;, &quot;married&quot;, &quot;married&quot;, &quot;widowed&quot;, &quot;divorced&quot;, &quot;married&quot;, &quot;married&quot;, &quot;married&quot;, &quot;separ~ ## $ edu &lt;ord&gt; &quot;ba&quot;, &quot;some college&quot;, &quot;high school&quot;, &quot;associate degree&quot;, &quot;md, phd, or jd&quot;, &quot;some college&quot;, &quot;associate degre~ ## $ military &lt;chr&gt; &quot;never served&quot;, &quot;never served&quot;, &quot;never served&quot;, &quot;never served&quot;, &quot;never served&quot;, &quot;never served&quot;, &quot;never serv~ ## $ union &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,~ ## $ race &lt;chr&gt; &quot;Hispanic&quot;, &quot;Asian or Pacific Islander&quot;, &quot;White&quot;, &quot;Asian or Pacific Islander&quot;, &quot;Native American&quot;, &quot;White&quot;, ~ ## $ children &lt;int&gt; 0, 1, 2, 1, 0, 0, 3, 0, 0, 0, 2, 2, 1, 2, 1, 0, 2, 2, 0, 0, 0, 1, 0, 3, 0, 0, 3, 0, 0, 2, 2, 2, 0, 0, 0, 0,~ ## $ sex &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,~ ## $ lgbtq &lt;chr&gt; &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;,~ ## $ income &lt;ord&gt; &quot;$110,000 or more&quot;, &quot;$65,000-$109,999&quot;, &quot;$65,000-$109,999&quot;, &quot;$25,000-64,999&quot;, &quot;$110,000 or more&quot;, &quot;under $2~ ## $ guns &lt;int&gt; 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 0, 4, 1, 0, 4, 1, 0, 0, 0, 0, 0, 4,~ ## $ partyid &lt;ord&gt; strong republican, independent, independent democrat, not very strong republican, independent, independent ~ This is giving us information about the dataframe anes2020. At the top, you can see that the dataframe has 8,280 rows. Since this is a survey, and each row represents one person that was surveyed, that tells us that the anes2020 dataframe represents a survey of 8,280 people. On the output above, you can see that the dataframe has 12 columns (because I told R to only display the first 12.) If you ran the command, you should see that the full dataframe has 68 columns, which means that the dataframe measures 68 variables. The glimpse() command in the tidyverse gives us a bit of information about each of these variables. On the left of the output you can see all of the variable names (age, marital, etc.). And then just to the right of that you can see the way that R is classifying those variables (, , , etc.). We will learn a bit more about those classifications in future chapters To the right of those classifications you can see the first set of values of each variable. Many dataframes are quite large, and you might find yourselves wanting to focus on a few variables at a time. The command select() can be helpful for this. Lets say that you want to focus in on how many guns people have in their homes and their feelings about the NRA. In that case, you would want to select the variables ft_nra and guns, which you can do like this: anes2020 %&gt;% select(guns, ft_nra) #focusing in on a few variables 2.4.1 A warning about the select() command Unfortunately, some of the packages that we will be installing in future labs also use the command select. If you ever run select and have more than one of these packages installed, R will give an error. When you get that error, you can add dplyr:: before select(), to tell R to look at the select() in the dplyr library (which comes with the tidyverse). Here is how the above command would look with that modification: anes2020 %&gt;% dplyr::select(guns, ft_nra) #focusing in on a few variables If you run either of these commands (with or without the dplyr::), and scroll up a bit in the Console, you will see output that begins like this: ## guns ft_nra ## 1 3 50 ## 2 0 50 ## 3 0 0 ## 4 0 NA ## 5 0 70 ## 6 0 15 ## 7 0 85 ## 8 0 50 ## 9 1 NA ## 10 2 50 This is a way of displaying dataframes that is unique to the tidyverse called a tibble. Each row represents a case, and each column represents a variable (except for the leftmost column, which is just the case number. Looking at case #1, you can see that that person owns 3 guns, and rates the NRA 50 out of 100. Case #2 also rates the NRA 50, but owns no guns. Notice that the dataframe that had been so large before, with 68 columns, now only has 2 columns. If you want to use that glimpse() command from above to just get summary information about each of our variables, you can use a second pipe (%&gt;%) like this: anes2020 %&gt;% select(guns, ft_nra) #focusing in on a few variables%&gt;% glimpse() #taking a different kind of look at our dataframe What if we want to see what the people in our sample with the most and fewest guns thought about the NRA? To do that, we can arrange our dataframe from smallest to largest or from largest to smallest on any of our variables. Or, with variables that include text, we can sort according to alphabetic or reverse alphabetic order. We can use the arrange() command to do that: anes2020 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(guns) #sorting this variable from smallest to largest When we run the above command, the Console shows us the feeling about the NRA by the people with the fewest guns first, and then on to the people with more guns. But what if we want to see the largest values on the guns variable first? To do that, we need to use desc() to modify arrange() like this: anes2020 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(desc(guns)) #sorting this variable from largest to smallest Finally, combining all of the commands that we learned today, we can use glimpse() to see information about each of our variables in this simplified version of our dataframe. To do that, we will need a third pipe: anes2020 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(desc(guns)) %&gt;% #sorting this variable from largest to smallest glimpse() #taking a different kind of look at our dataframe 2.5 Review of this chapters commands Command Purpose Library install.packages() To install new packages, or libraries of R commands. You have to do this only once on your computer per library when your computer is connected to the internet so that it will download the relevant library. Base R View() Displays a dataframe as a spreadsheet. Notice that the word View is capitalized. Base R summary() Displays summary information (like mean and median) about a numeric variable. To call a variable, use a dollar sign. So, like this: summary(dataframe$variable) Base R table() Displays the number of times each value of a variable appears in a dataframe. Most useful for variables with a relatively small number of possible values. To call a variable, use a dollar sign. So, like this: table(dataframe$variable) Base R library() To ask R to load a library into your session. You must use this command every time that you open R if you want it to use a particular library. Base R %&gt;% The pipe. Useful with manipulating data in the tidyverse. dplry (tidyverse) glimpse() Display general information about a dataset, including looking at the variable names, types, and a few values in rows. Often preceded by a pipe. dplyr (tidyverse) select() Allows you to select a few variables to focus on. Often preceded by a pipe. You can use the variable names without dollar signs. dplyr (tidyverse) arrange(desc()) Sorts a dataframe from largest to smallest values of a particular variable (or in reverse-alphabetical order). Often preceded by a pipe. You can use the variable names without dollar signs. dplyr (tidyverse) 2.6 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 2 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse. Look over the codebooks from our three datasets: anes2020, states2010, and world, and choose a variable that you are interested in. Using either the table() or the summary() command, generate some information about the values of that variable. If you the summary() command does not give you useful information, use the table() command, and vice versa. Make one or more observations about that variable from question 2. Choose a second variable from the same dataframe. Use a pipe (%&gt;%) and the select() command, select only the variable from question 2 and the variable from this question. Use a pipe to modify the command from step 4, and using the arrange() command (with or without desc()), arrange the dataframe by one of the two variables. Make one or more observations about that output from question 5. "],["graphing.html", "Chapter 3 Graphing and describing variables 3.1 Getting started with this chapter 3.2 Getting started with ggplot and graphing 3.3 Central tendancy and dispersion 3.4 A note on using the states2010 and world dataframes 3.5 Review of this chapters commands 3.6 Review exercises", " Chapter 3 Graphing and describing variables 3.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. We are going to install three new packages to use today. One is called epiDisplay and the other is called Hmisc. Enter these two commands into your Console one by one: install.packages(&quot;epiDisplay&quot;) install.packages(&quot;scales&quot;) Now, open a new script file and save it in your scripts folder as chapter 3 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 3, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(epiDisplay) #the tab1 command helps us make nice frequency tables library(scales) #this helps us to put more readable scales on our graphs Now select all the text on this page, run it, and then save it. 3.2 Getting started with ggplot and graphing R has some nice graphic abilities built in, but the ggplot2 package that comes with the tidyverse is even more powerful. In this lab, we will learn to make bar graphs of nominal and ordinal variables and histograms of interval variables. NOTE: You can find templates to reproduce all of the graphs from this workbook in the script file 20093 ggplot2 templates which is among files that you should have downloaded in section 1.2, step 7. If you cant find it, you can download it here. Download it and save it to your R Scripts folder for future use. 3.2.1 Bar graphing a nominal variable A nominal variable is a variable where the distance between the values does not give us any information, and the possible values can be listed in any order without confusing us. R does not have a single way to represent nominal variables, but on the three datasets that I have uploaded for this book I have classified the nominal variables as character variables. R sometimes abbreviates this . If you type this into the console: glimpse(anes2020) you can see that several of the variables in the ANES dataset are classified as characters, including marital, military, and lgbtq. If you ever want to know how R has classified a variable, you can us the class() command. For example, if you type class(anes2020$marital) you can see that R calls the marital variable a character variable. Bar graphs give us a sense of how the cases that we studied are distributed across possible values for a nominal variable. To generate a bar graph using ggplot, you begin with the command ggplot(), and then the first thing you type into the parenthesis is the name of the dataframe that you will be working with. The ggplot package is similar to the dplyr package that uses the pipe (%&gt;%) to mean and then, but instead of a pipe ggplot uses a plus sign. This code will generate a bar graph of that marital variable (if you enter and run the code, you will see the graph that follows it): ggplot(anes2020, aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status of respondents&quot;)+ xlab(&quot;Marital status&quot;) The NA bar represents the number of cases where there is missing data for this question. Often you will not want to include that bar in your graph. To get rid of it, you can use a pipe when you call the dataset and tell R to filter out the cases where marital is NA. Like this: ggplot(anes2020 %&gt;% filter(!is.na(marital), aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status of respondents&quot;)+ xlab(&quot;Marital status&quot;) If you execute that command, you will see that now the NA bar is missing. While Marital status of respondents is not a very long title, sometimes you will have graphs with longer titles that you want r to put on two or more lines. To tell R to move to a new line, just insert \\n where you want the line break to go, like this: ggplot(anes2020 %&gt;% filter(!is.na(marital)), aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status\\nof respondents&quot;)+ xlab(&quot;Marital status&quot;) 3.2.2 Saving a graph To save a graph, you can click the export plot menu right above it: You can either save it as an image, which you will later be able to import into word or other software, or you can copy it to clipboard and immediately paste it into another piece of software. Either of those work. 3.2.3 Bar graphing an ordinal variable An ordinal variable is a variable where the distance between the values does not give us any information, but the possible values must be listed in a particular order to make sense. R is most willing to treat a variable as ordinal when it is listed as an ordered factor, which R sometimes abbreviates as . If you type this into the console: glimpse(anes2020) you can see that several of the variables in the ANES dataset are classified as ordered factors, including edu, partyid, and religimp Reminder: if you ever want to know how R has classified a variable, you can use the class() command. For example, if you type this: class(anes2020$edu) ## [1] &quot;ordered&quot; &quot;factor&quot; you can see from the output R calls the edu variable ordered factor. Partisanship is often measured in an ordinal way, including on the anes2020 dataframe. To generate a bar graph of the partyid variable (an ordinal measure of partisanship), we can use this code: ggplot(anes2020 %&gt;% filter(!is.na(partyid)), aes(x=partyid))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Strength of party identification of respondents&quot;)+ xlab(&quot;Party&quot;) 3.2.4 What to do about overlapping labels If you look at the previous graph, you might notice that the labels at the bottom overlap. This makes it hard to read! To deal with overlapping labels like this, you can add this line to your code (please note that R will not understand this code if you did not install the scales package in section 3.1 and then run the command library(scales)): scale_x_discrete(guide = guide_axis(n.dodge=2))+ This is how our new code looks: ggplot(anes2020 %&gt;% filter(!is.na(partyid)), aes(x=partyid))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Strength of party identification of respondents&quot;)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ xlab(&quot;Party&quot;) 3.2.5 Generating a histogram An interval variable is a variable where the distance between the values gives us useful information and where the possible values must be listed in a particular order to make sense.. In this class, it is generally safe to assume that a variable that is neither an ordered factor nor a character variable is interval. However, variables with only two values (such as 0 and 1) should not be treated as interval. You can, in theory, make a bar graph of an interval variable. However, since there are so many values to most interval variables, those bar graphs become very difficult to read. Instead, we often generate histograms of interval variables. Histograms are bar graphs where each bar represents a range of values of the variable of interest instead of a single bar. For example, here is how we can make a histogram of the population variable in our world dataset: ggplot(world, aes(x = UNDP_pop)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Histogram of countries&#39; populations&quot;)+ xlab(&quot;population&quot;) 3.2.6 Removing scientific notation from axes The above graph is ok, but because the population numbers are so big, R has converted them to scientific notation. To force R not to do this, we can use a command made possible by the scales package that we installed in section 3.1: scale_x_continuous(labels = label_comma()) Combined with the rest of our code, we can generate the graph like this: ggplot(world, aes(x = UNDP_pop)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ scale_x_continuous(labels = label_comma())+ #gets rid of scientific notation ggtitle(&quot;Histogram of countries&#39; populations&quot;)+ xlab(&quot;population&quot;) 3.2.7 Adjusting bin width on a histogram The bars on a histogram are called bins. Looking at this graph, we can see that R has chosen to set the bins at around 50,000,000. What if we wanted to make the bins narrower? We can set the bin width like this: ggplot(world, aes(x = UNDP_pop)) + geom_histogram(binwidth=10000000, fill = &quot;purple&quot;, colour = &quot;black&quot;)+ scale_x_continuous(labels = label_comma())+ ggtitle(&quot;Histogram of countries&#39; populations&quot;)+ xlab(&quot;population&quot;) Those narrower bins let us see more of the variation in the variable  we can see that the biggest group of countries, by far, has a population of 10,000,000 or less. 3.3 Central tendancy and dispersion With all variables, it is helpful to both you and your readers to make observations about their central tendencywhat a typical case looks like, and dispersionhow the actual values are spread out across possible cases. 3.3.1 Central tendency and dispersion of nominal variables With nominal variables, there are two useful techniques to help us discuss central tendancy and dispersion. One is covered above, in section 3.2.1: you can make a bar graph. Second, you can also make a frequency table. To make a frequency table, you can use the table() command to produce a simple one in base R, but the tab1 command that is available through the epiDisplay library that we installed in section 3.1 is more flexible. So, lets try to look at the region variable from the states2010 dataset. Enter the following into todays practice R script file, and run it: #make a frequency table and bar graph of the region variable in states2010 tab1(states2010$region, cum.percent=FALSE) ## states2010$region : ## Frequency Percent ## midwest 12 23.5 ## northeast 9 17.6 ## south 17 33.3 ## west 13 25.5 ## Total 51 100.0 You can see that this command generates two sets of output: a graph that you can see if you click the plots tab in the bottom right, and some information in the Console. The graph is ok, but the graphs that we generated above are nicer, so we will use those instead. The information in the Console tells you that there are 12 states in the dataframe that are classified as being in the midwest, 9 that are classified as being in the northeast, 17 in the south, and 13 in the west. The next column to the right tells you what percent of states are in each category. There is only one meaningful way to measure the central tendency of a nominal variable: the mode. However, R does not have a direct way of measuring the mode. In this case, that is not a problem, as there are only four possible regions in the dataset, and you can clearly see that the largest number of states are in the south. However, sometimes there will be a lot more values than this! In that kind of situation, you can add the qualifier sort.group=decreasing to the tab1 command, like this: #make a frequency table and bar graph of the region variable in states2010 #adding sort.group=&quot;decreasing&quot; to list the values in reverse order of frequency #this is useful for finding the mode tab1(states2010$region, cum.percent=FALSE, sort.group=&quot;decreasing&quot;) ## states2010$region : ## Frequency Percent ## south 17 33.3 ## west 13 25.5 ## midwest 12 23.5 ## northeast 9 17.6 ## Total 51 100.0 You can see from this output that the mode has come to the top. So, between the bar graph and the frequency table, we can observe that the modal state is in the South. 17, or 33.3% of states, are in that region. That is the central tendency of our region variable. Regarding the dispersion of this variable, we can note that no region has a majority of states, and the states seem to be reasonably evenly distributed across regions, with 33.3% of states in the modal region (the south) and 17.6% of states in the least common region (the northeast). 3.3.2 Central tendency and dispersion of ordinal variables With ordinal variables, we can use the same techniques that we use for nominal variables but with one exception: since the order of the possible values does give us important information for ordinal variables, the cumulative percent also gives us useful information. Thus, we can ask R to report the cumulative percent too. Lets try it with the results of the question how important is religion to you? that was asked in the 2020 ANES survey. We can use this command (notice how we are now writing cum.percent=TRUE because we are dealing with an ordinal variable): tab1(anes2020$religimp, cum.percent=TRUE) ## anes2020$religimp : ## Frequency %(NA+) cum.%(NA+) %(NA-) cum.%(NA-) ## extremely important 2277 27.5 27.5 27.6 27.6 ## very important 1585 19.1 46.6 19.2 46.8 ## moderately important 1596 19.3 65.9 19.3 66.2 ## a little important 1066 12.9 78.8 12.9 79.1 ## not important at all 1725 20.8 99.6 20.9 100.0 ## NA&#39;s 31 0.4 100.0 0.0 100.0 ## Total 8280 100.0 100.0 100.0 100.0 R is giving us a lot of information here! The first column on the left is the possible values, ranging from extremely important to not important at all. There are also some NAs  cases for which we do not have data on this question. The second column is how frequently each answer is given. Looking at that column, we can already tell that the modal value is extremely important; 2277 of the 8280 people surveyed answered that religion is extremely important to them. The third column is the percent of cases with each value, including the NAs for which we have no information. The fourth column is the cumulative percent including NAs. This is the percentage of cases that got the value of interest or a lower value. So, 27.5% answered extremely important, 46.6% answered extremely important or very important, 65.9% answered extremely important, very important, moderately important, etc. The fifth and sixth column repeat the third and fourth column, but they exclude the NAs. In general, you should use the columns that exclude NAs when interpreting your data. Regarding the central tendency of this variable, we can again ask R to calculate the mode like we did with our nominal variable, but since this is an ordinal variable, we can also ask R to calculate the median. To calculate the mode, we can add the sort group: decreasing qualifier that we used in section 3.3, and the mode will rise to the top. tab1(anes2020$religimp, cum.percent=TRUE, sort.group = &quot;decreasing&quot;) ## anes2020$religimp : ## Frequency %(NA+) cum.%(NA+) %(NA-) cum.%(NA-) ## extremely important 2277 27.5 27.5 27.6 27.6 ## not important at all 1725 20.8 99.6 20.9 100.0 ## moderately important 1596 19.3 65.9 19.3 66.2 ## very important 1585 19.1 46.6 19.2 46.8 ## a little important 1066 12.9 78.8 12.9 79.1 ## NA&#39;s 31 0.4 100.0 0.0 100.0 ## Total 8280 100.0 100.0 100.0 100.0 Our output tells us what we already know  the extremely important group is the largest, but it is not the majority. R has a median() command that we will be able to use with interval data. However, unfortunately we cant use it with ordinal data. There is a nice trick to finding the median of an ordinal variable, though. Take a look at those cumulative percents in the output that we generated without the sort.group= decreasing qualifier. Focus on the column with NAs excluded  the right-most column. The first value for which the cumulative percent is 50 or higher is the median value of that variable. So, we can say that the median person in our sample says that religion is moderately important to them. So, now we can make two observations about the central tendency of our religion variable  the mode is extremely important and the median is moderately important. What can we say about the dispersion? To discuss the dispersion it is helpful to generate a bar graph. We ca look at the one automatically generated by tab1, but I prefer the look of the ones that we generate with ggplot. So, we can use the same code from section 3.2.3 to generate a bar graph of this variable (Note that we make the x-axis table disappear with NULL because the graph title already explained what the x-axis means): ggplot(anes2020 %&gt;% filter(!is.na(religimp)), aes(x=religimp))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;How important is religion to ANES respondents in 2020?&quot;)+ xlab(NULL) What can we say about the dispersion of this variable? Well, based on the frequency table we can note that there is no value where the majority of cases lie  the cases are reasonably well spread out across possible values, ranging from 27.6 percent at extremely important to 12.9 percent at a little important. We can also note that the distribution of this variable is u-shaped: the two highest values are extremely important and not important at all. 3.3.3 Central tendancy and dispersion of interval variables When we look at interval variables, we can use the mean, median, and mode to discuss the central tendency. R makes it easy to calculate the mean and median, with the command summary() that we learned in section 2.2 (you can also use the commands mean() and median() with the same result). So, for example, lets look at the variable x_tax_burden in the states2010 dataset. This is a states per capita taxes divided by that states per capita income. In other words, it tells us the proportion of their income that the average citizen pays in state taxes in each state. We can use the summary command: summary(states2010$x_tax_burden) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07000 0.08650 0.09400 0.09465 0.10100 0.12800 Here, we see that the mean and median, .09465 and .094, are quite close to each other. This is generally a sign that the variable is not skewed. How about the mode? To generate the mode, we can use the same command that we used for nominal and ordinal variables: tab1(states2010$x_tax_burden, sort.group = &quot;decreasing&quot;) If you enter that command into the Console, you will see a lot of output, but if you scroll to the top you will see that the most common value for the x_tax_burden variable, the mode, is .093; it occurs 5 times. Now we have three measures of the central tendency of our x_tax_burden variable: the mean is 09465, the median is .094, and the mode is .093. How about the dispersion? With interval variables, we can use a number of strategies to represent the dispersion. We can have R calculate the standard deviation and the interquartile range (the 3rd quartile minus the 1st quartile), and we can generate a histogram. To generate the standard deviation, we can use sd(), and to generate the interquartile range, we can use IQR(): sd(states2010$x_tax_burden) ## [1] 0.01221282 IQR(states2010$x_tax_burden) ## [1] 0.0145 We can also use the code from section 3.2.5 to visually display the central tendancy of our variable, like this: ggplot(states2010, aes(x = x_tax_burden)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Histogram of US states&#39; tax burden, 2010&quot;)+ xlab(NULL) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This histogram shows us that this variable is relatively bell-shaped with no outliers that really stand out. 3.3.4 Standard deviation and IQR when you have NAs One issue with the sd and IQR commands is that if there is one case in the dataframe that you are looking at with missing data (i.e. coded as na), then R will give NA as the output. You can see this if you type: sd(anes2020$age) ## [1] NA To address this, just add ,na.rm=TRUE after the variable with the NAs, like this: sd(anes2020$age, na.rm=TRUE) ## [1] 17.20718 That just tells R to ignore the NAs when calculating the standard deviation (and you can do the same thing with the interquartile range). 3.4 A note on using the states2010 and world dataframes When looking at the states2010 and world dataset, there are times when we might want to know which values of a variable come from which states or countries. When we want to know this, the select and arrange commands from the tidyverse can be helpful. For example, this code can help us learn about which states have which tax burdens: states2010 %&gt;% select(state, x_tax_burden) %&gt;% #focusing in on a few variables arrange(x_tax_burden) #sorting this variable from smallest to largest #to sort from largest to smallest, you would write #arrange(desc(x_tax_burden)) When you execute this code, what do you learn about the states with the largest and smallest tax burdens? 3.5 Review of this chapters commands Command Purpose Library class() Tells us how a specific vector is categorized. Is it a character vector, an ordered factor, etc.? Base R ggplot() Begins a ggplot graphic. Feel free to refer to the 20093 ggplot2 templates script file to find templates for all graphs you will make using this workbook ggplot2 (tidyverse) mean() Calculates the mean of a variable. Base R median() Calculates the median of a variable. Base R sd() Calculates the standard deviation of a variable. Use na.rm=TRUE if there is some missing data, like this: sd(dataframe$variable, na.rm=TRUE). Base R IQR() Calculates the interquartile range of a variable. Use na.rm=TRUE if there is some missing data, like this: IQR(dataframe$variable, na.rm=TRUE). Base R 3.6 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 3 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse, epiDisplay, and scales. Choose a nominal variable from either anes2020, states2010, or world. Generate an appropriate graph and calculate the relevant statistics to make observations about this variables central tendency and dispersion. Create a new file in your word processor, and paste your graph in. Also list all relevant statistics to help you make observations about this variables central tendency and dispersion. Make some observations about the variable from #2s central tendency and dispersion. Is there anything that surprises you about it? What? Choose an ordinal variable from either anes2020, states2010, or world. Generate an appropriate graph and calculate the relevant statistics to make observations about this variables central tendency and dispersion. Paste your graph into the word processor file that you generated for #2. Also list all relevant statistics to help you make observations about this variables central tendency and dispersion. Make some observations about the variable from #4s central tendency and dispersion. Is there anything that surprises you about it? What? Choose an interval variable from either states2010 or world. Generate an appropriate graph and calculate the relevant statistics to make observations about this variables central tendency and dispersion. Paste your graph into the word processor file that you generated for #2. Also list all relevant statistics to help you make observations about this variables central tendency and dispersion. Discuss the countries or states that have the highest and lowest values on your variable of interest (see the note at the bottom of section 3.4 for help with this). Are there any countries or states that surprise you on this list? Make some observations about the variable from #6s central tendency and dispersion. Is there anything that surprises you about it? What? "],["recoding.html", "Chapter 4 Recoding and comparing values of variables 4.1 Getting started with this chapter 4.2 Recoding ordinal and nominal variables 4.3 Dealing with variables coded 0/1 4.4 Simplifying interval variables 4.5 Cross-tabulation tables 4.6 Mean comparison tables 4.7 Review of this chapters commands 4.8 Review exercises", " Chapter 4 Recoding and comparing values of variables 4.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. We are going to install three new packages to use today. One is called epiDisplay and the other is called Hmisc. Enter these two commands into your Console one by one: install.packages(&quot;Hmisc&quot;) install.packages(&quot;tigerstats&quot;) Now, open a new script file and save it in your scripts folder as chapter 4 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 4, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(epiDisplay) #the tab1 command helps us make nice frequency tables library(Hmisc) #the cut2 command helps us simplify interval variables library(tigerstats) #colPerc can also be useful with crosstabs Now select all the text on this page, run it, and save it. 4.2 Recoding ordinal and nominal variables There are times that you will want to change the values of variables. Perhaps you are interested in one particular value of a key variable (such as married people, or high-income countries). Perhaps your variable has some values which you would like to treat as missing. Or, perhaps you will want to simplify a variable, or change a variable from character to numeric or vice versa. When doing so, you will have two powerful allies: the mutate() command and the recode() command, both available through the tidyverse package. To start, lets take a look at the ideology variable in the anes2020 dataset. We can use the tab1() command that we learned last lesson to look at how the values are distributed, running this command tab1(anes2020$ideology) ## anes2020$ideology : ## Frequency %(NA+) %(NA-) ## extremely liberal 369 4.5 5.2 ## liberal 1210 14.6 17.1 ## slightly liberal 918 11.1 13.0 ## moderate 1818 22.0 25.8 ## slightly conservative 821 9.9 11.6 ## conservative 1492 18.0 21.1 ## extremely conservative 428 5.2 6.1 ## NA&#39;s 1224 14.8 0.0 ## Total 8280 100.0 100.0 Looking at this output, we can see that there are seven possible values (excluding the NAs), ranging from extremely liberal to extremely conservative. There are times when we might want all of this detail, and we should not throw away data. However, we will often just want to compare liberals, moderates, and conservatives. To simplify this variable, we will first tell R to make a new version of the ANES dataframe that will house our new variable. Then, we will use mutate and recode to give values for our new variable. Here is what the code will look like: anes2020&lt;-anes2020 %&gt;% mutate(ideology3=recode(ideology, &#39;extremely liberal&#39;=&quot;liberal&quot;,&#39;liberal&#39;= &quot;liberal&quot;,&#39;slightly liberal&#39;=&quot;liberal&quot;, &#39;slightly conservative&#39;=&quot;conservative&quot;, &#39;conservative&#39;=&quot;conservative&quot;, &#39;extremely conservative&#39;=&quot;conservative&quot;)) There is a lot going on in the above command, so lets try to break it down: POST OPERATION DATAFRAME&lt;-PRE OPERATION DATAFRAME %&gt;% mutate(NEW VARIABLE NAME=recode(OLD VARIABLE NAME, OLD VARIABLE VALUE #1=&quot;NEW VARIABLE VALUE #1&quot;, OLD VARIABLE VALUE #2=&quot;NEW VARIABLE VALUE #2&quot;,)) Using mutate and recode together, we are generating a new version of our dataframe that has a new variable whose values are based on another variable. I have an important piece of advice when using this kind of command: be sure to copy and paste variable names and values from your Console screen to your script file, because it is very easy to make a typographical error when transcribing variable names and values. Even if you are careful, there will be times when you make an error in your code and R sends you an error message. Most of the time, you will be able to quickly figure out the source of your error, but if you ever cannot, please feel free to ask your instructor! Sometimes, you will realize that your error modified the dataframe in ways that you dont like. In that case, you can go back to the original dataframe. If you were using anes2020, you can use this command to restore that dataframe: load(&quot;Data/anes2020.Rda&quot;) Or, if that command doesnt work for you, you can follow the instructions in section 1.5 to reload the original dataframe. To confirm that our recode worked, we can run table on both our old and our new variable, like this: table(anes2020$ideology) ## ## extremely liberal liberal slightly liberal moderate slightly conservative ## 369 1210 918 1818 821 ## conservative extremely conservative ## 1492 428 table(anes2020$ideology3) ## ## liberal moderate conservative ## 2497 1818 2741 When we compare these two tables, we can see that our recoding worked. We have 1818 moderates in each variable variable. And, if we add the numbers of extremely liberal, liberal, and slightly liberal respondents (which we can do in R!), we can see that 369+1210+918=2497. The same is also true with the conservatives. What if we were interested in examining the horseshoe theory  that people on the extremely left and extreme right have some major things in common? In that case, we might want to generate a new variable which measures whether or not someone is an extremist. To do that, we could do another version of the mutate/recode command like we did above. But that would require us to type all seven values of our original variable into our code again, which takes a little while. Instead, we could also use the .default qualifier to tell R: assign this value to all other cases. Here is how that code would look: anes2020&lt;-anes2020 %&gt;% mutate(extremist=recode(ideology, &#39;extremely liberal&#39;=&quot;extremist&quot;, &#39;extremely conservative&#39;=&quot;extremist&quot;, .default=&quot;not extremist&quot;)) Now, when we run the command: table(anes2020$extremist) ## ## extremist not extremist ## 797 6259 we can see that we successfully created our new variable. 4.3 Dealing with variables coded 0/1 When looking at data in the real world, we will often find variables coded with only two possible values  0 or 1. These variables are sometimes called dummy variables or binary variables. In general, these variables are used to mean the presence or absence of some characteristic. For example, either someone is married (1), or they are not (0). Either a country is a democracy (1) or it is not (0). On the anes2020 dataframe, I have coded the variable sex in this way. If you refer to the codebook, you can see that 0 means male and 1 means female. It will be useful mathematically to have it coded this way in future labs. But it will be less useful when generating graphics, since the 1 and 0s produced on the axis and in the legends will not be that informative. So, we may want to generate a second sex variable that uses words instead of numbers. We can do so with this code: anes2020&lt;-anes2020 %&gt;% mutate(sex2=recode(sex, &#39;1&#39;=&quot;female&quot;,&#39;0&#39;=&quot;male&quot;)) We can check our results with this code (we could also use tab1 instead of table here): table(anes2020$sex) ## ## 0 1 ## 3763 4450 table(anes2020$sex2) ## ## female male ## 4450 3763 4.4 Simplifying interval variables Interval variables give us a lot of information, and it is sometimes too much! For example, we might want to generate a graph comparing age groups, rather than a graph of individuals at each age between 18 and 80. This is where the cut2() command from the Hmisc package can come in handy. Lets say we want to create three roughly evenly distributed age groups. We can use cut2 like this: anes2020$age3&lt;-cut2(anes2020$age, g=3) The g=3 is telling R to divide age into three roughly equal groups and make them into a new variable called age3. If we wanted 4 groups, or 2 groups, or 5 groups, we could just change that number. Now lets run table() on our new variable: table(anes2020$age3) ## ## [18,42) [42,63) [63,80] ## 2674 2705 2553 This output is telling us that our new variable has three values[18,42), [42,63), and [63,80]. This notation means that the first group has 18-year-olds to those younger than 42 (that is what the ) means), the second group has 42-year-olds to those younger than 63 (again, we learn this from the closed parenthesis), and the third group has 63-year-olds and older. We could use mutate(recode()) on those value names, like we did above, but here is an easier solution. The cut2 command generates an ordered factor, which is how R likes to treat ordinal variables. Ordered factors in R have levels that are always displayed in the same order (it might be helpful to think of them as going from low to high). To see the levels of our new variable, we can use this command: levels(anes2020$age3) ## [1] &quot;[18,42)&quot; &quot;[42,63)&quot; &quot;[63,80]&quot; This output means that [18,42) is the low level of the new age3 variable, [42,63) is the medium level, and [63,80] is the high level. To rename those variables, we can just make a list using the c() command that we learned in lab 1, like this: levels(anes2020$age3)&lt;-c(&quot;young&quot;, &quot;middle aged&quot;, &quot;old&quot;) When we run that command, R renames the levels of our age3 variable. When renaming the levels of a variable, be sure to list the new levels in an order that corresponds with the old levels of the variable. If you typed old first, in the command below, you would be telling R to call people from 18-42 old, and you would also make old the lowest level of your age3 variable. To confirm that this worked, you can type table(anes2020$age3) and you should see your new labels. 4.5 Cross-tabulation tables Up until now, we have focused on single variables at a time. This is an essential component of social scientific analysis. However, many (perhaps most?) of the questions that we are interested in are about the relationship between two or more variables. Most of the rest of the new work that we will do in R from this point forward will be geared toward helping us to make observations and inferences about the relationship between more than one variable. One way that we can start to examine the relationship between more than one variable is by making cross-tabulation tables (often called crosstabs). These are essentially two frequency tables presented in a table. Crosstabs are most useful when looking at the relationship between an ordinal or nominal variable and another ordinal or nominal variable. For example, lets look at the relationship between education and income in the anes2020 dataframe. We can hypothesize that people with more education will make more income. To test this, we can first use the xtabs() command, like this: xtabs(~income+edu, data=anes2020) ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd ## under $24,999 160 409 390 208 221 96 31 ## $25,000-64,999 122 442 569 364 490 190 50 ## $65,000-$109,999 40 255 346 277 550 329 96 ## $110,000 or more 22 130 270 180 652 480 203 Lets break down how we used this command: we put the dependent variable, income, first, and then the independent variable, edu, second. Crosstabs generally have the independent variable in columns and the dependent variable in rows. This setup lets us to that. The output that our command generated is a basic crosstab. It tells us that 160 people with less than a high school education make under $24,999 per year, 122 people with less than a high school education make $25,000-64,999, etc. Now we might be tempted to look at this and say: Graduate school is a waste of time! Only 203 people from our sample with MDs, PhDs, and JDs make $110,000 or more, while 270 people with some college education make $110,000 or more. However, that would be a mistake! To understand why, we should look at the column totals from our crosstab, which we can produce with this command: addmargins(xtabs(~income+edu, data=anes2020)) ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd Sum ## under $24,999 160 409 390 208 221 96 31 1515 ## $25,000-64,999 122 442 569 364 490 190 50 2227 ## $65,000-$109,999 40 255 346 277 550 329 96 1893 ## $110,000 or more 22 130 270 180 652 480 203 1937 ## Sum 344 1236 1575 1029 1913 1095 380 7572 Take a look at those column totals. Our anes2020 dataframe has 380 people that got MAs, PhDs, or JDs, while it has 1575 people that have some college. So, returning to those two numbers from above, the 203 MDs, PhDs, and JDs in the highest salary category make up 53.4% of all of the MDs, PhDs, and JDs in our sample (because 203/380=.534), while the 270 people with some college making the highest salary category make up only 17.1% of all of the people with some college in our sample (because 270/1575=.171). Thus, when interpreting crosstabs it is important to look at the column percents, and not just the raw numbers. To make R generate all of the column percents in a crosstab at the same time, we can use the colPerc() command from the tigerstats package that we installed for today. We can enter this command: colPerc(xtabs(~income+edu, data=anes2020)) ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd ## under $24,999 46.51 33.09 24.76 20.21 11.55 8.77 8.16 ## $25,000-64,999 35.47 35.76 36.13 35.37 25.61 17.35 13.16 ## $65,000-$109,999 11.63 20.63 21.97 26.92 28.75 30.05 25.26 ## $110,000 or more 6.40 10.52 17.14 17.49 34.08 43.84 53.42 ## Total 100.00 100.00 100.00 100.00 100.00 100.00 100.00 Remember when interpreting this output that the numbers are now the column percents, not the raw numbers. If you take a look at this output, you will see that people with more education seem to make more money. 4.6 Mean comparison tables Crosstabs are useful when we have two nominal or ordinal variables. But what if we want to look at the relationship between a nominal or ordinal variable and an interval variable? In that case, a mean comparison table can be helpful. The tidyverse has some commands that we can use to generate such a table. To demonstrate, lets look at our states2010 dataframe, and the relationship between the region variable and the evangelical_pop variable (which measure the percentage of the population of a state that identifies as evangelical). The first thing that we need to learn is the group_by() command. This tells R: make groups based on the variable that we tell you. In our case, we want to make groups based on our independent variable (which, for now, must be ordinal or nominal). Once we have done that, we can use the summarise() command to get the mean. For good measure, we can also use the summarise() command to calculate the standard deviation as well as the size of each group. Here is what that command looks like: states2010 %&gt;% group_by(region) %&gt;% summarise(mean=mean(evangelical_pop), sd=sd(evangelical_pop), n=n()) ## # A tibble: 4 x 4 ## region mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 midwest 16.2 4.00 12 ## 2 northeast 4.86 1.67 9 ## 3 south NA NA 17 ## 4 west 22.3 16.2 13 We can see mean Midwest state has 16.2% of its population that is evangelical (with a standard deviation of 4), while the mean population in the Northeast has 4.86% of its population that is evangelical (with a standard deviation of 1.67). The reason that the South is NA is because there is a Southern state for which we do not have data on the evangelical_pop data (it happens to be the District of Columbia, not technically a state). To filter out states without data on that variable, we can add another line to our command: filter(!is.na(evangelical_pop))%&gt;% (we used a version of this command in section 3.2.1). Our new command looks like this: states2010 %&gt;% filter(!is.na(evangelical_pop)) %&gt;% group_by(region) %&gt;% summarise(mean=mean(evangelical_pop), sd=sd(evangelical_pop), n=n()) ## # A tibble: 4 x 4 ## region mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 midwest 16.2 4.00 12 ## 2 northeast 4.86 1.67 9 ## 3 south 28.1 11.2 16 ## 4 west 22.3 16.2 13 Now we can see that, perhaps not surprisingly, that regions appear to differ a lot in the percentage of evangelicals living in a typical state. 4.7 Review of this chapters commands Command Purpose Library mutate(recode()) A set of commands to rename the values of variables. dplyr (tidyverse) cut2() Turn an interval variable into an ordinal variable. Remember to specify g= to tell R how many groups to split your variable into. Hmisc levels() You can use this command to see the levels of an ordered factor, or you can also use it to assign new levels to an ordered factor, like this: levels(df$variable&lt;-c(new level1,new level2,) Base R xtabs() Makes a crosstab table with raw values. Remember to define variables like this: xtabs(~DV+IV, data=DATAFRAME) Base R addmargins(xtabs()) Makes a crosstab table with row and column totals. Base R colPerc(xtabs()) Makes a crosstab table with column percents. Useful for making comparisons. tigerstats group_by() A dplyr command that is useful when summarizing a dataframe according to a nominal or ordinal variable. Usually used with the pipe (%&gt;%). dplyr (tidyverse) summarise() A dplyr command that is useful when summarizing a dataframe according to a nominal or ordinal variable. Usually used with the pipe (%&gt;%). dplyr (tidyverse) 4.8 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 4 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse, Hmisc, and tigerstats libraries. Choose an interval variable from either anes2020, states2010, or world that you are interested in thinking about as a dependent variable (dont choose one of the variables that we used as an example in this chapter) Generate an appropriate graph and calculate the relevant statistics to make observations about this variables central tendency and dispersion. Create a new file in your word processor, and paste your graph in. Also list all relevant statistics to help you make observations about this variables central tendency and dispersion. Use the cut2() command from tigerstats to make the variable from question 2 into an ordinal variable with between two and five groups. Use the table() command to make sure that the values of your new variable make sense. Use the levels()&lt;-c() commands to rename the values of the variable that you created in question 2 so that they will make sense when they show up on tables and in graphs. Use the table() command to make sure that your renaming worked. Choose another variable from the same dataset that you want to use as an independent variable. If your variable is nominal or ordinal, it must have more than two possible values. In your word processor file, briefly (in 1-4 sentences) discuss a hypothesis about the relationship between your independent and dependent and dependent variable. What do you expect to see and why? Using cut2(), mutate(recode()), and/or levels()&lt;-c() recode your independent variable so that it will be easier to interpret. If your variable was interval, you should turn it into an ordinal variable. If it was already ordinal or nominal, you should turn it into a simpler nominal or ordinal variable. Using xtab() and colPerc(), generate a crosstab with the variable that you generated in question 4 and the variable that you generated in question 6. In your word processor file, briefly (in 1-4 sentences) discuss what you can conclude about your hypothesis from question 5, referring specifically to some of the percents from the colPerc() crosstab in your answer. Using group_by() and summarise(), generate a mean comparison table, grouping by the variable that you generated in question 6, and calculating the mean of the interval variable from question 2. In your word processor file, interpret your mean comparison table in 1-4 sentences, referring to some of the specific means. If this table suggests a different conclusion from your crosstab, discuss why this might be the case. "],["making-controlled-comparisons.html", "Chapter 5 Making controlled comparisons 5.1 Getting started with this chapter 5.2 Representing crosstabs with stacked bar graphs 5.3 Representing mean comparison tables with boxplots 5.4 Adding a control to a crosstab 5.5 Adding a control to a mean comparison table 5.6 Review of this chapters commands 5.7 Review exercises", " Chapter 5 Making controlled comparisons 5.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now, open a new script file and save it in your scripts folder as chapter 5 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 5, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap library(tigerstats) #colPerc can also be useful with crosstabs Now select all the text on this page, run it, and save it. 5.2 Representing crosstabs with stacked bar graphs In chapter 4 we learned how to generate crosstabs (and, crucially for interpretation, crosstabs with percents in columns). Crosstabs themselves have a lot of numbers on them, and can thus be difficult to interpret on their own. Many social scientists address this problem by generating graphs. A stacked bar graph is a really nice tool for visually depicting relationships in a bar graph. The code to generate stacked bar graphs is a little bit long and complex, so just feel free to copy and paste the below or the code from the 20093 ggplot2 templates file that you should ahve downloaded in section 1.2, step 7. Lets return to the relationship that we were looking at in section 4.5, between education (our independent variable) and income (our dependent variable). Here is the code: #first, generate a dataframe called &quot;plotting.data&quot; based on grouped variables from #anes2020 plotting.data&lt;-anes2020 %&gt;% filter(!is.na(edu)&amp;!is.na(income)) %&gt;% group_by(edu, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;edu&#39;. You can override using the `.groups` argument. #second, make the graph ggplot(plotting.data, aes(x = edu, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ #this line is in here because #without it the x-axis labels #overlap. You can remove it if #that isnt a problem scale_fill_viridis_d(name=&quot;income&quot;)+ #this color palate is made to be readable for #colorblind people ggtitle(&quot;Education and income in the 2020 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) One thing to note in this code: it begins by creating a new dataframe called plotting.data. When you run it, that dataframe will show up in your RStudio environment. If you run it again with new variables, the new plotting data that you make will overwrite the old one. If it bothers you having that plotting.data object in your environment, you can always remove it with the command rm(plotting.data). Another thing to note: the scale_fill_viridis_d()+ line of code asks R to use a color palette that is easy to read for people that are colorblind. There are many, many choices of color palette in R, and you are welcome to play around with colors, but it is also fine to just stick to this palette. If youd rather have a black and white graph, there is code for that in the 20093 ggplot2 templates file. Looking at the graph, we can see that our X-axis is our independent variable, education, and our Y-axis is the column percentages of our dependent variable, income. Focusing on the yellow region, we can clearly see that each increase in education seems to correspond to an increase in the percent of people in the highest income earning category (with the possible exception of the movement between some college and associate degree). Focusing on the dark blue region, we can clearly see that, as level of education increases, the percent of people in the lowest income category decreases. In short, this graph has all of the same information that we got when looking at the crosstab that we generated with: colPerc(xtabs(~income+edu, data=anes2020)) from our previous lab, but it is much easier to take in a bunch of that information at the same time. 5.3 Representing mean comparison tables with boxplots In addition to the histogram that we introduced in chapter 3, another useful way of representing the distribution of a single interval variable is with a boxplot. Here is the code for a boxplot of the variable (you can also find this code in the 20093 ggplot2 templates file): #make a boxplot of evangelicals ggplot(states2010, aes(y = evangelical_pop)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ scale_x_continuous(breaks = NULL) + ggtitle(&quot;Religion in US states&quot;)+ xlab(NULL)+ ylab(&quot;Percent of state residents that are evangelical&quot;) ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). A boxplot gives us some interesting information about an interval variable. The bottom of the box is the 1st quartile (25% of cases have that value or lower) and the top of the box represents the 3rd quartile (75% of cases have that value or lower). The horizontal line represents the median value, and the vertical lines coming out of the box extend to the first quartile minus 1.5 multiplied by the interquartile range and the third quartile plus 1.5 multiplied by the interquartile range. Cases above and below this line are outliers, and are identified by points on the graph. The one outlier in this graph is Utah, which I figured out using this code: states2010 %&gt;% select(state, evangelical_pop) %&gt;% arrange(desc(evangelical_pop)) Here is a labeled version of a boxplot for your reference: So, in other words, we can learn a lot about a variable from a single graph! Boxplots are also a useful way to look at the properties of an interval variable across several values of a nominal or ordinal variable, which is what we were doing with our mean comparison table in chapter 4. Lets take a graphical look at the relationship between the region that a state is in and the percent of evangelicals that live in that state. We can set up that kind of graph with the following code: #make boxplots of regions and evangelicals ggplot(states2010, aes(x = region, y = evangelical_pop)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ ggtitle(&quot;Region and religion in US states&quot;)+ xlab(&quot;Region&quot;)+ ylab(&quot;Percent of state residents that are evangelical&quot;) ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). This graph shows us a lot of cool things. We can see that the states with the smallest percentages of evangelicals in the South appear to have roughly the same percent as the highest outlier in the Northeast. We can also see that, despite the existence of an extreme outlier (which we learned above is Utah), the states in the West still seems to have a smaller percent of evangelicals than states in the South. 5.4 Adding a control to a crosstab So far, we have learned to look at the relationship between pairs of variables. However, in real social scientific analysis we will often be interested in more than that. We will be interested in other factors that could be the true cause of variation in our dependent variable. In other words, we often want to know whether the correlation that we observe is spurious. If the relationship between the independent and dependent variable is the same at different values of a control, we call the relationship between the independent and dependent variables, controlling for the third, additive. If the relationship between the independent and dependent variables is different at different values of a control, we call that relationship an interaction. Lets return to the relationship that we looked at earlier, between education and income. Here is the crosstab that we generated in chapter 4: ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd ## under $24,999 46.51 33.09 24.76 20.21 11.55 8.77 8.16 ## $25,000-64,999 35.47 35.76 36.13 35.37 25.61 17.35 13.16 ## $65,000-$109,999 11.63 20.63 21.97 26.92 28.75 30.05 25.26 ## $110,000 or more 6.40 10.52 17.14 17.49 34.08 43.84 53.42 ## Total 100.00 100.00 100.00 100.00 100.00 100.00 100.00 Lets look at the row representing the people who make $110,000 or more per year. As we move from the lowest level of education to the highest, we can see that the percent of people in the highest income category moves from 6.4% to 43.84%. We can actually subtract 6.4 from 43.84 to say that the effect size is roughly 37.44%. In other words, 37.44% more people with the highest degree of education are in the highest income category than are people in the lowest education category. Do we think that this result would be roughly the same for men and women? To answer this question, we can use a neat tool that we have actually played with a bit in previous chapters: the filter command that is a part of the dplyr package (and thus works with pipes). The ANES dataset has a variable called sex, where 0 means men and 1 means women. Lets have R make that crosstab again, but only for men, and then again, but only for women. To do that, we can use this code: #men only colPerc(xtabs(~income+edu, data=anes2020 %&gt;% filter(sex==0))) ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd ## under $24,999 38.69 26.56 20.31 13.65 11.30 8.00 6.93 ## $25,000-64,999 36.31 37.54 34.27 34.99 24.76 15.16 12.38 ## $65,000-$109,999 16.67 22.95 23.13 32.01 27.45 28.42 23.76 ## $110,000 or more 8.33 12.95 22.28 19.35 36.49 48.42 56.93 ## Total 100.00 100.00 100.00 100.00 100.00 100.00 100.00 A quick note: if we wanted to use filter with a variable whose values are defined by text, such as our income variable, we must put the text in quotation marks. Here, we do not have to use quotation marks because sex is defined numerically. If we do the same comparison that we did above, we can see that as men more from the lowest to the highest education category, the percent in the highest income category increases from 8.33% to 48.42%. The total effect size is about 40.09%, which is a few percent higher than the 37.44% overall effect size that we witnessed above. Now lets look at women only: #women only colPerc(xtabs(~income+edu, data=anes2020 %&gt;% filter(sex==1))) ## edu ## income less than high school high school some college associate degree ba ma md, phd, or jd ## under $24,999 53.98 39.42 28.32 24.52 11.71 9.35 9.60 ## $25,000-64,999 34.66 33.97 37.69 35.58 26.37 19.03 14.12 ## $65,000-$109,999 6.82 18.43 21.04 23.56 30.04 31.29 27.12 ## $110,000 or more 4.55 8.17 12.95 16.35 31.87 40.32 49.15 ## Total 100.00 100.00 100.00 100.00 100.00 100.00 100.00 Applying the same analysis that we did above, we can see that as womens education level increases from the lowest to the highest level, the percent of people in the highest income bracket moves from 4.55% to 40.32%; a total effect size of 35.77% (compared with 40.09 for men). In other words, this data suggests that both men and women are economically rewarded for more education, but that men seem to be rewarded a bit more. If we were to write up these results, at this point we would have to use our own judgment to decide whether the effect size differs enough to call the relationship between education and income controlling for gender an interaction, or whether it makes more sense to call it additive. We can use more advanced statistical tools to make that determination with more rigor, but for now, we will rely on our judgment. There is not a single tool to graph relationships between three ordinal or nominal variables in an easy to interpret way. Instead, we can use the filter%&gt;% command from above to make two graphs, and then use insert table in our word processor to nearly display them side by side. Here is the code for the graph for only men (with the line filter(sex==0) %&gt;% #filtering so that we only look at men added): plotting.data&lt;-anes2020 %&gt;% filter(!is.na(income)&amp;!is.na(edu)) %&gt;% filter(sex==0) %&gt;% #filtering so that we only look at men group_by(edu, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;edu&#39;. You can override using the `.groups` argument. ggplot(plotting.data, aes(x = edu, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d()+ ggtitle(&quot;Education and income for men in the 2020 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) And here is the code for the graph for only women: plotting.data&lt;-anes2020 %&gt;% filter(!is.na(income)&amp;!is.na(edu)) %&gt;% filter(sex==1) %&gt;% #filtering so that we only look at women group_by(edu, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;edu&#39;. You can override using the `.groups` argument. ggplot(plotting.data, aes(x = edu, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d()+ ggtitle(&quot;Education and income for men in the 2020 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) 5.5 Adding a control to a mean comparison table The ANES survey has a number of feeling thermometer questions, where respondents were asked how warmly or coldly they felt toward institutions, individuals, and groups of people. These questions all range from 0 (meaning that the respondent feel very negatively toward the subject), to 100 (meaning that the respondent feels very positively about the subject). For this section, I am going to treat the feeling thermometer about Donald Trump as my dependent variable. For my independent variable, I am going to look at a question that respondents were asked about how outraged they feel about how things are going in the country these days, which is coded as outrage in the anes2020 dataframe. My hypothesis will be this: when comparing individuals, those that were outraged about how things are going will be more likely to feel negatively about Donald Trump than will those who were not outraged about how things are going. If you runtable(anes2020$outrage), you will see that the variable is coded ordinally with five values ranging from not at all to extremely. Before running the analysis, I am going to simplify this variable so that we are comparing people who are very or extremely outraged with everyone else. I am going to call my new variable outrage2. Here is the code that I will use to simplify this variable: anes2020&lt;-anes2020 %&gt;% mutate(outrage2=recode(outrage, &#39;very&#39;=&quot;outraged&quot;, &#39;extremely&#39;=&quot;outraged&quot;, .default=&quot;not outraged&quot;)) Notice how this code uses the .default qualifier rather than typing out the three other possible values that I wanted to recode into not outraged. That is a nice shortcut. Now, lets make a mean comparison table to see how the mean Donald Trump feeling thermometer score differs between outraged and non-outraged people: anes2020 %&gt;% filter(!is.na(ft_trump)&amp;!is.na(outrage2)) %&gt;% group_by(outrage2) %&gt;% summarise(mean=mean(ft_trump), sd=sd(ft_trump), n=n()) ## # A tibble: 2 x 4 ## outrage2 mean sd n ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 not outraged 56.6 35.9 3420 ## 2 outraged 28.4 39.2 4617 This suggests that our hypothesis is true. People who were outraged in 2020 gave President Trump an average score of 28.4 on a feeling thermometer, while people who were not outraged gave President Trump an average score of 56.6 on a feeling thermometer: a difference of 28.4. That seems like a pretty substantial difference! To graph this relationship, we can use this code: ggplot(anes2020 %&gt;% filter(!is.na(outrage2)), aes(x = outrage2, y = ft_trump)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ ggtitle(&quot;ANES 2020, Feelings about President Trump by level of outrage&quot;)+ xlab(&quot;Is R outraged?&quot;)+ ylab(&quot;Feelings about President Trump\\n(100 is warm and 0 is cold)&quot;) ## Warning: Removed 225 rows containing non-finite values (stat_boxplot). This graph shows us that the feeling thermometer scores for President Trump seem to have ranged from 0 to 100 for both outraged and non-outraged respondents. However, the median score for outraged respondents was a 0, while the median score for not outraged respondents was a 65, which is just below the 3rd quartile of outraged respondents (which was a 70). When doing this analysis, I suspected that what is really going on had to do with partisanship. Maybe democrats were more likely to be outraged than Republicans since the president was a Republican. To test this hypothesis, I took a look at my partyid variable, and I saw that it had seven values ranging from strong democrat to strong republican. I would like to simplify this variable to make it a bit easier to interpret these results, so I do so with the following code (notice how I save myself some typing with the .default qualifier): anes2020&lt;-anes2020 %&gt;% mutate(partyid3=recode(partyid, &#39;strong democrat&#39;=&quot;democrat&quot;, &#39;not very strong democrat&#39;=&quot;democrat&quot;, &#39;independent democrat&#39;=&quot;democrat&quot;, &#39;independent&#39;=&quot;independent&quot;, .default=&quot;republican&quot;)) Now lets add our control variable to our mean comparison table. It is easier to add a control variable into a mean comparison table than it was to add a control variable into our cross tab. To do so, we can use the following code: anes2020 %&gt;% #first I filter out the NAs for all 3 variables filter(!is.na(ft_trump)&amp;!is.na(partyid3)&amp;!is.na(outrage2)) %&gt;% #then I group by the control first, and then the IV group_by(partyid3, outrage2) %&gt;% #then I summarise in the same way that I do with a two-variable table summarise(mean=mean(ft_trump), sd=sd(ft_trump), n=n()) ## `summarise()` has grouped output by &#39;partyid3&#39;. You can override using the `.groups` argument. ## # A tibble: 6 x 5 ## # Groups: partyid3 [3] ## partyid3 outrage2 mean sd n ## &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 democrat not outraged 20.8 25.9 955 ## 2 democrat outraged 4.56 13.9 2782 ## 3 independent not outraged 45.7 30.1 487 ## 4 independent outraged 24.5 30.8 436 ## 5 republican not outraged 76.8 25.1 1968 ## 6 republican outraged 77.3 29.5 1391 The comments explain what is happening in the code. It is almost the same as the code for a two variable mean comparison table, but I group by the control (first) and then the independent variable, rather than just grouping by the independent variable, as I did in the two-variable code. Lets look at this output. First, focus on the first two rows. They tell us that Democrats who were outraged in 2020 felt about 16.24 points more coldly toward President Trump than Democrats who were not outraged (because 20.8-4.56=16.24). For independents, the effect of outrage was even larger. Independents who were outraged felt 21.2 points more coldly toward President Trump than independents who were not outraged (because 45.7-24.5=21.2). For Republicans, there seemed to be very little relationship at all between outraged and feeling about President Trump. In fact, people who were outraged felt -0.5 points more coldly toward President Trump because 76.8-77.3=-0.5). Or, put another way, outraged Republicans felt 0.5 points more warmly toward President Trump than did non-outraged Republicans. In other words, the relationship between outrage in 2020 and feelings about President Trump is very different at different levels of partisanship. We thus call this an interactive relationship. To graphically represent this relationship, we can use this code (which you can also find in the 20093 ggplot2 templates file): ggplot(anes2020 %&gt;% filter(!is.na(partyid3) &amp; (!is.na(outrage2))), aes(x = partyid3, y = ft_trump, fill=outrage2)) + geom_boxplot()+ ggtitle(&quot;ANES 2020, Feelings about President Trump by PartyID and Outrage&quot;)+ xlab(&quot;Party ID&quot;)+ ylab(&quot;Feelings about President Trump\\n(100 is warm and 0 is cold)&quot;)+ scale_fill_discrete(name = &quot;Level of outrage&quot;) ## Warning: Removed 212 rows containing non-finite values (stat_boxplot). This boxplot clearly shows how the relationship between outrage and feelings about President Trump was different for Democrats and independents than it was for Republicans. This is a really nice example of an interaction, and it suggests that outrage may have had different meanings for Democrats, independents, and Republicans. 5.6 Review of this chapters commands Command Purpose Library Filter(VARIABLE==VALUE) %&gt;% A way to filter out cases that you are not interested in. If the value you are interested in is coded as text (as is the case with our nominal and ordinal variables), be sure to put it in quotation marks. dplyr (tidyverse) 5.7 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 5 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse, Hmisc, tigerstats, and scales libraries. Choose a variable from either anes2020, states2010, or world that you are interested in thinking about as a dependent variable (dont choose one of the variables that we used as an example in this chapter). Generate an appropriate graph and calculate the relevant statistics to make observations about this variables central tendency and dispersion. Create a new file in your word processor, and paste your graph in. Also list all relevant statistics to help you make observations about this variables central tendency and dispersion. Choose a second variable from the same dataframe that you used for question 2 to use as an independent variable (i.e. as a variable that you think might cause variable in your dependent variable). In your word processor file, write a few sentences about what kind of relationship you expect to see between your independent and dependent variables and why. If the variable that you selected in #3 is interval, use the cut2() and/or mutate(recode()) commands to make it into an ordinal variable. If the variable that you selected on #3 is ordinal or nominal, make observations about the variables central tendency and dispersion in your word processor file (supported by an appropriate graph and appropriate statistics). Generate a crosstab or a mean comparison table (as appropriate) to examine the relationship between your independent variable from #3 and your dependent variable from #2, and also generate the appropriate graph to examine this relationship. Paste your graphics and your crosstab or mean comparison table (in a readable format) into your word processor file. Be sure to format your crosstab or mean comparison table in a way that is easy for the reader to understand (this might involve creating a table). In your word processor document, briefly (in 1-4 sentences) discuss what you conclude about your hypothesis based on the output from #5. Now choose another variable to use as a control. Using cut2() and/or mutate(recode()), recode this variable so that it has only a few values (two, or three at most). Generate a crosstab or a mean comparison table (as appropriate) to examine the relationship between your control from #7, your independent variable from #3 and your dependent variable from #2, and also generate the appropriate graph to examine this relationship. Paste your graphics and your crosstab or mean comparison table (in a readable format) into your word processor file. Be sure to format your crosstab or mean comparison table in a way that is easy for the reader to understand (this might involve creating a table). In your word processor document, briefly (in 1-4 sentences) discuss what you conclude about your hypothesis based on the output from #8. Would you say that the relationship between your independent variable and your dependent variable, controlling for your control variable, is additive, interactional, or spurious? Why? "],["inferences.html", "Chapter 6 Making inferences from sample means 6.1 Getting started with this chapter 6.2 Constructing a confidence interval around a mean 6.3 Constructing a confidence interval around a proportion 6.4 Constructing confidence intervals around several proportions at once 6.5 T-test of independent means 6.6 T-test of independent proportions 6.7 Review of this chapters commands 6.8 Review exercises", " Chapter 6 Making inferences from sample means 6.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now, open a new script file and save it in your scripts folder as chapter 6 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 6, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap Now select all the text on this page, run it, and save it. 6.2 Constructing a confidence interval around a mean The fundamental task of much statistical analysis is to make mathematically informed inferences about populations from samples. We can see this most clearly when looking at data like the ANES, which is a sample of Americans from which we want to make inferences about all Americans. But even when looking at our states or world dataset, in which our dataframe includes data about all states and (almost) all countries in the world, it is still helpful to think about making inferences about populations. In this case, we might think of the population as states or countries that we cannot observe because they are hypothetical, they have not yet come into existence (because they are in the future) or they existed in the past. In other words, we are studying our group of states and countries to make inferences about hypothetical states and countries, states and countries in the future, states and countries in the past, or state and countries that we cannot observe for some other reason. One inference that we often want to make about a population is the mean of some variable. Given the mean that we observe in sample, what is the range of outcomes for the population mean that we might reasonably expect to observe? This is the formula for the confidence interval around a mean: \\[CI = \\bar{x} ± t_{critical}{\\frac{s}{\\sqrt{n}}}\\] The critical value for t will vary with the sample size. S is the sample standard deviation, and n is the sample size. You might recall from class that \\[{\\frac{s}{\\sqrt{n}}}\\] is the standard error of the sample mean. Lets imagine that we want to estimate the mean feeling thermometer value for Americans view of the Centers for Disease Control and Prevention (the CDC). First we can ask R to calculate the mean of our sample, using the mean() command that we learned in Chapter 3. If we were to enter mean(anes2020$ft_cdc), we would get NA as our output, because there are some cases with missing data. To address this, we must ask R to remove the NAs for the purposes of this operation with na.rm=TRUE, like this: mean(anes2020$ft_cdc, na.rm=TRUE) ## [1] 70.19634 This tells us that the mean of our sample is 70.19634. Or, in other words, the average person in our sample gives the CDC a 70.2 on a feeling thermometer with possible values ranging from 0-100. Based on this, what can we say about the population mean? To answer this question, we can ask R to construct a confidence interval around this mean, with this command: t.test(ft_cdc~1, data=anes2020) ## ## One Sample t-test ## ## data: ft_cdc ## t = 253.43, df = 7272, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 69.65338 70.73931 ## sample estimates: ## mean of x ## 70.19634 Before we look at the output, lets look at the command. We are asking R to construct a t-test of the mean of only one variable (which explains the ~1), and we are telling R to find that variable in the anes2020 dataframe. Looking at the output, after the title, One Sample t-test the next three lines are not particularly useful. The fourth and fifh lines, however, are quite helpful. They tell us that the 95% confidence interval that we can put around our sample mean goes from 69.7 to 70.7. In other words, there is a 95% chance that the true population mean feeling thermometer when this survey was taken was somewhere between 69.7 and 70.7. If we wanted to be even more confident than 95%say, for example, we wanted to be 99% confident that our interval contains the population mean-we could include a line in our command setting the confidence interval like this: t.test(ft_cdc~1, data=anes2020, conf.level=.99) ## ## One Sample t-test ## ## data: ft_cdc ## t = 253.43, df = 7272, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 69.48270 70.90999 ## sample estimates: ## mean of x ## 70.19634 When you run this command, notice that if we want to be more confident that our interval contains the true population mean, the margin between our lower and upper estimate gets larger. Similarly, if we set the confidence interval to 90%, or 80%, the interval gets smaller. 6.3 Constructing a confidence interval around a proportion For nominal and ordinal variables, it does not make sense to calculate a mean, but we still often want to make inferences about a population from a sample. In fact, these are one of the most commonly reported kinds of inferences that we who follow politics hear: 54.2% of people support this policy, 32.1% of people support that politician, etc. To ask R to calculate a confidence interval around a proportion, we first need two pieces of information: how many cases got the value that we are interested in, and how many cases are there total. So, for example, lets say that we want to use our anes2020 dataset to estimate the proportion of Americans that are married. We can first run the table command, like this: table(anes2020$marital) ## ## divorced married never married separated widowed ## 1221 4322 1951 163 567 When we run this command, we see that there are 4,322 married people in our sample. Unfortunately, the table command itself does not tell us the total number of cases in our sample, but to get that, we can ask R to sum all the values that it shows in the table command like this: sum(table(anes2020$marital)) ## [1] 8224 When we run this command, we get the total number of cases for which R has data on the marital question: 8,224. This is the sum of all of the numbers that we saw above, when we ran the table() command. Since the table() command automatically excludes the NAs, this sum also excludes NAs. So, now we know that there were 4,322 married people in our dataset out of the 8,224 people for whom we have data. We could do have R do some simple math and calculate a proportion by entering 4332/8224 into the Console. The output tells us that 0.526751about 52.7%of individuals in our sample are married. Based on this finding about our sample, what can we say about our population? To help us generalize, we can use those two numbers (4,332 married people out of 8,224 total people) along with the prop.test() command to help generate a confidence interval around that proportion. When using the prop.test() command, in the parenthesis we first put the number of cases with the value that we are interested in, then a comma, then the total number of cases. So, to estimate the proportion of Americans that are married, we would use prop.test() like this: prop.test(4332,8224) ## ## 1-sample proportions test with continuity correction ## ## data: 4332 out of 8224 ## X-squared = 23.434, df = 1, p-value = 1.293e-06 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.5158893 0.5375875 ## sample estimates: ## p ## 0.526751 Just like the output from the t-test, the first few lines are testing a hypothesis that we are likely not interested in. In this case, the default is to test the hypothesis that the population proportion is actually .5. If you want to set that hypothesis to a different value, you can add p=the value you are interested in after a comma. So, for example, if you had a hypothesis that 70% of Americans are married, you could test that hypothesis with this command: prop.test(4332,8224, p=.7) But the most important part of this output begins with the fourth line. This shows that the 95% confidence interval around the estimate that the proportion of Americans that are married goes from .5158893 to .5375875. Or, put another way, based on our sample, we can infer that 95% of the time, a sample like ours would be drawn from a population where between 51.6% and 53.8% of the population was married. If we want to change the values of our confidence interval, to be 99% confident (or 90% confident, or 80% confident), we can add a comma and then conf.level= and then the level that we are interested in, expressed as a proportion. So, for example, if we wanted to be 99% confident that our interval contains the true population proportion, we could alter the command like this: prop.test(4332,8224, conf.level=.99) Note that the interval gets wider as the confidence level goes up. 6.4 Constructing confidence intervals around several proportions at once In the previous section, we learned how to construct a confidence interval around a single proportion at a time. However, what if we want to calculate confidence intervals around all values of a nominal or ordinal variable at the same time? To do that, you can use the following code. Just replace anes2020 with the dataframe that you are interested in analyzing, and replace the two instances of marital (in the second and third lines of the code) with the name of the variable that you are interested in. You can keep everything else the same: anes2020 %&gt;% filter(!is.na(marital)) %&gt;% group_by(marital) %&gt;% summarise(group.n=n()) %&gt;% mutate(total.n=sum(group.n)) %&gt;% mutate(proportion=group.n/total.n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[1]) %&gt;% mutate(upper_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[2]) ## # A tibble: 5 x 6 ## # Rowwise: ## marital group.n total.n proportion lower_ci upper_ci ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 divorced 1221 8224 0.148 0.141 0.156 ## 2 married 4322 8224 0.526 0.515 0.536 ## 3 never married 1951 8224 0.237 0.228 0.247 ## 4 separated 163 8224 0.0198 0.0170 0.0231 ## 5 widowed 567 8224 0.0689 0.0636 0.0747 Reading down this output, we can see that a proportion of .148 (or 14.8%) of our sample is divorced. The next two columns tell us that there is a 95% chance that the true population proportion of divorced people is between 14.1% and 15.6%. We can see the married values on the second line, with a familiar confidence interval, and each additional line clearly displays the confidence interval around the proportion that we are interested in. And as we continue to read down, we can see the remaining proportions with their associated confidence intervals. 6.5 T-test of independent means We often find ourselves in a situation where we want to compare the means of two different groups in our sample, and make inferences about the population. So, for example, we might want to compare men and women, Democrats and Republicans, voters and non-voters, etc. For the purposes of this example, we will be comparing democracies and non-democracies in our world dataframe. Before we can start with this comparison, though, we have to do a bit of housekeeping. The democracy variable that we have in the world dataframe is an interval variable, with values ranging from 14.3 (a value shared by countries including Equatorial Guinea, Eritrea, North Korea, and Saudi Arabia) to 100 (a value shared by Andorra, Australia, the Bahamas, Canada, and the United States). To generate an ordinal variable to measure democracies, partial democracies, and non-democracies, we can use the cut2 command like this: world$democ3&lt;-cut2(world$fh_democ, g=3) When we run this command, we get a new ordinal variable in the world dataframe that has three levels. To see what those levels are named, we can use the levels command, like this: levels(world$democ3) ## [1] &quot;[14.3, 57.1)&quot; &quot;[57.1, 92.8)&quot; &quot;[92.8,100.0]&quot; We can rename those levels to more intuitive names like this: levels(world$democ3)&lt;-c(&quot;non-democracy&quot;,&quot;partial democracy&quot;,&quot;democracy&quot;) The question that I want to address is whether democracies have more economic equality than non-democracies. The conventional measure of economic equality is the GINI coefficient, which ranges from 0 (total equality) to 100 (total inequality). Using the commands that we learned in chapter 4, we can first examine what the mean differences are between democracies, partial democracies, and non-democracies on their GINI coefficient, with this command: world %&gt;% filter(!is.na(democ3)) %&gt;% filter(!is.na(UNDP_Gini2014)) %&gt;% group_by(democ3) %&gt;% summarise(mean=mean(UNDP_Gini2014), sd=sd(UNDP_Gini2014), n=n()) ## # A tibble: 3 x 4 ## democ3 mean sd n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 non-democracy 38.8 6.77 51 ## 2 partial democracy 43.8 10.3 53 ## 3 democracy 37.2 9.25 32 This output suggests that the most equal societies are democracies, followed by non-democracies, with partial democracies being the least equal of all. Are these differences due to random sampling error, or do they reflect a population-level difference between democracies and non-democracies? In other words, are these differences statistically significant? Before conducting a significance test, it is important to specify our null and research hypotheses. In this case, our null hypothesis is that there is no relationship between regime type and level of equality, and our research hypothesis is that democracies have more equality than non-democracies. To test our hypotheses, we can use the same t.test() command from above, but we have to give it more information first. Here is the command that we can use: t.test(UNDP_Gini2014 ~ democ3, data=world %&gt;% filter(democ3==&quot;democracy&quot;|democ3==&quot;non-democracy&quot;)) ## ## Welch Two Sample t-test ## ## data: UNDP_Gini2014 by democ3 ## t = 0.8757, df = 51.746, p-value = 0.3852 ## alternative hypothesis: true difference in means between group non-democracy and group democracy is not equal to 0 ## 95 percent confidence interval: ## -2.137422 5.446797 ## sample estimates: ## mean in group non-democracy mean in group democracy ## 38.82000 37.16531 Notice how the first thing in the parentheses after t.test is the dependent variable (i.e. the variable that we are calculating the mean of), followed by a ~ and then our independent variable (democ3). One thing that is a little tricky is that R does not like to do a t-test when the independent variable has more than two values. To address this, I have used the dplyr filter command to set it so that I am only looking at cases of democ2 coded democracy or non-democracy (i.e. I am filtering out the NAs and partial democracies). The | between the two conditions (it is the character above the \" on your keyboard) meansor. Now take a look at the output. There are a few things to note here. First of all, we could see that the reported p-value is .3852. That tells us that, if we accept the research hypothesis that democracies have more equality than non-democracies, there is a probability of .3852, or a 38.5% chance, that we are making Type I error (rejecting a true null hypothesis). Since the critical value for p is generally set to .05 (meaning we are not willing to take more than a 5% risk of Type I error), we would fail to reject the null hypothesis. We can also see that we would fail to reject the null hypothesis by looking at the confidence interval that R reports. Since this is a t-test of two samples, the confidence interval is actually of the difference between the two means. In other words, that confidence interval is telling us that there is a 95% chance that the true population difference in GINI coefficient between democracies and non-democracies is between -2.14 and 5.45. Since that interval includes 0, we cannot rule out the possibility that there is no difference at all between our two means in the population. We can also graph these results. As usual, we will put our independent variable (regime type) on the X-axis, and our dependent variable, GINI score, on the Y axis. For todays new feature, we will also add 95% confidence intervals to our graph. This gives us the ability to see whether the differences between all of the bars on our graph are statistically significant. And, while an independent samples t-test can only be between the means of two groups at a time, this graph will let you look at more than two groups at a time. Below is the code: plotting.data&lt;-world %&gt;% filter(!is.na(democ3)) %&gt;% filter(!is.na(UNDP_Gini2014)) %&gt;% group_by(democ3) %&gt;% summarise( n=n(), mean=mean(UNDP_Gini2014), sd=sd(UNDP_Gini2014)) %&gt;% mutate(se=sd/sqrt(n)) %&gt;% mutate(ci=se * qt((1-0.05)/2 + .5, n-1)) ggplot(plotting.data) + geom_bar(aes(x=democ3, y=mean), stat=&quot;identity&quot;, fill=&quot;purple&quot;, alpha=0.5) + geom_errorbar( aes(x=democ3, ymin=mean-ci, ymax=mean+ci), width=0.4, colour=&quot;black&quot;, alpha=0.9, size=1.5) + ggtitle(&quot;Equality in democracies and non-democracies&quot;, subtitle=&quot;Error bars represent 95% confidence intervals&quot;)+ xlab(NULL)+ ylab(&quot;Gini coefficient in 2014 (lower numbers mean more equality)&quot;) Here we can see the overlapping 95% confidence intervals on each bar, reminding us that the difference that we observe is not statistically significant. 6.6 T-test of independent proportions There are times when we want to compare two or more proportions to examine whether or not the differences are due to random sampling error or some genuine correlation at the population level. Unfortunately, R does not make it easy to run a t-test comparing two proportions. However, we can use dplyr in a way similar to what we did in section 6.3 to generate some helpful output. Before we begin, we have to know three things: What is the nominal or ordinal independent variable that we are interested in examining? What is the nominal or ordinal dependent variable that we are interested in examining? For our dependent variable, what is the value that we want to focus on? For the following example, I am interested in the relationship between whether or not a voter thought that the 2020 government response to COVID was appropriate or not and whether or not the person voted for Trump in the 2020 election. To generate a table that displays that information, I will use the following code. Please note that this is very similar to the code that we used in section (ci-around-proportions). When using this code to generate your own table, you can replace anes2020 with the dataframe that you want to look at, v2020 with your dependent variable, covid with your independent variable, and v2020==Donald Trump with: DV==&quot;the value that you are interested in&quot; Here is the code: anes2020 %&gt;% filter(!is.na(v2020)) %&gt;% filter(!is.na(covid)) %&gt;% group_by(covid) %&gt;% summarise( n=n(), numerat=sum(v2020==&quot;Donald Trump&quot;)) %&gt;% mutate(proportion=numerat/n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[1]) %&gt;% rowwise() %&gt;% mutate(upper_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[2]) ## # A tibble: 5 x 6 ## # Rowwise: ## covid n numerat proportion lower_ci upper_ci ## &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Much too quick 93 63 0.677 0.571 0.769 ## 2 Somewhat too quick 63 56 0.889 0.778 0.950 ## 3 About right 1888 1720 0.911 0.897 0.923 ## 4 Somewhat too slow 595 325 0.546 0.505 0.587 ## 5 Much too slow 3169 286 0.0902 0.0806 0.101 This output has a bunch of great information. The first row, much too quick, for example, tells us that a total of 93 people in the ANES study in 2020 thought that the governments COVID response moved much too quick. And of those 93, the second number tells us that 63 of them voted for President Trump. The third number tells us that 67.7% of people that thought that the governments response was much too quick voted for President Trump. The lower_ci and upper_ci columns tell us that there is a 95% chance that the true population proportion of Trump voters who thought that the COVID response was much too quick is between .571 (or 57.1%) and .769 (or 76.9%). If we read down the rows, we will see that some of the confidence intervals overlap with one another. When these confidence intervals overlap, we can say that the differences are not statistically significant (or that we cannot rule out the chance that the differences that we observed are due to random sampling error). So, for example, the confidence interval between the proportion of Trump votes that thought that the governments COVID response was somewhat too quick and the proportion that thought that it was about right overlap. The upper limit of the somewhat too quick interval is .95, which is higher than the lower limit of the about right interval (.897). This is easier to see on a graph. We can generate a graph with code similar to what we used in section (t-test-ind-means): plotting.data&lt;-anes2020 %&gt;% filter(!is.na(v2020)) %&gt;% filter(!is.na(covid)) %&gt;% group_by(covid) %&gt;% summarise( n=n(), numerat=sum(v2020==&quot;Donald Trump&quot;)) %&gt;% mutate(proportion=numerat/n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[1]) %&gt;% rowwise() %&gt;% mutate(upper_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[2]) ggplot(plotting.data) + geom_bar(aes(x=covid, y=proportion), stat=&quot;identity&quot;, fill=&quot;purple&quot;, alpha=0.5) + geom_errorbar(aes(x=covid, ymin=lower_ci, ymax=upper_ci), width=0.4, colour=&quot;black&quot;, alpha=0.9, size=1.5) + ggtitle(&quot;Assessment of Gov&#39;t covid response and proportion voting for Trump, ANES 2020&quot;, subtitle=&quot;Error bars represent 95% confidence intervals&quot;)+ xlab(NULL)+ ylab(&quot;Proportion voting for Trump&quot;) This graph shows us that most of the differences between these bars are statistically significant. In other words, peoples assessment of the governments response to COVID in 2020 does seem to be pretty strongly correlated with whether or not they voted for President Trump. Now when looking at this graph, we might be tempted to observe that it certainly seems like the vast majority of our sample voted for President Trump. After all, more than 50% of respondents in every category, from much too quick to somewhat too slow voted for President Trump. In only the final category, much too slow, were Trump voters a minority. What is going on here? To get a clue, we can look at those bars around the 95% confidence intervals. Notice that the narrowest interval is around much too slow. This is because that was, by far, the biggest group of people that we surveyed, which means that our estimate can be more precise (and thus the bars can be closer together). To see that even more clearly, lets take another look at that table that we generated above: ## # A tibble: 5 x 6 ## # Rowwise: ## covid n numerat proportion lower_ci upper_ci ## &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Much too quick 93 63 0.677 0.571 0.769 ## 2 Somewhat too quick 63 56 0.889 0.778 0.950 ## 3 About right 1888 1720 0.911 0.897 0.923 ## 4 Somewhat too slow 595 325 0.546 0.505 0.587 ## 5 Much too slow 3169 286 0.0902 0.0806 0.101 The last row tells us that there were 3,169 people who thought that the governments response to COVID was much too slow. This is much, much larger than any of the other groups. So, when interpreting these kinds of graphs, we want to be careful not to make interferences that arent warranted by the graph. The graph does indeed show us that people that approved of the governments handling of COVID in 2020 were more likely to vote for President Trump, but it does NOT show us that there were more Trump voters in 2020. If you want to know the proportion of our sample that voted for Trump with a 95% confidence interval, you can use table(anes2020$v2020), sum(table(anes2020$v2020)), and prop.test() to calculate it. 6.7 Review of this chapters commands Command Purpose Library t.test() To calculate a t-test. With a one sample test, you put the variable that we are interested in ~1, and then a comma, and then data=your dataframe. For an an independent samples t-test, you put DV~IV in the parenthesis, and then a comma, and then data= your dataframe. Base R t.test(, mu=?) A t-test of a single sample is testing the hypothesis that the population mean of the variable you are testing is 0. If you want to test another hypothesis (for example, that the average TCU student has a 17 purple shirts), you can set mu to be the number that you are hypothesizing. Base R t.test(, conf.level=?) The default confidence interval for a t-test is .95. If you want to set it to something else, you can do it with the conf.level command. Base R sum(table()) This command adds up all of the values in a frequency table. It is useful when calculating proportions. Base R prop.test() This command calculates a confidence interval around a proportion. You must give R two numbers. The number of your cases with the value that you are interested in, followed by a comma, and then the total number of cases that you have data on for that variable (which you can calculate with sum(table()). For example, if you had a sample with 500 people and 200 said that bagels were their favorite baked good, you would write prop,test(200,500) to get a confidence interval around the proportion of people that like bagels best. Base R prop.test(, p=?) The default hypothesis for an estimate of a population proportion is that the proportion is .5 (or 50%). If you want to change that, you can type p=and then the proportion that you are hypothesizing about. Base R prop.test(, conf.level=?) The default confidence interval for a test of proportions is .95. If you want to set it to something else, you can do it with the conf.level command. Base R 6.8 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 6 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse, Hmisc, and scales libraries. Choose an interval variable from either anes2020, states2010, or world (dont choose one of the variables that we used as an example in this chapter). Use t.test() to estimate the mean of this variable and a 95% confidence interval. Use t.test() again to generate a 99% confidence interval. Create a new file in your word processor. In a few sentences, interpret the means and confidence intervals from question 2. Choose a nominal or ordinal variable from either anes2020, states2010, or world (dont choose one of the variables that we used as an example in this chapter). Choose one particular value of that variable, and using table(), sum(table(), and prop.test(), calculate the proportion of cases with that value and a 95% confidence interval around that value. In the word processor file that you interpreted from #3, interpret the proportion and confidence interval from #4. Look at the variables that you chose in #2 and #4, and decide which one youd rather analyze as a dependent variable. Now choose a nominal or ordinal variable as your independent variable. Using techniques from chapter 3, generate some statistics and a graph to discuss the central tendency and dispersion of your variable. Paste your graph into your word document. In a few sentences, hypothesize about the relationship between your independent and dependent variables. What do you expect to see and why? If your dependent variable is interval, use the t.test() command to test your hypotheses (remember that you might have to use filter so that you are only looking at two values for your independent variable). If your dependent variable is ordinal or nominal, use the code from section 6.6 of this chapter to generate confidence intervals around proportions of a value of interest of your DV for all possible values of your IV. Graph the relationship between your IV and DV using techniques from either section 6.5 or 6.6. Paste your graph into your word processor file as well as a table that shows the statistical significance of the relationships that you are interested in. Based on your output from #8 and #10, what do you conclude about your hypothesis from #7 and why? "],["chi-squared-etc.html", "Chapter 7 From chi-squared to somers d 7.1 Getting started with this chapter 7.2 Caculating chi-squared 7.3 Lambda and the PREs 7.4 Cramers V 7.5 Somers D 7.6 Review of this chapters commands 7.7 Review exercises", " Chapter 7 From chi-squared to somers d 7.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now type install.packages(\"DescTools\") into the Console. This new package will help us calculate Cramers V, Lambda, and Somers D. Now, open a new script file and save it in your scripts folder as chapter 7.2 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 7, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap library(tigerstats) #colPerc can also be useful with crosstabs library(DescTools) #this has tools to calculate Cramer&#39;s V, Lambda and #Somers&#39; D Now select all the text on this page, run it, and save it. 7.2 Caculating chi-squared In chapter 6, we learned two tools to think about relationships between variables: a t-test of independent means and a t-test of independent proportions. These tests let us compare two groups and see whether their means or proportions differ in a statistically significant way. However, what if we want to compare more than two groups at the same time? And, in the case of proportions, what if we want to look at several different proportions at once? For example, the proportion of people that voted for Trump, Biden, Jo Jorgeson, and the other candidates as well? In other words, to return to a concept from chapter 4, what if we wanted to look at a whole cross tabulation table (a crosstab) and decide whether the values that we observe are likely due to random sampling error, or due to a correlation at the population level? The most basic way to do that is by calculating chi-squared. That statistic tells us whether the values that we observe in a crosstab are so different from what we would expect to observe if there was no association between at the population level that we can conclude that there likely is such a relationship. For this example, we will think about the relationship between the dominant religion of a country and the degree of economic equality in that country. We will think through that question with help from our world dataframe, using the Religion variable and the UNDP_Gini2014 variable. Before we generate our crosstab, we first have to deal with an issue: the UNDP_Gini2014 variable is interval. If we generate a crosstab with that variable as the DV, that crosstab will have one row for each value of that variable, which would likely give us hundreds of rows. We dont want that! Instead, lets use the technique that we learned in chapter 4 section 4 and simplify that variable into a new variable called gini3. First, we will use the cut2 command to generate a variable with three groups: world$gini3&lt;-cut2(world$UNDP_Gini2014, g=3) Next, we can look at the levels of our new ordinal variable, like this: Finally, we can rename those levels, like this: ```r levels(world$gini3)&lt;-c(&quot;equal&quot;,&quot;somewhat equal&quot;,&quot;not equal&quot;) Now, we can generate a crosstab, with this command (remember when generating a crosstab we put our dependent variable first): addmargins(xtabs(~gini3+Religion, data=world)) ## Religion ## gini3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath Sum ## equal 1 1 0 16 8 0 7 12 45 ## somewhat equal 8 0 1 20 2 3 5 7 46 ## not equal 0 0 0 4 1 6 5 28 44 ## Sum 9 1 1 40 11 9 17 47 135 This crosstab gives us raw numbers, which are interesting to look at. It also foreshadows a potential limitation to our conclusions, because we only have one predominantly Jewish state (Israel) and one predominantly Hindu state (India) in our sample, which means that we will be attempting to generalize about those two kinds of states from a tiny sample. We will discuss that more later. To better interpret this crosstab, we need to look at column percents, which we can do with this command: colPerc(xtabs(~gini3+Religion, data=world)) ## Religion ## gini3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath ## equal 11.11 100 0 40 72.73 0.00 41.18 25.53 ## somewhat equal 88.89 0 100 50 18.18 33.33 29.41 14.89 ## not equal 0.00 0 0 10 9.09 66.67 29.41 59.57 ## Total 100.00 100 100 100 100.00 100.00 100.00 100.00 Take a look at that table. You can see that there are some pretty noticeable differences in the way that economic inequality is distributed across states with different major religions. For example, the religion with the largest percentage of equal states is Eastern Orthodox, and the religious with the largest percent of not equal states is Other, followed by Roman Catholic. To help us visualize these differences, we can generate a stacked bar graph using the technique that we learned in section 5.2: plotting.data&lt;-world %&gt;% filter(!is.na(Religion)&amp;!is.na(gini3)) %&gt;% group_by(Religion, gini3) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ## `summarise()` has grouped output by &#39;Religion&#39;. You can override using the `.groups` argument. #second, make the graph ggplot(plotting.data, aes(x = Religion, y = freq, fill=gini3)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d(name=&quot;Economic equality&quot;)+ ggtitle(&quot;Countries&#39; major religion and degree of economic equality&quot;)+ xlab(NULL)+ ylab(NULL) Clearly, there are differences in the distribution of economic equality in countries with different major religions. But, are these differences statistically significant? To test that, we can ask R to calculate the Chi-squared value that is associated with this relationship. To do this, we can use the chisq.test() command, and in the parenthesis we can put the command that generates crosstab that we want to run this test on. Note: be careful to use the chisq.test() command on the command for a crosstab with actual values and not percents, or the value produced by the test will not be accurate. Here is how we would generate a chi-squared value for the relationship between dominant religion of a country and degree of economic equality: chisq.test(xtabs(~gini3+Religion, data=world)) ## Warning in chisq.test(xtabs(~gini3 + Religion, data = world)): Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: xtabs(~gini3 + Religion, data = world) ## X-squared = 56.827, df = 14, p-value = 4.191e-07 Look at what R tells us here. First, it reports that it will be giving us the results of a Pearsons Chi-squared test. Second, it repeats the command that we entered. Third, it reports the value for chi-squared (which it writes as X-squared), tells us our degrees of freedom (calculated by (number of rows-1)*(number of columns-1)) and reports our p-value. Lower p values mean that the relationship that we are observing is more likely to be statistically significant (i.e. not due to random sampling error). In this case, our p-value is very low: 4.191*10-7, or .0000004191, which means that, according to the chi-squared test, this relationship is statistically significant (because 4.191*10-7 is lower than .05, our conventional benchmark). However, look at warning at the top: R is warning us that this estimate may be wrong. Why? This has to do with the expected values of the cells in our crosstab. Lets take another look at the crosstab that we generated earlier: ## Religion ## gini3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath Sum ## equal 1 1 0 16 8 0 7 12 45 ## somewhat equal 8 0 1 20 2 3 5 7 46 ## not equal 0 0 0 4 1 6 5 28 44 ## Sum 9 1 1 40 11 9 17 47 135 Look at top left cell: countries where Eastern religions are dominant and that have economic equality. There is one country in that category. If there were no relationship at all between a countrys religion, we would expect that cell to be the row total*the column total/the total number of cases in the table. So, (45*9)/135, which is 3. As a general rule, if some cells in a crosstab have an expected value of less than 5, value of the chi-squared test will be artificially high, and we will be more likely to make Type I error (rejecting a true null hypothesis). In this case, we can actually look at the expected values of all of our cells with this command: chisq.test(xtabs(~gini3+Religion, data=world))$expected ## Warning in chisq.test(xtabs(~gini3 + Religion, data = world)): Chi-squared approximation may be incorrect ## Religion ## gini3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath ## equal 3.000000 0.3333333 0.3333333 13.33333 3.666667 3.000000 5.666667 15.66667 ## somewhat equal 3.066667 0.3407407 0.3407407 13.62963 3.748148 3.066667 5.792593 16.01481 ## not equal 2.933333 0.3259259 0.3259259 13.03704 3.585185 2.933333 5.540741 15.31852 Looking over this output, we can see that we have many cells with expected values below 5, and thus our estimated chi-squared is less helpful. 7.3 Lambda and the PREs One way to address the deficiency in chi-squaredthat chi-squared is less accurate when the expected values of some of our cells is quite smallis with Lambda. Lambda is the first of several Proportional Reduction in Error (PRE) statistics that we will be learning about in class. PRE statistics produce a value between 0 and 1 or between 0 and |1| (in the case of PRE statistics that let us look at the direction of relationships). These statistics are useful in interpreting both the significance and strength of relationships. A PRE statistic with a value of 0 means that knowing the independent value has no impact on our ability to correctly predict the value of the dependent variable, while a PRE statistic with a value of 1 (or -1, in the case of a negative relationship) means that knowing our independent variable will allow us to perfectly predict the value of our dependent variable (these kinds of perfect relationships do not exist in actual political science). A PRE statistic of .35, for example, tells us that knowing our independent variable improves our ability to guess the dependent variable by a probability of .35 (or 35%) compared with how good our guess would be if we did not know the independent variable. Here is a general rule of thumb when interpreting PRE-statistics on social science data (this is based on Pollock and Edwards 2019, 225): Range of values Strength of relationship 0 and .1 Weak relationship 0 and -.1 Weak relationship .1 and .2 Moderate relationship -.1 and -.2 Moderate relationship .2 and .3 Moderately strong relationship -.2 and -.3 Moderately strong relationship Above .3 or below -.3 Strong relationship We can ask R to calculate Lambda in a way similar to the way that we had R calculate chi-square: Lambda(xtabs(~gini3+Religion, data=world)) ## [1] 0.2824859 In other words, R is telling us that knowing the religion of a country improves the probability that we will correctly guess the level of economic equality in that country by about .28 (or 28%). That is a moderately strong relationship. However, that statistic alone does not tell us about statistical significance (or the chance that the relationship that we observe is due to random sampling error). To fix that, we can add conf.level=.95 to our command from above (we could also set the confidence level to .99, or .9, or something else, but .95 is the standard in political science). This is our new command: Lambda(xtabs(~gini3+Religion, data=world), conf.level=.95) ## lambda lwr.ci upr.ci ## 0.2824859 0.1715779 0.3933939 While this output does not directly report a significance level, unlike the t-tests and chi-squared tests that we ran earlier, the confidence interval that it reports is extremely helpful. This tells is that there is a 95% chance that the true value of Lambda is somewhere between .17 and .39 (which would be a very strong relationship). In other words, we are pretty confident that this relationship is moderate or moderately strong. If this reported interval had included 0, we would have concluded that the relationship was not statistically significant. 7.4 Cramers V Lambda sometimes underestimates the strength of a relationship when there is little variation between the overall mode of a sample and the modes of each of our groups. In that case, we can use Cramers V instead. Or, given how easy it is to ask R to calculate these statistics, we can actually ask R to calculate chi-squared, lambda, and Cramers V, and if they provide similar information, that is a good sign that there might be something to our hypothesis. Unlike Lambda, Cramers V is not a PRE statistic, which means that we cant interpret it in terms of improved probability of guessing our dependent variable with knowledge of our independent variable. However, like Lambda, it reports a statistic between 0 and 1, and higher values mean that we have a stronger relationship. We can also use the same  conf.level=.95 qualifier to get a confidence interval around our estimate, like this: CramerV(xtabs(~gini3+Religion, data=world), conf.level=.95) ## Cramer V lwr.ci upr.ci ## 0.4587703 0.2721846 0.5290952 Seeing this confidence interval with a lower bound nowhere near zero gives us additional evidence that there is likely a relationship between religion and economic equality at the population level. 7.5 Somers D All of the measures of association that we have looked at so far have been useful for thinking about the relationship between two ordinal or nominal variables. But if we have two ordinal variables, we can also start to think about the direction of our relationship. In other words, we can go beyond asking whether knowing our independent variable helps us predict our dependent variable and ask whether an increase in our independent variable leads to an increase (or decrease) in our dependent variable. This kind of question does not make sense when discussing nominal variables, such as marital status. In other words, it does not make sense to ask whether marital status increasing leads to income increasing, because there is no order in which the values for the marital status variable (divorced, married, never married, separated, and widowed) must be listed. Lets focus on two ordinal variables in the anes2020 dataframe as an example: freedomofpress and trustmedia. The freedomofpress variable is an answer to the question How important is it that news organizations are free to criticize political leaders? Respondents were given 5 choices ranging from extremely important to not important at all. In other words, this variable is a measure of respondents general commitment to the principle of freedom of the press. The trustmedia variable is an answer to the question In general, how much trust and confidence do you have in the news media when it comes to reporting the news fully, accurately, and fairly? Respondents were given five choices ranging from none to a great deal. My theory is that people with more commitment to the principle of freedom of the press might therefore trust the media more, because commitment to the principle of freedom of the press suggests that a respondent believes in the potential for the press to do good work, and is thus more likely to see the press doing good work and therefore trust the press. Before we can do our statistical tests of this theory, we can take a look the data in graphics and crosstabs to see whether the data seems to support our theory. We can use the following two commands to generate a crosstab and a table of the columns percentages (notice that I put the dependent variable, trustmedia, first in these commands): xtabs(~trustmedia+freedomofpress, data=anes2020) colPerc(xtabs(~trustmedia+freedomofpress, data=anes2020)) One tip before we move on: R is almost always willing to create an object so that you dont have to keep typing the same things over and over. So, in this case, you can make your crosstab an object called media.table like this: media.table&lt;-xtabs(~trustmedia+freedomofpress, data=anes2020) Then, to see what that table looks like, you can run this line of code: media.table ## freedomofpress ## trustmedia extremely important very important moderately important a little important not important at all ## none 594 449 579 231 527 ## a little 531 417 485 210 178 ## some 989 554 578 124 97 ## a lot 804 245 115 27 18 ## a great deal 326 59 41 19 18 After that, to generate the table with column percents, you can go like this: colPerc(media.table) ## freedomofpress ## trustmedia extremely important very important moderately important a little important not important at all ## none 18.31 26.04 32.20 37.81 62.89 ## a little 16.37 24.19 26.97 34.37 21.24 ## some 30.49 32.13 32.15 20.29 11.58 ## a lot 24.78 14.21 6.40 4.42 2.15 ## a great deal 10.05 3.42 2.28 3.11 2.15 ## Total 100.00 100.00 100.00 100.00 100.00 Lets look at the second table, with the percents. This is telling us that among those who think that freedom of the press is extremely important, 18.31% have no trust in the media. However, among those who do not think that freedom of the press is important at all, 62.89% have no trust in the media. That seems like a pretty strong piece of evidence for our theory! We have a number of choices for how to represent this relationship graphically. Since we reviewed a stacked area graph earlier in this chapter (which would be a cool way of looking at this relationship), for this section lets generate a bar graph with error bars around the 95% confidence interval for one of the values of our dependent variable. We already did this in section 6.6, but here is the code that will help us generate a graph for this data: plotting.data&lt;-anes2020 %&gt;% filter(!is.na(trustmedia)) %&gt;% filter(!is.na(freedomofpress)) %&gt;% group_by(freedomofpress) %&gt;% summarise( n=n(), numerat=sum(trustmedia==&quot;none&quot;)) %&gt;% mutate(proportion=numerat/n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[1]) %&gt;% rowwise() %&gt;% mutate(upper_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[2]) ggplot(plotting.data) + geom_bar(aes(x=freedomofpress, y=proportion), stat=&quot;identity&quot;, fill=&quot;purple&quot;, alpha=0.5) + geom_errorbar(aes(x=freedomofpress, ymin=lower_ci, ymax=upper_ci), width=0.4, colour=&quot;black&quot;, alpha=0.9, size=1.5) + ggtitle(&quot;Importance of press freedom and trust in the media, ANES 2020&quot;, subtitle=&quot;Error bars represent 95% confidence intervals&quot;)+ xlab(&quot;How important is press freedom?&quot;)+ ylab(&quot;Proportion with no trust in media&quot;) Notice that the bars are going in the way our theory predicted  people that think that press freedom is less important also seem to trust the press less. The error bars on these columns do not seem to overlap with the exception of the upper bar on the moderately important column and the lower bar in the a little important column. In other words, most of these differences seem to be statistically significant, and the relationship is heading in the direction that we predicted. However, what are we to make of those overlapping error bars? And what about all of the data that is not on this graph (i.e. all four other values on media trust)? We can run the tests that we learned about earlier this chapter on our crosstab with the following commands. First, if you didnt generate the crosstab object before, you can do that now like this: media.table&lt;-xtabs(~trustmedia+freedomofpress, data=anes2020) Now you can simply execute these three commands: chisq.test(media.table) ## ## Pearson&#39;s Chi-squared test ## ## data: media.table ## X-squared = 1350.3, df = 16, p-value &lt; 2.2e-16 Lambda(media.table, conf.level = .95) ## lambda lwr.ci upr.ci ## 0.04627059 0.03725581 0.05528537 CramerV(media.table, conf.level = .95) ## Cramer V lwr.ci upr.ci ## 0.2027150 0.1907419 0.2124269 These three tests all show a statistically significant relationship. The reported Lambda is pretty weak, but this could be because of the tendency of Lambda to sometimes underestimate the strength of relationships when the subgroup modes are too similar to the sample mode (if you look at the crosstab with percents above you will see that at four of the five levels of our independent variable, the modal respondent reports no trust in the media, which is also the sample mode). The statistics that we just calculated let us discuss the strength (in the case of Lambda) and significance (in the case of Cramers V and chi-squared) of the relationship that we are interested in, but how about the direction? To examine the direction of a relationship between two ordinal variables, we can use a Somers D test. Somers D is a Proportional Reduction in Error (PRE) statistic like Lambda. Unlike Lambda, however, which ranges from 0 to 1, Somers D can range from 0 to 1 OR -1. A perfect negative relationship, where knowledge of the independent variable lets us guess the value of the dependent variable with absolute certainty, would generate a Somers D score of -1, while a perfect positive relationship would generate a Somers D score of 1. A Somers D score of 0 means that knowing the independent variable gives us no insight at all in predicting the dependent variable. Before we run Somers D, we need to see whether our hypothesis is predicting a positive or a negative relationship. What does it mean to increase or decrease on those two variables? For this, we can use the levels() command, like this: levels(anes2020$freedomofpress) ## [1] &quot;extremely important&quot; &quot;very important&quot; &quot;moderately important&quot; &quot;a little important&quot; &quot;not important at all&quot; levels(anes2020$trustmedia) ## [1] &quot;none&quot; &quot;a little&quot; &quot;some&quot; &quot;a lot&quot; &quot;a great deal&quot; The levels() command only works with ordered factors (which is how R treats ordinal variables), and the output lists the scores of the variable from low to high. So, the lowest value on freedomofpress mean that the respondent thinks that freedom of the press is extremely important, while the highest value means that the respondent thinks that freedom of the press is not important at all. Increasing values means less respect for freedom of the press. With trustmedia, the lowest value means that the respondent has no trust in the media, while the highest value means that the respondent has a great deal of trust in the media. Increasing values means more trust in the media. Since our hypothesis is that more respect for freedom of the press leads to more trust of the media, we thus hypothesize a negative relationship between our two variables. We run Somers D just like how we ran Cramers V, Lambda, and chi-squared, above: SomersDelta(xtabs(~trustmedia+freedomofpress, data=anes2020), conf.level=.95) ## somers lwr.ci upr.ci ## -0.3069552 -0.3239353 -0.2899751 If you made the media.table crosstab object above, you can also run Somers D on that, like this: SomersDelta(media.table, conf.level=.95) This is telling us that there is a .95 probability that Somers D at the population level is between -.32 and -.29. In other words, we are pretty confident that there is a strong (or moderately strong), significant, and negative relationship between belief in freedom of the press and trust in the media. One note: Somers D is an asymmetric test, which means that it gives different results when you treat one of your two variables as independent than when you treat that same variable as dependent. So, be careful to list your dependent variable first when setting up your test. 7.6 Review of this chapters commands Command Purpose Library chisq.test() Runs a chi-squared test on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command, but you can also use it on a table that you generate with other commands or on a pair of variables that you define with a $. Base R Lambda() Runs a Cramers V test (a test of the relationship between two nominal or ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools CramerV() Runs a Lambda test (a PRE test of the relationship between two nominal or ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools SomersDelta() Runs a Somers D test (a PRE test of the relationship between two ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools 7.7 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 7 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse, Hmisc, scales, tigerstats, and DescTools libraries. Choose an interval variable from either anes2020, states2010, or world that you want to think about as a dependent variable (dont choose one of the variables that we used as an example in this chapter). Use cut2 to convert that variable into an ordinal variable with at least three values. Using mutate(recode()), label the values of that variable in a way that makes sense. Generate a graphic to show that variables central tendency and dispersion (using the technique from section 3.2). Create a new file in your word processor. Paste the graphic that you generated from #2 into your word processor file. In a few sentences, discuss the central tendency and dispersion of that variable. Choose a second ordinal or interval variable from the same dataset that you used for #2. If the variable is interval, use cut2 to make it an ordinal variable, and use mutate(recode()) to label the values in a way that makes sense. In your word processor file, briefly generate and discuss a hypothesis. What do you expect to see in this relationship and why? Using xtab() and colPerc(xtabs)), generate crosstabulation relationships between these two variables. Graph the relationship between these two variables using the techniques that we reviewed in section 7.5 of todays chapter or using a stacked bar graph that we reviewed in section 7.2 of todays chapter. Paste your graph into your word processor, making sure that the labels make sense. Conduct a chi-squared test, a Lambda test, a Cramers V test, and a Somers D test of this relationship, and paste the key output into your word processor file. Based on the output from #8, write a few sentences about what you conclude about your hypothesis and why? Is the relationship statistically significant? Is it strong? It is ok if your results do not support your hypothesis. That is how science works! References "],["pearsons-r.html", "Chapter 8 From Pearsons r and linear regression 8.1 Getting started with this chapter 8.2 Pearsons R 8.3 The scatterplot 8.4 Bivariate linear regression 8.5 Review of this chapters commands 8.6 Review exercises", " Chapter 8 From Pearsons r and linear regression 8.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now type install.packages(\"ggrepel\") into the Console. This new package will help us label our graphs in a way that is easier to read. Now, open a new script file and save it in your scripts folder as chapter 8 practice. Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 8, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(ggrepel) Finally, select all the text on this page, run it, and then save it. 8.2 Pearsons R What happens when we are interested in the relationship between two interval variables? Up into now, our solution has been to convert those variables into ordinal variables and use chi-squared, Lambda, Cramers V, and/or Somers D. However, when we do that, we are throwing out information. For example, if we convert an interval measure of respondents age into an ordinal variable coded young, middle aged, and old, we are throwing out the distinctions between, for example, 30 and 31-year-olds (assuming that they both fall into our young category) and between 80 and 81-year-olds (assuming that they both fall into our old category). Pearsons R is a test that lets us look at the relationship between two interval variables. It produces a statistic that ranges from -1 to 1, with negative numbers indicating a negative relationship and positive numbers indicating a positive relationship. Although, unlike Lambda and Somers D, it is not a Proportional Reduction in Error (PRE) statistic, values farther from 0 indicate stronger relationships, and values closer to 0 indicate weaker relationships (with 0 meaning no relationship). R makes it very easy to calculate Pearsons R. You can simply use the command cor.test(). Below, I test the relationship between two variables that I have found in the states2010 dataframe: citi6013, which is a measure of citizen ideology ranging from 0 to 100 (with higher numbers meaning more liberal and lower numbers being more conservative) and pctfemaleleg (a measure of the percent of a states legislators that are women). My hypothesis is that in states where the population is more liberal, there will be more female legislators. In other words, I am hypothesizing that Pearsons R will be positive. Here is the command: cor.test(states2010$pctfemaleleg, states2010$citi6013) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 4.288, df = 48, p-value = 8.655e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2904799 0.7018161 ## sample estimates: ## cor ## 0.5262759 Pearsons R is a symmetrical test, which means that it produces the same value regardless of which variable you treat as independent or dependent. So, it does not matter which order you enter the variables into a cor.test() command. This is Rs Pearsons R output. R calls this Pearsons product-moment correlation, which is just another way of saying Pearsons R. Look at the bottom number of this output: 0.527. That number is positive, which is consistent with my hypothesis. It also seems relatively far for zero, although as I noted above, that is more difficult to interpret with Pearsons R because it is not a Proportional Reduction in Error statistic. The p-value noted at the top ofthe output is 8.655e-05, which is Rs shorthand for 8.655x10-5, which is another way of saying 0.00008655. That is the probability that we would get a sample like this from a population where there was not correlation between our two variables. Since it is (much) lower than 0.05, we can conclude that this relationship is statistically significant. We can also conclude that by looking at the 95% confidence interval around our estimated Pearsons R value. R reports a confidence interval of 0.29 to 0.7, which does not include 0. Thus, we are 95% confident that there is a positive correlation between our two variables at the population level. 8.3 The scatterplot To visualize a relationship between two interval variables, we can generate a scatterplot, one of my favorite graphs to generate. Here is the code that we can use to visualize the relationship between those two variables above. In general, consistent with other graphs you have made, remember to put your independent variable on the x-axis and your dependent variable on the y-axis (as always, you can find a template for this graph in the RStudio code file called 20093 ggplot2 templates.r. ggplot(states2010, aes(x = citi6013, y = pctfemaleleg)) + geom_point(alpha=.3)+ geom_text_repel(aes(label = state), size=3)+ #if you dont have data that you want to #use to name your dots, leave this line #off. labs(x = &quot;Ideology of citizenry (higher numbers mean more liberal)&quot;, y = &quot;Percent of state legislators that are women&quot;)+ ggtitle(&quot;Citizen ideology and female representation in state legislatures, 2010&quot;) ## Warning: Removed 1 rows containing missing values (geom_point). ## Warning: Removed 1 rows containing missing values (geom_text_repel). The neat thing about scatterplots is that they display the values of all of the cases of the two variables that you are interested in for all of your data. So, looking at this graph, we can see dots representing all 50 states. How far each dot is along the x-axis tells us how liberal their citizenry is, and how far each dot is along the y-axis tells us the percent of their state legislators that are women. We are also able to label our dots. This is something that is possible when you are comparing a relatively small number of units that you are able to identify in a way that makes sense to your readers. Labeling dots usually doesnt make sense with survey data, such as what we find in the ANES dataframe, because there are many thousands of individuals who took that survey, and they took it anonymously. The above graph alone tells a pretty clear story about the relationship between our two variables  it is easy to visualize a positive relationship going from Oklahoma in the bottom left to Vermont in the top right. However, what if we wanted to ask R to fit a line to this data? In other words, what if we wanted R to find the line that is closest to all of the points on this graph? To do that, we can add a single line to our code from above: ggplot(states2010, aes(x = citi6013, y = pctfemaleleg)) + geom_point(alpha=.3)+ geom_smooth(method=&#39;lm&#39;, formula= y~x)+ #this is the line of code we add here geom_text_repel(aes(label = state), size=3)+ labs(x = &quot;Ideology of citizenry (higher numbers mean more liberal)&quot;, y = &quot;Percent of state legislators that are women&quot;) ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). ## Warning: Removed 1 rows containing missing values (geom_text_repel). The line on this graph is called a regression line, and it is the line that is as close as possible to all of the dots on this graph. The shaded region around it is a 95% confidence interval. Visualize drawing a line from the top left corner of that region to the bottom right corner. If that line that you are visualizing was horizontal or sloping down, then we could not rule out the possibility that there was no relationship or a negative relationship between the two variables that we are looking at there. But even that imaginary line slopes upwards, which is good evidence for our hypothesis. 8.4 Bivariate linear regression As I mentioned in the last section, the line that goes through the graph that we just generated is called a regression line. Lets now spend a few minutes reviewing some of what we know about lines from algebra. This is the formula for a line: Y=MX+B X is our independent variable, and Y is our dependent variable. M is the slope of our line. In other words, a one unit increase in X leads to an M unit increase in Y (or, if M is negative, a one unit increase in Y leads to an M unit decrease in Y). B is the Y intercept of our line. When X is zero, B will be the value of our line. When we ask R to draw a regression line on a scatterplot, we are asking R to first find the line that most closely fits all of the points in our scatterplot, and then generate a formula for that line. To generate that formula, R needs to find the slope and the intercept. To ask R to report what slope and intercept it found, we can use this command: lm(formula = pctfemaleleg ~ citi6013, data = states2010) ## ## Call: ## lm(formula = pctfemaleleg ~ citi6013, data = states2010) ## ## Coefficients: ## (Intercept) citi6013 ## 13.2388 0.2326 In this code, we must put the dependent variable first, and then the independent variable second. The lm stands for linear model (because we are asking R to generate the formula for a line). This is the most basic form of regression, an extremely powerful family of statistical techniques that are used across the social and natural sciences. This most basic form of regression is called Ordinary Least Squared, or OLS regression, because calculating it involves find the line with the minimum squared distance between each point in a scatterplot and that line. Looking at the output, R is giving us two numbers: they are Rs estimates for the slop and the intercept in the formula Y=MX+B. Recall that our independent variable is the ideology of a states population, and our dependent variable is the percent of female legislatures in a state. So, we can rewrite the Y=MX+B formula with those variables, like this: %female legislators=M(state ideology) + B And now we can replace M with the slope (which is right under citi6013, the ideology measure on our output) and B with the intercept (which is right number the world intercept in our output): %female legislators=0.23(state ideology) + 13.24 This formula is telling us two things. First, it is telling us that for every one unit increase in our independent variable (our state ideology scale with higher numbers meaning more liberal), the percent of female legislators increases by 0.23. Second, it is telling us that in a state where our state ideology scale was 0 (meaning the residents were as conservative as possible), 13.24% of the state legislators would be women. Now we know, looking at the second graph in section 8.3, that this line doesnt perfectly fit the data. Some points are right on the line, like Florida, but others are far above the line, like Colorado, or far below the line, like South Carolina. In order to get a sense of how well our regression actually fits the data, we can ask R to summarize our regression, like this: summary(lm(formula = pctfemaleleg ~ citi6013, data = states2010)) ## ## Call: ## lm(formula = pctfemaleleg ~ citi6013, data = states2010) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5990 -3.7725 -0.5453 3.0920 14.0931 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.23882 2.69136 4.919 1.06e-05 *** ## citi6013 0.23255 0.05423 4.288 8.66e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.904 on 48 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.277, Adjusted R-squared: 0.2619 ## F-statistic: 18.39 on 1 and 48 DF, p-value: 8.655e-05 There is a lot in this output, but there are a few things that you should focus on. First, look at the estimated slope and intercept. Those are the same as above, when we just did the lm() command without summary(). Second, look at the column that says Pr(&gt;|t|). That is the significance level of the coefficient and the intercept. In other words, that is the probability that we would see a sample like this from a population where the intercept was 0 and where the slope was 0. The significance level of the intercept is less important, but lets think about the significance level of the slope. If the slope were 0, then whatever happened to X, Y would not change. Lets look at the formula to remember why: %female legislators=M(state ideology) + B If our slope (M) were 0, then the estimated percent of female legislators would be B whatever the states ideology was. And thus, there would be no relationship between state ideology and percent of female legislators. So, if we can say that it is unlikely that the true slope of our line is 0 (because the p-value is below the level of risk of error that we are willing to take, usually set to .05 in political science), then we can conclude that there is likely to be a relationship between our two variables. Since the p-value of our slope is 8.66*10-5, we can conclude that we are pretty unlikely to get this kind of data from a population with no relationship between our independent and dependent variables. Therefore, we can conclude that this relationship is statistically significant. There is one final aspect of this output that we should discuss: the \\(R^2\\) and adjusted \\(R^2\\) values. \\(R^2\\) is a Proportional Reduction in Error (PRE) measure of the strength of a regression relationship. It ranges from 0 to 1, and it tells us the proportion of variation in our dependent variable that is explained by our independent variable. Because \\(R^2\\) values are often a little bit inflated for some statistical reasons that we dont need to get into in this chapter, it is generally better to use the adjusted \\(R^2\\) to interpret regression results. So, in this case, an adjusted \\(R^2\\) of .2619 tells us that variation in ideology can account for about 26.19% of variation in the percent of female legislators in a state. 8.5 Review of this chapters commands Command Purpose Library cor.test() Generates a Pearsons R statistic for the relationship between two variables. Variables can be listed in any order and should be written dataframe$variable. Base R lm(formula = DV ~ IV, data = DATAFRAME) Generates the slope and intercept for a regression line. Replace DV with the dependent variable you are interested in, the IV with the independent variable, and DATAFRAME with the dataframe that you are using. Base R summary(lm()) Generates much more data about a regression relationship than the above command. Replace the  with the IV, DV, and DATAFRAME the same way that you would with the above command. Base R 8.6 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 8 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse and ggrepel libraries. Choose an interval variable from either states2010 or world (NOT ANES) that you want to think about as a dependent variable (dont choose one of the variables that we used as an example in this chapter). Generate a histogram and statistics to help you visualize and discuss the central tendency and dispersion of this variable. Create a new file in your word processor. Paste the graphic that you generated from #2 into your word processor file. In a few sentences, discuss the central tendency and dispersion of that variable. Choose a second interval variable from the same dataset that you used for #2. In your word processor file, briefly generate and discuss a null and research hypothesis. What do you expect to see in this relationship and why? Use cor.test() to calculate a Pearsons R value for this relationship. Paste the output into your word processor file, and interpret it in a few sentences. What can you conclude about this relationship and why? Make a scatterplot of the relationship between these two variables using the techniques that you learned in section 8.3. Make sure that your your graph has a regression line and that the points are labelled. Paste your graph into your word processor. Run a linear regression of the relationship between the two variables that you are looking at in todays analysis. Paste the output into your word processor. In a few sentences, interpret the estimated intercept, coefficient, significance levels, and adjusted \\(R^2\\). "],["regression.html", "Chapter 9 Multiple regression 9.1 Getting started with this chapter 9.2 Multiple regression 9.3 Stargazer output 9.4 Dummy variables in multiple regression 9.5 Graphing multiple regression 9.6 Review of this chapters commands 9.7 Review exercises", " Chapter 9 Multiple regression 9.1 Getting started with this chapter To get started in todays chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now type install.packages(\"stargazer\") into the Console (Stargazer is designed by Hlavac (2022)). This new package will help us label our graphs in a way that is easier to read. Next, open a new script file and save it in your scripts folder as chapter 9 practice. Write this on the page: #################################### # Your name # 20093 Chapter 9, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(ggrepel) library(stargazer) Now select all the text on this page, run it, and then save it. Finally, select all the text on this page, run it, and then save it. 9.2 Multiple regression In the last chapter, we learned how to do bivariate ordinary least squared (OLS) regression. This is an extremely powerful tool for statistical inference because it both provides a proportional reduction in error estimate (\\(R^2\\)), which tells us exactly how much of the variation in our dependent variable is explained by variation in our independent variable, and it provides us with a coefficient estimate, which tells us exactly what we expect to happen to our dependent variable with every one unit increase in our independent variable. But there is one more reason that regression is a tool beloved by scientists. It provides an easy way to control for many different independent variables at the same time. Recall that in Chapter 8 we discussed how when R is running a regression it is estimating the values of M and B in the formula Y=MX+B. Another way that we can write that same formula is like this: \\[Y=_0+ _1X_1\\] In this version of the formula, we call the intercept \\(_0\\) ( is the Greek letter Beta) instead of B, and we call the slope \\(_1\\) instead of M. When conducting bivariate regression, we are asking R to give us the values of \\(_0\\) (the intercept, or B) and \\(_1\\) (the slope, or M) that best fits our data. When we want to conduct multiple regression, we can use the same principle, but we just have to add more independent variables and s, like this: \\[Y=_0+ _1X_1 + _2X_2 + _3X_3++ _nX_n\\] Multiple regression results include an estimate for all of those s. The estimates are called partial regression coefficients, which is generally shortened to coefficients. And they are very powerful! In the above equation, Rs estimated value for \\(_1\\) tells us what we expect to happen to our dependent variable, Y, when we increase \\(X_1\\) by one unit, and when we hold \\(X_2\\), \\(X_3\\), and all other independent variables in our equation constant. Thus, if someone were to say Hey, I think that the relationship that you observe between \\(X_1\\) and Y is spurious; variation in \\(X_2\\), which happens to be correlated with \\(X_1\\), is the actual cause of variation in Y! You can reply, in this multiple regression, I controlled for \\(X_2\\) (in other words, I looked at the relationship between \\(X_1\\) and Y at similar values of \\(X_2\\)) and I still found a meaningful relationship between \\(X_1\\) and Y. Thus, I can rule out spuriousness! Lets try an example. We will start with a bivariate regression analysis. Under the Trump administration, feelings on the federal agency Immigration and Customs Enforcement (ICE) became pretty politicized. So, I hypothesize that people that feel more warmly about the Democratic Party will likely feel more coldly about ICE (and vice versa). To test this hypothesis, we can run the following regression: lm(formula = ft_ice ~ ft_dems, data=anes2020) ## ## Call: ## lm(formula = ft_ice ~ ft_dems, data = anes2020) ## ## Coefficients: ## (Intercept) ft_dems ## 69.8606 -0.4475 This output helps us write the equation for the relationship between feelings on the Democratic Party and feelings in ICE: ft_ice = 69.8606 + -0.4475 (ft_dems) So in other words, R is estimating that someone who gives Democrats a score of 0 would give ICE a 69.86, and when feelings about the Democratic Party increase by 1, feelings about ICE decrease by 0.45. This is consistent with my theory. However, are these estimates statistically significant? And how much variation in feelings on ICE will our model explain? We could just put summary() around the command that we entered above, but instead we are going to try something a little different. We are going to call the regression of the relationship between feelings on Democrats and feelings on ICE model.1 and then ask R to summarize model.1, like this: model.1&lt;-lm(formula = ft_ice ~ ft_dems, data=anes2020) summary(model.1) ## ## Call: ## lm(formula = ft_ice ~ ft_dems, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -69.861 -19.651 1.467 18.181 74.894 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 69.860595 0.543358 128.57 &lt;2e-16 *** ## ft_dems -0.447543 0.009737 -45.96 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.29 on 6969 degrees of freedom ## (1309 observations deleted due to missingness) ## Multiple R-squared: 0.2326, Adjusted R-squared: 0.2325 ## F-statistic: 2112 on 1 and 6969 DF, p-value: &lt; 2.2e-16 This output shows us that the estimates of the slope and intercept are both statistically significant, and that the adjusted \\(R_2\\) is .2325, which means that variation in feelings about the Democratic Party explains about 23.25% of variation in feelings about ICE. There are many variables which are likely to be correlated with feelings about Democrats which might actually explain the variation in feelings about ICE. To start, lets look at our age variable, since we know that older people are more likely to be conservative than are younger people. When we control for age, is the relationship between feelings about the Democratic Party and ICE still statistically significant? To help us answer this question, we need to have R estimate \\(_1\\) and \\(_2\\) in this formula: ft_ice=\\(_0\\) + \\(_1\\)ft_dems + \\(_2\\)age We can do that with this command: model.2&lt;-lm(formula = ft_ice ~ ft_dems+age, data=anes2020) summary(model.2) ## ## Call: ## lm(formula = ft_ice ~ ft_dems + age, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -81.427 -17.172 0.951 17.502 88.915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.039775 1.085838 43.32 &lt;2e-16 *** ## ft_dems -0.441223 0.009521 -46.34 &lt;2e-16 *** ## age 0.429844 0.018050 23.81 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.27 on 6717 degrees of freedom ## (1560 observations deleted due to missingness) ## Multiple R-squared: 0.291, Adjusted R-squared: 0.2907 ## F-statistic: 1378 on 2 and 6717 DF, p-value: &lt; 2.2e-16 Looking at the Pr(&gt;|t|) column in this output, we can see that the intercept and both coefficient estimates are statistically significant, because the numbers are much, much lower than .05. The value of adjusted \\(R_2\\) is .2907, which means that variation in age and feelings about the Democratic Party explains about 29% of variation in feelings about ICE. We can also use the estimates column to estimate the s in the above equation, like this: ft_ice=47.04 - 0.44ft_dems + 0.43age This equation is telling us that, based on our data, R predicts that someone who gave the Democratic Party a 0 and was 0 years old (not a realistic condition, as is often the case with intercepts in multiple regression analysis) would give ICE a 47.04 out of 100. Holding their age constant and increasing their feelings about Democrats by 1 decreases their feelings about ICE by 0.44. Holding their feelings about Democrats constant and increasing their age by 1 increases their feelings about ICE by 0.43. In other words, we know that the partial relationship between feelings on Democrats and feelings about ICE, controlling for age, is not spurious. And we also know that the partial relationship between age and feelings about ICE, controlling for age, is also not spurious. 9.3 Stargazer output The output that we generated in the last section is pretty difficult to read, and would not look good in a paper. But, we have a nice solution  the stargazer package that we installed above! Once we have installed and loaded that package, we can invoke it like this: stargazer(model.1, model.2, type=&quot;html&quot;, out=&quot;lab9regressions.html&quot;, column.labels = c(&quot;Dems only&quot;, &quot;Dems and age&quot;), intercept.bottom = FALSE, single.row=FALSE, notes.append = FALSE, header=FALSE) This is telling R to make a table with regression results from both of our regressions and to save it as a file called lab9regressions.html. If you want to change the name, you can just change what you write in the quotes after out=. Just dont type any periods before .html, and keep the .html at the end. If you have more than two models that you want to run, you can just add them, but you have to add as many labels as you have models after column.labels. When you run this command, a bunch of gibberish will pop up in your consol. Dont worry about that. Just look in the main folder where you have your Scope and Methods labs project stored. There, you should see an html file called lab9regressions.html (unless you changed the name to something else). Open that file  it should open in your web browser. Highlight all of the text, copy it (using Ctrl-C on a Windows machine or Command-C on a Mac), and paste it into a word processor document. You should see something like this: Dependent variable: ft_ice Dems only Dems and age (1) (2) Constant 69.861*** 47.040*** (0.543) (1.086) ft_dems -0.448*** -0.441*** (0.010) (0.010) age 0.430*** (0.018) Observations 6,971 6,720 R2 0.233 0.291 Adjusted R2 0.233 0.291 Residual Std. Error 26.289 (df = 6969) 25.269 (df = 6717) F Statistic 2,112.484*** (df = 1; 6969) 1,378.138*** (df = 2; 6717) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 This is an extremely useful table, very much like what you see when you read many academic journal articles that use regression analysis. At the top, you see a title that identifies the dependent variable for all regressions in the table  feelings on ICE. The column on the left provides the variable names. You should generally go in and change variable names and the title to something that will be easy for your readers to understand. So, for example, instead of ft_ice, you might write feelings on ICE. The second column is the results of the first regression that we ran, with only feelings on Democrats as an independent variable (you can tell that the regression did not include the age variable because the spot where the estimated coefficient for age would go is blank). The constant is the intercept (they are the same thing). The third column is the results of the second regression analysis. If you look at the Adjusted \\(R^2\\) row, you can see how the Adjusted \\(R^2\\) changes as you add and subtract variables to the regression equation. When Adjusted \\(R^2\\) decreases as you add more independent variables, that means that whatever increase in explanatory power that you get from the new variables is more than offset by the loss in degrees of freedom that you take by adding more variables. 9.4 Dummy variables in multiple regression We often want to include nominal variables in regression analysis. For example, we want to know whether things like gender, religion, and race might cause variation in our dependent variable. However, since the values in a nominal variable can be listed in any order, it does not make sense to talk about increasing the value of a nominal independent variable. We get around this by converting a nominal independent variable into a series of dummy variables (variables coded 0, for when an attribute is absent, and 1, for when it is present). So, for example, the union membership variable in anes2020 is coded so that respondents get a 1 if they are in a union and a 0 if they are not. When including this in a regression, we would be estimating this equation: ft_ice=\\(_0\\)+ \\(_1\\)union When interpreting the results of this regression, for people not in unions, the value of the union variable is 0, and thus the estimated value of ft_ice would be \\(_0\\) (because \\(_1\\) multiplied by 0 equals 0). For people in unions, the estimated value of ft_ice would be \\(_0\\) + \\(_1\\) (because \\(_1\\) multiplied by 1 equals \\(_1\\)). Lets run this regression using this command: summary(lm(ft_ice~union, data=anes2020)) ## ## Call: ## lm(formula = ft_ice ~ union, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -49.932 -19.932 0.068 20.068 52.969 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.9322 0.3845 129.86 &lt; 2e-16 *** ## union -2.9010 1.0399 -2.79 0.00529 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.97 on 7035 degrees of freedom ## (1243 observations deleted due to missingness) ## Multiple R-squared: 0.001105, Adjusted R-squared: 0.000963 ## F-statistic: 7.782 on 1 and 7035 DF, p-value: 0.005291 This output tells us a few important things. First, it helps us fill out the equation from above, like this: ft_ice=49.32 + -2.9union In other words, R is estimating that the average non-union member gives ICE a 49.32, while the average union member gives R a 49.32-2.9, which is 46.42. And these estimates are both statistically significant. Second, the adjusted \\(R^2\\) value of this equation tells us that variation in union membership alone only explains about 0.096% of variation in feelings on ICE. Thus, there are likely a lot of major causes that are missing from this model. An important note: Lets take a minute to generate a dummy variable for non-union members, like this: anes2020&lt;-anes2020 %&gt;% mutate(nonunion=recode(union,&#39;1&#39;=0,&#39;0&#39;=1)) This variable is the inverse of our union variable. While the union variable is coded 1 for union members, the nonunion variable is coded as 1 for non-union members. If we include that variable in the regression model above, to estimate this equation: ft_ice=\\(_0\\)+ \\(_1\\)union+\\(_2\\)nonunion we are giving R an impossible task. That it because no case will have a value of 0 for both our union and non-union variables. If we ignore this impossibility, and force R to do it anyway, R does this: summary(lm(ft_ice~union+nonunion, data=anes2020)) ## ## Call: ## lm(formula = ft_ice ~ union + nonunion, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -49.932 -19.932 0.068 20.068 52.969 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.9322 0.3845 129.86 &lt; 2e-16 *** ## union -2.9010 1.0399 -2.79 0.00529 ** ## nonunion NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.97 on 7035 degrees of freedom ## (1243 observations deleted due to missingness) ## Multiple R-squared: 0.001105, Adjusted R-squared: 0.000963 ## F-statistic: 7.782 on 1 and 7035 DF, p-value: 0.005291 R essentially says no thank you. If you look at the row labeled nonunion, you will see 4 NAs. That is because R noticed that the union and nonunion variables were redundant with one another and left the nonunion variable out. Aside from that row, the rest of the output is the same as when we ran the regression with only the union variable. The union variable is a nominal variable with only two values: either each respondent is a union member or not. But it is still important to avoid including redundant variables in regression even when we are dealing with nominal independent variables with more than two values, such as race. If we enter table(anes2020$race), we can see that the race variable in ANES has six values: Asian or Pacific Islander, Black, Hispanic, Multiple Races, Native American, and White. Lets see what happens when we include that variable in a regression analysis with ft_ice as our dependent variable, like this: summary(lm(ft_ice~race, data=anes2020)) ## ## Call: ## lm(formula = ft_ice ~ race, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.349 -21.725 -1.725 18.275 59.630 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.349 1.949 24.293 &lt;2e-16 *** ## raceBlack -5.281 2.304 -2.292 0.0219 * ## raceHispanic -6.979 2.278 -3.063 0.0022 ** ## raceMultiple Races -3.538 2.768 -1.278 0.2013 ## raceNative American 7.000 3.136 2.232 0.0256 * ## raceWhite 4.376 1.992 2.196 0.0281 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.69 on 6988 degrees of freedom ## (1286 observations deleted due to missingness) ## Multiple R-squared: 0.0195, Adjusted R-squared: 0.0188 ## F-statistic: 27.8 on 5 and 6988 DF, p-value: &lt; 2.2e-16 What R has done here is converted our race variable into six dummy variables: one for each value in the original race variable, and estimated the intercept and coefficient for this equation: ft_ice=\\(_0\\)+ \\(_1\\)Black + \\(_2\\)Hispanic + \\(_3\\)MultipleRaces + \\(_4\\)NativeAmerican + \\(_5\\)White Notice that our original race variable had six categories, but there are only five dummy variables in the model. That is because if R included the sixth valueAsian or Pacific Islanderthen R would have put itself in a position where it was trying to estimate the ft_ice for a person would cannot exist in our data  someone with 0s on all 0 possible values for race. Thus, R chooses one category to be the reference category, and leaves that one out. Look back and the equation just before this paragraph. The intercept that R estimated (47.35) is the predicted value of feelings about ICE for someone who identified themselves as Asian or Pacific Islander. Since that is the reference category, that did not show up in our model, that is what R expects to find when all of the other dummy variables that it generated are set to 0. -5.281 is the expected differences between someone who is an Asian or Pacific Islander and someone who is Black; in other words, Blacks seem to have a lower estimation of ICE, on average, than Asians or Pacific Islanders. And the rest of the coefficients estimate the differences between the groups that they identify and Asian Pacific Islanders. R chose Asian or Pacific Islander as the reference category because it comes first in alphabetical order when looking at the values for the race variable. This would be useful analysis if we were writing a paper about the public opinion of Asians and Pacific Islanders. However, given that whites are the majority in our sample, and whites are possibly less likely to have negative experiences with ICE than those who identify with other racial groups, we might want to compare the various ethnic minority groups in this sample with whites. To do that, we would have to first make the race variable into a factor, like this: anes2020$race.factor&lt;-as.factor(anes2020$race) Next, we have to relevel the variable, telling R what we want the reference category to be, like this: anes2020$race.factor&lt;-relevel(anes2020$race.factor, ref=&quot;White&quot;) This will compare all the racial groups in this variable to whites. So, if we run a regression with ft_ice as our dependent variable and race.factor as our independent variable, like this: summary(lm(ft_ice~race.factor, data=anes2020)) ## ## Call: ## lm(formula = ft_ice ~ race.factor, data = anes2020) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.349 -21.725 -1.725 18.275 59.630 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.7248 0.4128 125.289 &lt; 2e-16 *** ## race.factorAsian or Pacific Islander -4.3757 1.9923 -2.196 0.0281 * ## race.factorBlack -9.6563 1.2960 -7.451 1.04e-13 *** ## race.factorHispanic -11.3551 1.2501 -9.083 &lt; 2e-16 *** ## race.factorMultiple Races -7.9134 2.0090 -3.939 8.26e-05 *** ## race.factorNative American 2.6245 2.4914 1.053 0.2922 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.69 on 6988 degrees of freedom ## (1286 observations deleted due to missingness) ## Multiple R-squared: 0.0195, Adjusted R-squared: 0.0188 ## F-statistic: 27.8 on 5 and 6988 DF, p-value: &lt; 2.2e-16 Now R is using the dummy variable for white as our reference category. So, interpreting this, we can see that R predicts that the average white person will give ICE a 51.72, the average Black persons score will be 4.38 points lower than that, the average Hispanic persons score will be 9.66 points lower than the average white person, etc. 9.5 Graphing multiple regression When you ask R to calculate a bivariate OLS regression, you are actually asking R to estimate the formula for a line. This is relatively easy to represent on a two-dimensional scatterplot. You can have R plot all of the points that it is using to estimate the line, as well as the line itself (like we did in Chapter 8). When you add a second independent variable, you are asking R to estimate the formula for a plane, which becomes harder to graph, and when you add a third, you are asking R to estimate the formula for a three-dimensional object, which is even harder to graph. Adding four or more independent variable makes it impossible to graph the whole relationship about which you are asking R to theorize. However, there are still ways to represent part of the relationship. For example, lets say that we have asked R to run this regression: summary(lm(ft_ice~ft_dem+race.factor, data=anes2020)) When we do this, we are asking R to estimate the intercept and coefficients in this equation: ft_ice=\\(_0\\)+ \\(_1\\)ft_dem + \\(_2\\)AsianPacificIslander + \\(_3\\)Black + \\(_4\\)Hispanic + \\(_5\\)MultipleRaces + \\(_6\\)NativeAmerican with white as the reference category for race. 9.5.1 Facets We can actually ask R to make a different scatterplot for every value of our nominal race variable, using the following code. ggplot(data=anes2020 %&gt;% filter(!is.na(sex2)), aes(x=ft_dems, y=ft_ice)) + geom_point(size=2, alpha=.5, color=&quot;purple&quot;) + theme_bw()+ #add axis labels labs(title = &quot;ICE and the Democratic Party&quot;, subtitle=&quot;ANES2020&quot;, x=&quot;Feeling thermometer about the Democratic Party&quot;, y=&quot;Feeling thermometer about ICE&quot;, color=NULL)+ #add regression line geom_smooth(method=&#39;lm&#39;, color=&quot;BLACK&quot;)+ #make different graphs for every value of a nominal or ordinal variable facet_wrap(~race.factor) This graph shows us the relationship between feelings about the Democratic Party and feelings about ICE at all values of our race variable. Note that the relationship that we observe in the zero-order regression persists for all values of our control except for Blacks, for whom feelings about the Democratic Party do not appear correlated with feelings about ICE. 9.5.2 Color coding dots We could also represent this same kind of relationship by color coding the dots. This would be a little bit difficult to read with the race variable, since there are six different races in the dataframe, but lets try with a variable with only two value: the sex variable. The sex variable is currently coded as a dummy variable, with 0 meaning male and 1 meaning female. Since R will use those labels on the graph that we generate, lets make a new variable called sex2 which is labeled male and female, like this: anes2020&lt;-anes2020 %&gt;% mutate(sex2=recode(sex, &#39;0&#39;=&quot;male&quot;,&#39;1&#39;=&quot;female&quot;)) Now, we can generate a graph with this code (which you can also find in the 20093 ggplot2 templates file that you downloaded in Lab 1 or that you can access here): ggplot(data=anes2020 %&gt;% filter(!is.na(sex2)), aes(x=ft_dems, y=ft_ice,color=sex2)) + geom_point(size=2, alpha=.5) + theme_bw()+ #add axis labels labs(title = &quot;ICE and the Democratic Party&quot;, subtitle=&quot;ANES2020&quot;, x=&quot;Feeling thermometer about the Democratic Party&quot;, y=&quot;Feeling thermometer about ICE&quot;, color=NULL)+ #add regression line geom_smooth(method=&#39;lm&#39;) Note that this graph has dots colored according to their value on our sex2 variable, and it also has estimated two regression lines  one for men and one for women. We can see that, for people that dislike the Democratic Party, men feel more positively about ICE than women overall, but that difference seems to disappear as feelings about the Democratic Party increase. 9.6 Review of this chapters commands Command Purpose Library stargazer() Use this code to generate a beautiful table displaying the results of one or more regressions. stargazer as.factor() Changes a character variable to a factor. Necessary when you want to set a different reference category when including a nominal variable in regression analysis. Base R relevel(, ref=) Changes the reference category of a factor. Necessary when you want to set a different reference category when including a nominal variable in regression analysis. Base R 9.7 Review exercises Lets practice some of the things that we learned in this chapter. Create a new R script called Chapter 9 Exercises, and save it to your R scripts folder. Make a header for the script based on the description from section 2.3. Use the library() command to load the tidyverse and stargazer packages. Choose an interval variable from states2010, anes2020 (not age!) or world that you want to think about as a dependent variable. Generate a histogram and statistics to help you visualize and discuss the central tendency and dispersion of this variable. Create a new file in your word processor. Paste the graphic that you generated from #2 into your word processor file. In a few sentences, discuss the central tendency and dispersion of that variable. Choose a second interval variable from the same dataset that you used for #2. In your word processor file, briefly generate and discuss a null and research hypothesis. What do you expect to see in this relationship and why? Run a regression of your independent variable on your dependent variable, and save that regression as model.1. Choose a nominal or ordinal variable (with more than two possible values) to be a second independent variable in your regression. If you have an interval variable that you prefer, simplify it into an ordinal variable using cut2, levels(), and/or mutate(recode()). Set the reference category to something that makes sense. Run the regression again with both your independent variable from #4 and this new variable as independent variables, and save the regression model as model.2. Using either facets or colored dots, make a scatterplot of the regression from #6, and paste it into your word processor document. Using the stargazer package (as explained in section 9.3), generate a table of model.1 and model.2, and paste that into your word processor document. In a few sentences, interpret the estimated intercept, coefficients, significance levels, and adjusted R2 of models 1 and 2 from #8, as well as your graph from #7. References "],["wild.html", "Chapter 10 R in the wild 10.1 Todays lab as a reference 10.2 Importing data into R 10.3 Cleaning data in R 10.4 Online R resources 10.5 The last command: how to get help 10.6 The Strausz method for improving in R", " Chapter 10 R in the wild 10.1 Todays lab as a reference Unlike the previous chapters, I am not going to ask you to start a practice document and follow along. Just read the chapter, know that this a resource that you have available as you become more advanced in your R skills and need more guidance. 10.2 Importing data into R 10.2.1 Importing CSV files For this workbook, you have been able to access our three dataframesworld, states2010, and ane2020as .Rda files. In other words, these files were already formatted for use with R. However, if you keep using, you will regularly run into data that has not been formatted for use with R. For example, when I wanted to generate the ANES dataframe, I went to the ANES website: https://electionstudies.org/data-center/2020-time-series-study/. I had to create a log in to download data, and once I did that, I was taken to a page that gave me a few different choices of formats to download the data in, and I chose to download the csv version, which is often a good choice. CSV stands for comma separated values, and it is a simple form of data storage that many different software packages can understand. I next created a folder called anes2020, and in that folder, I made three new folders: one called rscripts, one called data, and one called notes (where I keep my codebooks and other reference materials related to that data). I then moved the cvs file that I had downloaded (which downloaded as anes_timeseries_2020_csv_20210719.cvs) into the data folder. After that, I made a new R project which I set to be in that anes2020 folder. Next, I imported the data with this command: anes2020&lt;-read.csv(&quot;data/anes_timeseries_2020_csv_20210719.csv&quot;) That dataset is huge: 1,771 variables! I read over the codebook (which I had also downloaded from the same webpage) and chose the variables that I wanted to keep for this class. I used this command to make a simplified version of that dataset (you need to have the tidyverse package loaded to use this command). anes2020.simplified&lt;-anes2020 %&gt;% select(V201005, V201103, V201115,V201117, V201151,V201152,V201156,V201157, V201200,V201217, V201225x, V201231x,V201233,V201234,V201237,V201262, V201324, V201336,V201337,V201345x,V201351,V201356x,V201366,V201368, V201369,V201377, V201392x,V201401,V201405x,V201414x,V201433, V201434,V201435,V201453,V201507x,V201508,V201510,V201516,V201544, V201549x,V201567, V201600,V201601,V201617x,V201628,V201644, V202159,V202160,V202161,V202162, V202163,V202164,V202165,V202166, V202167,V202168,V202169,V202170, V202171,V202172,V202173,V202174, V202175,V202178,V202181,V202182,V202183,V202187,V202073) That command looks like a lot, but all it is doing is telling R to make a new dataframe called anes2020.simplified and then selecting only that list of variables (instead of the 1,771 variables in the original dataframe). 10.2.2 Importing Excel files Sometimes you will find an excel file that you want to import. There is a very useful package that you can install to help with that called readxl. Once you have installed it, you can use the read_excel command to bring Excel files into R. After I downloaded the Excel version of the DEMOCRACY CROSS-NATIONAL DATA, RELEASE 4.0 FALL 2015 from Dr. Pippa Norriss webpage, https://www.pippanorris.com/data. I read it into R with this command: world &lt;- read_excel(&quot;data/Democracy Cross-National Data V4.1 09092015.xlsx&quot;) 10.2.3 Importing other files When dealing with data that is not formatted as a Rda, Excel, or CSV file, the package foreign is often helpful. Here is an example of how I have used the read.spss command from the package foreign command to read in an SPSS-generated .sav file: df &lt;- read.spss(&quot;data/previous semester data.sav&quot;, use.value.label=TRUE, to.data.frame=TRUE) 10.3 Cleaning data in R One you have imported data, there are a number of things that you want to do to do before you do analysis. It always helps to glimpse() your new dataframe and look at the codebook, if you can find one. When I was first looking at the ANES data for this lab workbook, I noticed that many of the variables had long names that were difficult to interpret, like this: V201005. I renamed them with this tidyverse command (note that the real command was longer because it included all the variables in the dataframe): anes2020.simplified&lt;-anes2020.simplified %&gt;% rename(attention=V201005, v2016=V201103, hope=V201115, outrage=V201117, ft_biden=V201151, ft_trump=V201152, ft_dems=V201156, ft_reps=V201157) You might find that are ordinal or nominal variables that are coded as numeric which you want to recode as factors, using the as.factor command, which you can do like this: df$variable&lt;-as.factor(df.variable) You can also use the codebook and/or the table() command to figure out how data is coded. Often missing data is coded as 8, 9, 99, or negative numbers. You want to make sure to recode that as NAs or R will treat those values as numbers and mess up your analysis. I used the tidyverse command mutate(na_if()) to do that. Here is how I accomplished that with the age variable in ANES: anes2020.simplified&lt;-anes2020.simplified %&gt;% mutate(age=na_if(age, &#39;-9&#39;)) And here is how I recoded the marital variable to exclude NAs and to give the numeric variables values that made sense: anes2020.simplified&lt;-anes2020.simplified %&gt;% mutate(marital=na_if(marital, &#39;-9&#39;)) %&gt;% mutate(marital=na_if(marital, &#39;-8&#39;)) %&gt;% mutate(marital=recode(marital, &#39;1&#39;=&quot;married&quot;,&#39;2&#39;=&quot;married&quot;,&#39;3&#39;=&quot;widowed&quot;, &#39;4&#39;=&quot;divorced&quot;,&#39;5&#39;=&quot;separated&quot;,&#39;6&#39;=&quot;never married&quot;)) 10.4 Online R resources When you want to know how to do something in R, you can always google it. There are many, many excellent free resources online. In addition, here are nice free online R resources: https://stackoverflow.com/ This is a community of people that ask and answer questions about lots of different software packages including R. You can search all of the questions for free as a non-member, or you can join (also free) and ask and answer questions. I have figured out how to do many things in R thanks to the community at this site. https://bookdown.org/ndphillips/YaRrr/ This is a very fun, free, online introduction to R, called YaRrr! The Pirates Guide to R, by Nathaniel D. Phillips. Actually, Dr. Phillips claims that he discovered the book and translated it from pirate-speak into English. https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-1/ This is part one of a four-part series of blog posts on Data Wrangling, by Suzan Baert. All four parts are very helpful in learning to manage data in R. https://r-graphics.org/ This is the R Graphics Cookbook, by Winston Chang. It is an extremely useful resource when using ggplot. It shows you how to make many, many kinds of graphs, and also how to modify the graphs that you have already made. https://www.apreshill.com/project/ohsu-dataviz/ This is a really nicely done, free, asynchronous online class called  Principles &amp; Practice of Data Visualization taught by Alison Hill. This class teaches you about data visualization in R with ggplot. https://r4ds.had.co.nz/ This textbook, R for Data Science, by Hadley Wickham and Garrett Grolemund is a terrific resource, available for free online. https://isabella-b.com/blog/ggplot2-theme-elements-reference/ This is a really helpful guide to theme elements in ggplot2 by Isabella Benabaye. https://henrywang.nl/ggplot2-theme-elements-demonstration/ This is a nice ggplot2 theme elements demonstration by Henry Wang. http://mattwaite.github.io/sports/ This textbook, Sports Data Analysis and Visualization by Matt Waite is a great resource for using R to analyze sports. https://bookdown.org/yihui/bookdown/ I used this textbook, bookdown: Authoring Books and Technical Documents with R Markdown, by Yihui Xie, to learn how to put this workbook online. 10.5 The last command: how to get help As you continue with R, you will often find yourself in a situation where you want to know what a command is for, or the specifics of how to use it. When you are in this situation, you can type a ? followed by that command, like this: ?table R will pull up documentation on the command. Rs documentation is sometimes hard to read (and reading it takes practice) but it can be a good place to start, before looking at the online resources in the previous section. If the command is from a package that you dont have installed at that time, you can type two question marks, and R will look through all of the packages in its online libraries, like this: ??cut2 10.6 The Strausz method for improving in R After taking this class and working through this workbook, you may decide that you want to keep using and improving at R for the reasons that I laid out in section 1.1: because R is free, powerful, professional-grade software and skills in R will really impress potential employers (and your friends and family!). Below is the method that I used to learn R. Every day, I set a timer for 15 minutes, and spent that time doing something in R. I began with though Nathaniel D. Phillips Pirates Guide and then I worked my way through the other resources in section 10.4. I also spent that time figuring out how to use R to solve problems in my own research; I made a vow to myself that from now on, I would do all analysis for my own research in R. You can also use that 15 minutes to do anything in R. Maybe you want to use R to analyze your favorite sport. Maybe you want to use R to learn about knitting patterns, or to keep track of data regarding your other hobbies or interests. There is even a package available called sourrr that is useful when generating recipes for sourdough bread (although I have found my favorite sourdough bread recipes in this book). Maybe you want to use R to gather and analyze data relating to games that you play or debates that you have with your friends. Or, maybe you want to use R to help you with research projects for other classes. Just choose something to do in those 15 minutes, and do it. If you do that for a while, you will surprise yourself with how much R you can do. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
