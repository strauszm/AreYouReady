[["index.html", "Are you ready for R? A Workbook for R for Political Science and Beyond: Version 3.0 Preface", " Are you ready for R? A Workbook for R for Political Science and Beyond: Version 3.0 Michael Strausz 2025-07-02 Preface I decided to learn R in late 2020 for a variety of reasons, including the reasons that I lay out in section 1.1: R is open source, free, and powerful. Early on in my studies, I decided that going forward I would also teach my introductory undergraduate course “Scope and Methods of Political Science” using R instead of SPSS (which I had used in the past), for those same reasons – it would be great for my students to get some experience with this powerful piece of software. As I reviewed the available workbooks, I did not find one that I was satisfied with. Carly Potz-Nielsen has written a terrific workbook with a colleague of hers which she shared with me. I considered using that, but ultimately, I decided to write my own workbook to fit the way that I have come to teach Scope and Methods. I also wanted to write an R workbook that uses the tidyverse package, because I think that is a great way to use R. I decided to make my workbook available for free online because that is consistent with the open source spirit of R. This workbook might be helpful to you if you are trying to teach yourself R, and you are also welcome to use it to teach students R in non-profit high schools and non-profit collegiate settings. Those wishing to use it for commercial reasons outside of non-profit educational uses should contact me for permission. "],["about-the-author.html", "About the author", " About the author I am Michael Strausz, a Professor and the Chair of the Political Science Department at Texas Christian University. I am the author of the 2019 book Help (Not) Wanted: Immigration Politics in Japan and the editor of the 2025 book No Island is an Island: Perspectives on Immigration to Japan. My research focuses on Japan’s immigration policy and the role of norms in international politics. I teach a variety of courses in comparative politics, international relations, research methods, and Asian studies. I graduated from Michigan State University’s James Madison College and earned my Ph.D. in political science from the University of Washington. I live just outside of Fort Worth, Texas with my spouse and our two children. In my free time, I enjoy cooking, hiking, and schmoozing. "],["acknowledgements.html", "Acknowledgements", " Acknowledgements I’d like to thank Seth Jolly, Katie Nissen, Adam Schiffer, Amy Sentementes, and David “The Bad Man” Parzen for their helpful suggestions. Most importantly, I’d like to thank my spouse Kate for coming up with the great title and for putting up with my (admittedly annoying) regular, enthusiastic discussion of the new things that I had learned to do and make in R over these last few years. "],["whats-new-in-version-3.html", "What’s new in version 3.0?", " What’s new in version 3.0? The first version of this workbook was a series of PDFs that I uploaded to my website in April 2022. Those were a bit clunky. They were difficult for students to copy code chunks out of (the formatting would get strange) and they were also difficult for me to update. In April 2023, I created a version of this workbook as a single website. I actually did this as I was teaching a class that used the workbook, so the students switched from PDFs to a website in the middle of the class. It was very dramatic! I continued to make small updates and corrections to the workbook regularly, but for version 3.0 I read over the whole workbook, and I clarified various explanations and fixed small errors. More substantively, I added commands to calculate the mode of nominal variables and the mode and median of ordinal variables to chapter 3 and I added a discussion of statistical significance to chapter 6. I also added some discussion of residuals to chapters 8 and 9. Finally, I updated the three datasets that accompany this workbook. I also removed the practice assignments at the end of each chapter. This is because I started writing new practice assignments for students each semester and distributing them in class. If you are an instructor and you’d like my most recent practice assignments for each chapter, please feel free to contact me. If you want to see the R projects that I used to generate these datasets, you can find them here: anes2024, states2025, and world2025. If you have suggestions and/or notice errors, please let me know! "],["getting-started.html", "Chapter 1 Getting started with R 1.1 Why R? 1.2 Create a project for this class 1.3 Add the course files to your project 1.4 Using the Console 1.5 Writing your first R script 1.6 What do to if the “load” command doesn’t load the dataframe 1.7 Review of this chapter’s commands", " Chapter 1 Getting started with R 1.1 Why R? R is a powerful, customizable, open source piece of software that is extremely useful for political science analysis (among many, many other uses). R has a steep learning curve (in other words, it is hard, especially at first), but your instructor is here to help you, and we will all be learning this together. Here are some of the biggest advantages of R: R is free! In the past, I have taught this class with SPSS. As of this writing, the professional version of SPSS costs $99 per month for a subscription. Because R is open source, users are regularly writing custom programs to improve it, which they tend to share for free. Because R is open source, there are great resources available for free online (see section 10.4 for some of my favorites). R is more powerful than SPSS. It can conduct statistical analyses and generate graphics that SPSS is not capable of (including maps with GIS data). R is professional-grade software, extremely commonly used by professional analysts. Familiarity with R will look great on your resume. It will show potential employers that you have some serious data skills and are capable of learning to code. Learning R will help you learn to code in other pieces of software as well, which will be an important skill in the future. In this course, I have no expectation that you have any background in coding or in statistical analysis. So, if you have no such experience, please do not worry! You are not alone. I would be surprised if more than a few of you have coding experience, and I will teach the class as if none of you do. If some parts of the class are review for you, that won’t hurt you either! 1.2 Create a project for this class Follow the following instructions to get your computer set up for this class. First, you will have to download and install two pieces of software on your computer. NOTE: You can install these programs on a Mac or a PC, but not on an iPad or a tablet that is not running Windows or the Mac OS. You can find the first piece of software, R, here. If you aren’t in the US, you can choose the mirror that is closest to you here. You can find the second piece of software, RStudio, here: https://www.rstudio.com/products/rstudio/. Be sure to download RStudio Desktop open source edition. Create a folder somewhere on your computer that you can easily find. If no place comes to mind use the Desktop. Title the folder “Scope and Methods Labs” (or another title that you would prefer). Open RStudio on your computer. Click on the “File” menu at the top, and then select “New Project.” Select “Existing Directory” on the menu that comes up. Navigate to the main folder that you just created (which I asked you to call “Scope and Methods Labs”), and click “Open.” Select “Create Project.” 1.3 Add the course files to your project Go to this website: https://github.com/strauszm/AreYouReady/tree/main/SupplementalWorkbookFiles. Click on the words “Supplemental Workbook Files.zip.” That will take you to a new screen where there is a link to the right with a little picture of an arrow pointing into a tray, like this: Click that link to download the file. On your computer, unzip the file, and you should see three folders inside: “codebooks”, “Data”, and “rscripts”. Copy the three folders and their contents to the project folder that you made before called “Scope and Methods Labs” (in section 1.2). Congratulations! You have set up R for this workbook (and for other uses too!) 1.4 Using the Console In order to use R we must use commands to tell R what to do. One way you can write commands is in the Console, here: At its most basic, you might think of R as a very, very fancy calculator. So, for example, I can type 45*21 in the Console (after the “&gt;”) and hit enter, and I have done my first R operation: What if we want to R remember that value? Then we can use this notation “&lt;-” to assign that value to an object. You will soon see that programming in R often involves creating objects, modifying objects, and analyzing objects. There are a few rules about object names. 1.4.1 Rules about naming objects Object names cannot begin with a number. Object names cannot have a space in them. R notices whether object names use capital or lower-case letters, so be careful with this. To avoid errors, copy and paste object names rather than typing them out. 1.4.2 Common practices when naming objects Common practices in naming objects are to use a “.” between words, a “_” between words, or to capitalize each word after the first. So either: an.object, an_object, or anObject. You can choose which style you prefer. Let’s make our first object, using this command (you can copy and paste this command into your Console): my.first.object&lt;-45*21 When you copy and paste that code into your Console, you should see something like this: Notice what happened when we created this object. R did not give us the answer to our math problem in the Console, but instead put the object that we created in the Environment pane. This kind of object, a single value, is called a scalar. You won’t deal with these very frequently in this class. 1.4.3 About vectors A more commonly used kind of object is a vector – a list of two or more values. What if I want to create a vector that includes the my.first.object scalar and another number? One way that I can build a vector is with the c() function. The c here stands for concatenate, which means “bring together.” Here is how I would use c() to build such a vector: my.second.object&lt;-c(my.first.object, 2) If you enter that into the Console, you will see in the Environment pane there are now two objects in your Environment: my.first object (a scalar with a value of 945) and my.second.object (a vector with two values: 945 and 2). But you actually don’t need to remember these objects, right? No need to have it gunking up your Environment. So you can use the rm() command to remove them (rm() means remove). Just type this into the Console, and you will see the first object, the scalar, disappear from the Environment: rm(my.first.object) Then type this into the Console and you will see the second object, the vector, disappear: rm(my.second.object) 1.5 Writing your first R script You will often want to write a series of commands in R that you will be able to refer back to and modify. In that case, instead of writing them in the Console, you can write them in a script file. Let’s write our first R script! Go to the File menu at the top of your screen, select “New File” and then select R Script. Before starting to write it, save it in the R scripts section of the folders that you made (click “File” then “Save As” and then give it a name, like “my first script”). One thing to know about R is that R ignores hashtags (like “#”). Once R reads a hashtag, it stops reading the rest of that line. So, we can use # to insert our comments to remind us what we are doing with specific chunks of code. Try entering something like this at the top of your page: #This is my first R script Now we are going to enter a command. We are going to use the load() command to load one of the datasets that we will be using in this class. Now type: load(&quot;Data/anes2024.Rda&quot;) #This is how I am loading some data that I will use later. Be sure to include the \"\" around the name and location of the dataset. The comment (following the #) is not necessary, but it is there for your reference. So, at this point you should have two lines of script. To run this script, you must first select it, by highlighting it with your cursor. Then, you can either click the little run button circled below or you can hit Cntl-Enter in a PC or Command-Enter on a Mac. Once you run this script, you should see a new object in your environment: anes2024 (if that didn’t work for you, see section 1.6). This is a dataframe – a collection of vectors that are combined to give us a lot of interesting information. In this case, this dataframe tells us about a survey of residents of the United States from 2024. We will look more at this in future weeks. Save your R script, and be sure that you save changes to this project. 1.6 What do to if the “load” command doesn’t load the dataframe Click on the folder called “Data” in the bottom right of your screen. That should open up a folder with the three dataframes that you downloaded earlier. Click on anes2024. You will get a warning message similar to this1: Click “Yes,” and anes2024 should appear in your environment. 1.7 Review of this chapter’s commands Command Purpose &lt;- To create an object. For example: “this.object&lt;-1” assigns the number 1 to an object called “this.object.” c() Concatenate. To combine more than one data point into a vector. rm() Remove an object from your environment. # Tells R to stop reading that line, so a useful prefix to for comments and reminders. load() Load an R-formatted dataframe. Your warning message should refer to anes2024, not anes 2020↩︎ "],["tidyverse.html", "Chapter 2 Welcome to the tidyverse 2.1 Installing packages 2.2 Exploring dataframes 2.3 Organizing your scripts 2.4 Welcome to the tidyverse 2.5 Review of this chapter’s commands", " Chapter 2 Welcome to the tidyverse 2.1 Installing packages One of the nice things about R is that users can create their own libraries of commands. These can be extremely powerful and useful tools. There is an excellent set of tools that are in a library called the “tidyverse.” To install the tidyverse on your local computer, first, make sure that your computer is attached to the internet. Second, enter this command into the Console of RStudio and press Enter: install.packages(&quot;tidyverse&quot;) Once you hit enter, you should see some action in your Console (what it says will depend on what version of R you are using). Some of it might be in red, but don’t worry about it. Now you have installed the tidyverse set of libraries of commands! We will regularly work with the tools that this library makes possible for us. 2.2 Exploring dataframes To begin exploring dataframes, open the project that you made in the last lab. There are two ways that you can do this: You can open RStudio, go to File, open project, and navigate to the file called Scope and Methods labs. You can navigate to the folders that you made last lab and double click the file called “Scope and Methods Labs,” like the circled file below: If you followed the instructions in section 1.5, you should see an object called anes2024 in your environment. To load the other two dataframes that you will need to use this workbook, copy and paste these two commands into your Console, one by one: load(&quot;Data/states2025.Rda&quot;) load(&quot;Data/world2025.Rda&quot;) If those commands don’t work, you should load anes2024 (if you haven’t already), states2025, and world2025 using the instructions from section 1.6. Those files are the dataframes that we will be doing most of our analysis on in this class. As I mentioned in the last chapter, a dataframe is essentially a group of vectors attached to one another, and is an extremely useful way of storing data for analysis. Here is a quick summary of those three dataframes: anes2024 is a snippet of the American National Election Survey, a survey that has been conducted at every election since 1948 by scholars based out of the University of Michigan and Stanford. This dataframe that I created only looks at the 2024 election. You can find more information about ANES at their website, https://electionstudies.org/ states2025 is a dataframe with information about US States. It is based on data that I collected from a variety of sources. You can find information about the source of each variable in the file “States 2025 codebook.pdf”, which is in the codebooks folder that you downloaded with the other class materials in section 1.3. If you’d like to suggest additional variables to go in this dataset, please contact me. world2025 is a dataframe with information about most of the countries in the world. It is based on data that I collected from a variety of sources. You can find information about the source of each variable in the file “World 2025 codebook.pdf”, which is in the codebooks folder that you downloaded with the other class materials in section 1.3. If you’d like to suggest additional variables to include, please contact me. To begin, let’s take a look at the anes2024 dataset using the “View()” command (note that “View” is capitalized, unlike many R commands). Enter this command into the Console and then hit Enter: View(anes2024) When you hit enter, the following should come up on your screen: This is a neat visualization of the ANES data. Each row represents one case. Since ANES is a survey, each case is a single individual who took the survey. Each column represents a variable: something that the survey designers wanted to measure about the people taking the survey. Look at row #1. That describes a person who is 50-years-old, married, formerly active duty in the military, not a union member, Hispanic, etc. If you want to see what all of the variables mean, you can look at the file ANES 2024 codebook, which you should have downloaded in section 1.3 as part of the “codebooks” folder. Now let’s say that we want to know a bit more about the age variable. How old was the average person that took our survey? To answer, we need to learn two more R commands: the “$” and the command “summary.” The “$” command helps us identify single columns within a dataframe. So, if we just want to see the age column, you can type this in the Console: anes2024$age When you do this, you will see that R gives you the age of every single person who took the survey. Kind of interesting, but not all that useful. So, let’s try something else. Type summary(anes2024$age) into the Console, and you get these results: summary(anes2024$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 18.00 39.00 54.00 53.15 68.00 80.00 279 This gives you a lot of useful information! You can see that the youngest age in the dataframe is 18 (likely because they were not surveying minors), and the oldest is 80.2 You will also see that the median age is 54 and the mean is 53.15. And, you will see that there are 279 NAs. “NA” is what R reports when it does not have data in a particular variable. So, there are 279 cases for whom we don’t have data on their ages. The “summary” command is very useful with numeric data, like age. However, what if you have a variable like “marital,” which measures the marital status of individuals? The mean and median of this variable would not be meaningful, but we might want to know how many married, single, divorced, etc. people there are in our dataframe. To get this information, we can use the “table” command, like this: table(anes2024$marital) That command generates this output: ## ## divorced married never married separated widowed ## 771 2745 1242 91 414 In other words, our dataframe has 771 divorced people, 2745 married people, 1242 never married people, etc. Useful stuff! 2.3 Organizing your scripts As you use R you will generate a lot of script files, and if you are not careful you might find yourself forgetting what some of your scripts are for. This is where the # characters can be really helpful. You can leave notes to yourself. This is a nice way to add headers to R script files: #################################### # Your name # Project, Purpose of R file # Date started : Date last modified #################################### Then, when you open a script file, you can see right away when you made it and what it is for. You can customize this header as is helpful to you. 2.4 Welcome to the tidyverse Perhaps the code package that has had the biggest impact on how people do R in the last few years had been the “tidyverse”. To take advantage of this great library which you installed in section 2.1, you first have to activate it. You will have to do this step every time that you open R, so I often put it at the top of my script files. To follow along, open a new script file and save it in your scripts folder as “chapter 2 practice.” Copy and paste this onto the page: #################################### # Your name # 20093 Chapter 2, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) Now select all the text on this page, run it, and then save it. You should see something like this in your Console: ── Attaching core tidyverse packages ────────────────── tidyverse 2.0.0 ── ✔ dplyr 1.1.4 ✔ readr 2.1.5 ✔ forcats 1.0.0 ✔ stringr 1.5.1 ✔ ggplot2 3.5.0 ✔ tibble 3.2.1 ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ✔ purrr 1.0.2 ── Conflicts ──────────────────────────────────── tidyverse_conflicts() ── ✖ dplyr::filter() masks stats::filter() ✖ dplyr::lag() masks stats::lag() ℹ Use the conflicted package to force all conflicts to become errors Don’t worry about the warning messages. The list of packages are the libraries of commands that come with the tidyverse library. The two that we will be using the most are ggplot2, which is terrific for graphing, and dplyr, which is great for managing data. Since you have installed the tidyverse, you will not have to install those packages separately – you already have them installed. 2.4.1 Introducing the ‘pipe’ in tidyverse Now that we have access to the tidyverse, we will use a few of the commands that are available to us through the dplyr package to get a better look at some of our data. Before we can do that, however, we need to learn about the somewhat strange feature of dplyr, the “pipe.” The pipe is this set of characters: %&gt;% You can always type those three characters to build a pipe, but you can also type Ctrl-Shift-M on a Windows machine or Cmd + Shift + M on a Mac. I am going to repeat this, in bold, because it is an extremely useful keyboard shortcut. You can always type those three characters to build a pipe, but you can also type Ctrl-Shift-M on a Windows machine or Cmd + Shift + M on a Mac. The pipe means “and then.” Let’s try an example, using the glimpse() command that is available in the tidyverse. Try entering this command into your script file: anes2024 %&gt;% glimpse() #taking a different kind of look at our dataframe Highlight that text and run that command. You should see a lot of output in the Console. If you scroll up a bit in the Console, you should see something like this (this is just the first 12 variables in this dataframe; you will have access to more on your screen): ## Rows: 5,521 ## Columns: 12 ## $ age &lt;int&gt; 50, 41, 44, 45, 80, 75, 41, 49, 45, 41, 59, 34, 42, 70, 58,… ## $ female &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,… ## $ marital &lt;chr&gt; &quot;married&quot;, &quot;married&quot;, &quot;married&quot;, &quot;married&quot;, &quot;widowed&quot;, &quot;div… ## $ mil &lt;chr&gt; &quot;formerly active duty&quot;, &quot;never served&quot;, &quot;never served&quot;, &quot;ne… ## $ union &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,… ## $ race &lt;chr&gt; &quot;Hispanic&quot;, &quot;Asian or Pacific Islander&quot;, &quot;White&quot;, &quot;Asian or… ## $ children &lt;int&gt; 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0,… ## $ landline &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,… ## $ smartphone &lt;dbl&gt; 1, 1, 1, NA, NA, 1, 1, 1, NA, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,… ## $ passport &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,… ## $ trans &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ gayLesbian &lt;chr&gt; &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;,… This is giving us information about the dataframe anes2024. At the top, you can see that the dataframe has 5,521 rows. Since this is a survey, and each row represents one person that was surveyed, that tells us that the anes2024 dataframe represents a survey of 5,521 people. On the output above, you can see that the dataframe has 12 columns (because I told R to only display the first 12.) If you ran the command, you should see that the full dataframe has 83 columns, which means that the dataframe measures 83 variables. The glimpse() command in the tidyverse gives us a bit of information about each of these variables. On the left of the output you can see all of the variable names (age, marital, etc.). And then just to the right of that you can see the way that R is classifying those variables (&lt;int&gt;, &lt;chr&gt;, &lt;ord&gt;, etc.). We will learn a bit more about those classifications in future chapters. To the right of those classifications you can see the first set of values of each variable. 2.4.2 select() in tidyverse Many dataframes are quite large, and you might find yourselves wanting to focus on a few variables at a time. The command select() can be helpful for this. Let’s say that you want to focus in on how many guns people have in their homes and their feelings about the NRA. In that case, you would want to select the variables “ft_nra” and “guns”, which you can do like this: anes2024 %&gt;% select(guns, ft_nra) #focusing in on a few variables 2.4.3 A warning about the select() command Unfortunately, some of the packages that we will be installing in future labs also use the command select. If you ever run “select” and have more than one of these packages installed, R will give an error. When you get that error, you can add “dplyr::” before select(), to tell R to look at the select() in the dplyr library (which comes with the tidyverse). Here is how the above command would look with that modification: anes2024 %&gt;% dplyr::select(guns, ft_nra) #focusing in on a few variables If you run either of these commands (with or without the dplyr::), and scroll up a bit in the Console, you will see output that begins like this: ## guns ft_nra ## 1 1 60 ## 2 0 50 ## 3 0 0 ## 4 0 70 ## 5 0 60 ## 6 0 30 ## 7 0 85 ## 8 0 50 ## 9 0 75 ## 10 0 50 This is a way of displaying dataframes that is unique to the tidyverse called a “tibble.” Each row represents a case, and each column represents a variable (except for the leftmost column, which is just the case number. Looking at case #1, you can see that that person owns 3 guns, and rates the NRA 50 out of 100. Case #2 also rates the NRA 50, but owns no guns. Notice that the dataframe that had been so large before, with 68 columns, now only has 2 columns. If you want to use that glimpse() command from above to just get summary information about each of our variables, you can use a second pipe (%&gt;%) like this: anes2024 %&gt;% select(guns, ft_nra) #focusing in on a few variables%&gt;% glimpse() #taking a different kind of look at our dataframe 2.4.4 arrange() in tidyverse What if we want to see what the people in our sample with the most and fewest guns thought about the NRA? To do that, we can arrange our dataframe from smallest to largest or from largest to smallest on any of our variables. Or, with variables that include text, we can sort according to alphabetic or reverse alphabetic order. We can use the arrange() command to do that: anes2024 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(guns) #sorting this variable from smallest to largest When we run the above command, the Console shows us the feeling about the NRA by the people with the fewest guns first, and then on to the people with more guns. But what if we want to see the largest values on the “guns” variable first? To do that, we need to use desc() to modify arrange() like this: anes2024 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(desc(guns)) #sorting this variable from largest to smallest 2.4.5 glimpse() in tidyverse Finally, combining all of the commands that we learned today, we can use glimpse() to see information about each of our variables in this simplified version of our dataframe. To do that, we will need a third pipe: anes2024 %&gt;% select(guns, ft_nra) %&gt;% #focusing in on a few variables arrange(desc(guns)) %&gt;% #sorting this variable from largest to smallest glimpse() #taking a different kind of look at our dataframe 2.5 Review of this chapter’s commands Command Purpose Library install.packages(““) To install new packages, or libraries of R commands. You have to do this only once on your computer per library when your computer is connected to the internet so that it will download the relevant library. Base R View() Displays a dataframe as a spreadsheet. Notice that the word “View” is capitalized. Base R summary() Displays summary information (like mean and median) about a numeric variable. To call a variable, use a dollar sign. So, like this: “summary(dataframe$variable)” Base R table() Displays the number of times each value of a variable appears in a dataframe. Most useful for variables with a relatively small number of possible values. To call a variable, use a dollar sign. So, like this: “table(dataframe$variable)” Base R library() To ask R to load a library into your session. You must use this command every time that you open R if you want it to use a particular library. Base R %&gt;% The “pipe.” Useful with manipulating data in the tidyverse. dplry (tidyverse) glimpse() Display general information about a dataset, including looking at the variable names, types, and a few values in rows. Often preceded by a pipe. dplyr (tidyverse) select() Allows you to select a few variables to focus on. Often preceded by a pipe. You can use the variable names without dollar signs. dplyr (tidyverse) arrange(desc()) Sorts a dataframe from largest to smallest values of a particular variable (or in reverse-alphabetical order). Often preceded by a pipe. You can use the variable names without dollar signs. dplyr (tidyverse) If you look in the codebook, you will see that the survey codes everyone 80 and over as 80.↩︎ "],["graphing.html", "Chapter 3 Graphing and describing variables 3.1 Getting started with this chapter 3.2 Getting started with ggplot and graphing 3.3 Central tendancy and dispersion 3.4 A note on using the states2025 and world2025 3.5 Review of this chapter’s commands", " Chapter 3 Graphing and describing variables 3.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. We are going to install three new packages to use today. One is called “epiDisplay” and the other is called “Hmisc.” Enter these two commands into your Console one by one: install.packages(&quot;epiDisplay&quot;) install.packages(&quot;scales&quot;) install.packages(&quot;DescTools&quot;) Now, open a new script file and save it in your scripts folder as “chapter 3 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 3, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(epiDisplay) #the tab1 command helps us make nice frequency tables library(scales) #this helps us to put more readable scales on our graphs Now select all the text on this page, run it, and then save it. 3.2 Getting started with ggplot and graphing R has some nice graphic abilities built in, but the ggplot2 package that comes with the tidyverse is even more powerful. In this lab, we will learn to make bar graphs of nominal and ordinal variables and histograms of interval variables. **NOTE: You can find templates to reproduce all of the graphs from this workbook in the script file “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3. 3.2.1 Bar graphing a nominal variable A nominal variable is a variable where the magnitude of differences between the values does not give us any information, and the possible values can be listed in any order without confusing us. R does not have a single way to represent nominal variables, but on the three datasets that I have uploaded for this book I have classified the nominal variables as character variables. R sometimes abbreviates this as &lt;chr&gt;. If you type this into the Console: glimpse(anes2024) you can see that several of the variables in the ANES dataset are classified as characters, including “marital,” “mil”, and “race”. If you ever want to know how R has classified a variable, you can us the class() command. For example, if you type class(anes2024$marital) you can see that R calls the “marital” variable a character variable. Bar graphs give us a sense of how the cases that we studied are distributed across possible values for a nominal variable. To generate a bar graph using ggplot, you begin with the command ggplot(), and then the first thing you type into the parenthesis is the name of the dataframe that you will be working with. The ggplot package is similar to the dplyr package that uses the pipe (%&gt;%) to mean “and then”, but instead of a pipe ggplot uses a “+”. This code will generate a bar graph of that marital variable (if you enter and run the code, you will see the graph that follows it): ggplot(anes2024, aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status of respondents&quot;)+ xlab(&quot;Marital status&quot;) 3.2.2 Removing the NA bar The NA bar represents the number of cases where there is missing data for this question. Often you will not want to include that bar in your graph. To get rid of it, you can use a pipe when you call the dataset and tell R to filter out the cases where “marital” is NA. Like this: ggplot(anes2024 %&gt;% filter(!is.na(marital)), aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status of respondents&quot;)+ xlab(&quot;Marital status&quot;) If you execute that command, you will see that now the NA bar has been removed. 3.2.3 Adding a second line to a graph’s title While “Marital status of respondents” is not a very long title, sometimes you will have graphs with longer titles that you want R to put on two or more lines. To tell R to move to a new line, just insert “\\n” where you want the line break to go, like this: ggplot(anes2024 %&gt;% filter(!is.na(marital)), aes(x=marital))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Marital status\\nof respondents&quot;)+ xlab(&quot;Marital status&quot;) 3.2.4 Saving a graph To save a graph, you can click the export plot menu right above it: You can either save it as an image, which you will later be able to import into word or other software, or you can copy it to clipboard and immediately paste it into another piece of software. Either of those work. 3.2.5 Bar graphing an ordinal variable An ordinal variable is a variable where the magnitude of the difference between the values does not give us any information, but the possible values must be listed in a particular order to make sense. R is most willing to treat a variable as ordinal when it is listed as an ordered factor, which R sometimes abbreviates as &lt;ord&gt;. If you type this into the Console: glimpse(anes2024) you can see that several of the variables in the ANES dataset are classified as ordered factors, including “income”, “bible”, and “religAttn” Reminder: if you ever want to know how R has classified a variable, you can use the class() command. For example, if you type this: class(anes2024$education) ## [1] &quot;ordered&quot; &quot;factor&quot; you can see from the output R calls the “education” variable “ordered” “factor”. Another variable that ANES2024 measures ordinally is income. To generate a bar graph of the income variable, we can use this code: ggplot(anes2024 %&gt;% filter(!is.na(income)), aes(x=income))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Respondent&#39;s household income&quot;)+ xlab(&quot;Income&quot;) 3.2.6 What to do about overlapping labels If you look at the previous graph, you might notice that the labels at the bottom overlap. This makes it hard to read! To deal with overlapping labels like this, you can add this line to your code (please note that R will not understand this code if you did not install the scales package in section 3.1 and then run the command library(scales)): scale_x_discrete(guide = guide_axis(n.dodge=2))+ This is how our new code looks: ggplot(anes2024 %&gt;% filter(!is.na(income)), aes(x=income))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Respondent&#39;s household income&quot;)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ xlab(&quot;Income&quot;) 3.2.7 Generating a histogram An interval variable is a variable where the magnitude of difference between the values gives us useful information and where the possible values must be listed in a particular order to make sense. In this class, it is generally safe to assume that a variable that is neither an ordered factor nor a character variable is interval. However, variables with only two values (such as 0 and 1) should not be treated as interval. You can, in theory, make a bar graph of an interval variable. However, since interval variables generally have many possible values, those bar graphs become very difficult to read. Instead, we often generate histograms of interval variables. Histograms are bar graphs where each bar represents a range of values of the variable of interest instead of a single bar. For example, here is how we can make a histogram of the population3 variable in our world dataset: ggplot(world2025, aes(x = pop)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Histogram of countries&#39; populations, 2019&quot;)+ xlab(&quot;population&quot;) 3.2.8 Removing scientific notation from axes The above graph is ok, but because the population numbers are so big, R has converted them to scientific notation. To force R not to do this, we can use a command made possible by the scales package that we installed in section 3.1: scale_x_continuous(labels = label_comma()) Combined with the rest of our code, we can generate the graph like this: ggplot(world2025, aes(x = pop)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ scale_x_continuous(labels = label_comma())+ #gets rid of scientific notation ggtitle(&quot;Histogram of countries&#39; populations, 2019&quot;)+ xlab(&quot;population&quot;) 3.2.9 Adjusting bin width on a histogram The bars on a histogram are called “bins.” Looking at this graph, we can see that R has chosen to set the bins at around 50,000,000. What if we wanted to make the bins narrower? We can set the bin width like this: ggplot(world2025, aes(x = pop)) + geom_histogram(binwidth=10000000, fill = &quot;purple&quot;, colour = &quot;black&quot;)+ scale_x_continuous(labels = label_comma())+ #gets rid of scientific notation ggtitle(&quot;Histogram of countries&#39; populations, 2019&quot;)+ xlab(&quot;population&quot;) Those narrower bins let us see more of the variation in the variable. 3.3 Central tendancy and dispersion With all variables, it is helpful to both you and your readers to make observations about their central tendency—what a typical case looks like, and dispersion—how the actual values are spread out across possible cases. 3.3.1 Central tendency and dispersion of nominal variables With nominal variables, there are two useful techniques to help us discuss central tendancy and dispersion. One is covered above, in section 3.2.1: you can make a bar graph. Second, you can also make a frequency table. To make a frequency table, you can use the table() command to produce a simple one in base R, but the tab1 command that is available through the epiDisplay library that we installed in section 3.1 is more flexible. So, let’s try to look at the region variable from the states2010 dataset. Enter the following into today’s practice R script file, and run it: #make a frequency table and bar graph of the region variable in states2010 tab1(states2025$region, cum.percent=FALSE) ## states2025$region : ## Frequency Percent ## midwest 12 23.5 ## northeast 9 17.6 ## south 17 33.3 ## west 13 25.5 ## Total 51 100.0 You can see that this command generates two sets of output: a graph that you can see if you click the “plots” tab in the bottom right, and some information in the Console. The graph is ok, but the graphs that we generated above are nicer, so we will use those instead. The information in the Console tells you that there are 12 states in the dataframe that are classified as being in the midwest, 9 that are classified as being in the northeast, 17 in the south, and 13 in the west. The next column to the right tells you what percent of states are in each category. There is only one meaningful way to measure the central tendency of a nominal variable: the mode. To calculate the mode of a nominal or ordinal variable with only a few variables, you can examine the frequency table and/or bar graph and see which value is the most frequent. You can also calculate the mode with the Mode command (note that the M in Mode is capitalized), which is in the DescTools package that we installed for this chapter. to calculate the mode for the region variable, you type the following into the Console: Mode(states2025$region,na.rm=TRUE) Note that I wrote na.rm=TRUE at the end of this command. This is because with Mode command, like several other commands in R, if there are any NAs in the variable that you are asking R to analyze, R will generate “NA” as an output without that na.rm=TRUE. When you hit enter, you should see this output: ## [1] &quot;south&quot; ## attr(,&quot;freq&quot;) ## [1] 17 The first line in this output is telling you that the mode of the region variable in the states2025 dataset is “south.” The third line tells us that “south” occurs 17 times, which is consistent with what we see in the frequency table above. Regarding the dispersion of this variable, we can look the frequency table and note that no region has a majority of states, and the states seem to be reasonably evenly distributed across regions, with 33.3% of states in the modal region (the south) and 17.6% of states in the least common region (the northeast). 3.3.2 Central tendency and dispersion of ordinal variables With ordinal variables, we can use the same techniques that we use for nominal variables but with one exception: since the order of the possible values does give us important information for ordinal variables, the cumulative percent also gives us useful information. Thus, we can ask R to report the cumulative percent too. Let’s try it with the results of the question “how frequently do you attend religious services?” that was asked in the 2024 ANES survey. We can use this command (notice how we are now writing cum.percent=TRUE because we are dealing with an ordinal variable): tab1(anes2024$religAttn, cum.percent=TRUE) ## anes2024$religAttn : ## Frequency %(NA+) cum.%(NA+) %(NA-) cum.%(NA-) ## every week 831 15.1 15.1 30.6 30.6 ## almost every week 559 10.1 25.2 20.6 51.2 ## once or twice a month 458 8.3 33.5 16.9 68.1 ## a few times a year 794 14.4 47.9 29.3 97.4 ## never 71 1.3 49.1 2.6 100.0 ## NA&#39;s 2808 50.9 100.0 0.0 100.0 ## Total 5521 100.0 100.0 100.0 100.0 R is giving us a lot of information here! The first column on the left is the possible values, ranging from “every week” to “almost never.” There are also some NAs – cases for which we do not have data on this question. The second column is how frequently each answer is given. Looking at that column, we can already tell that the modal value is “every week”; 831 of the 5521 people surveyed answered that they attend religious services every week. You can check this answer with the Mode command that we used above: Mode(anes2024$religAttn, na.rm=TRUE). The third column is the percent of cases with each value, including the NAs for which we have no information. The fourth column is the cumulative percent including NAs. This is the percentage of cases that got the value of interest or a lower value. So, 15.1% answered “every week”, 25.2% answered “almost every week” or “every week”, 33.5% answered “once or twice a month”, “almost every week”, or “every week”, etc. The fifth and sixth column repeat the third and fourth column, but they exclude the NAs. In general, you should use the columns that exclude NAs when interpreting your data. We can look at these cumulative percents to find the median. Focus on the column with NAs excluded – the right-most column. The first value for which the cumulative percent is 50 or higher is the median value of that variable. So, we can say that the median person in our attends religious services “almost every week.” We can also find the median of an ordered factor using the Median command (with a capital M) that comes with the DescTools package that we installed for thi.s chapter. If you copy and paste this command into your Console–Median(anes2024$religAttn,na.rm=TRUE)–you will get confirmation that the median of religAttn is “almost every week.” So, now we can make two observations about the central tendency of our religAttn variable – the mode is “every week” and the median is “almost every week.” What can we say about the dispersion of religAttn? To discuss the dispersion of an ordinal or nominal variable it is helpful to generate a bar graph. We can look at the one automatically generated by tab1, but I prefer the look of the ones that we generate with ggplot. So, we can use the same code from section 3.2.5 to generate a bar graph of this variable (Note that we make the x-axis table disappear with “NULL” because the graph title already explained what the x-axis means): ggplot(anes2024 %&gt;% filter(!is.na(religAttn)), aes(x=religAttn))+ geom_bar(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;How often Respondent attends religious services, 2024&quot;)+ xlab(NULL) What can we say about the dispersion of this variable? Well, based on the frequency table and bar graph we can say the variable is nearly bimodal and u-shaped. The two largest groups of respondents attend religious services either every week or a few times per year. 3.3.3 Central tendancy and dispersion of interval variables When we look at interval variables, we can use the mean, median, and mode to discuss the central tendency. R makes it easy to calculate the mean and median, with the command summary() that we learned in section 2.2 (you can also use the commands mean() and median() with the same result). So, for example, let’s look at the variable trumpMargin2024 in the states2025 dataset. This is the proportion of a state’s 2024 vote that went to Donald Trump minus the proportion of a state’s vote that went to Kamala Harris. In other words, it is the proportion by which Trump won or lost a given state. summary(states2025$trumpMargin2024) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.83800 -0.10950 0.03200 0.04945 0.21050 0.45800 Here, we see that the mean and median, .04945 and 0.03200, are quite close to each other. This is generally a sign that the variable is not skewed. How about the mode? To generate the mode, we can use the same command that we used for nominal and ordinal variables: Mode(states2025$trumpMargin2024,na.rm=TRUE) If you enter that command into the Console, you will that there are two values that occur twice in the dataset: .131 and .305. Those are both the modes. The mode of an interval variable generally gives us much less interesting information about its central tendancy than do the mean and median, and this case is no exception. How about the dispersion? With interval variables, we can use a number of strategies to represent the dispersion. We can have R calculate the standard deviation and the interquartile range (the 3rd quartile minus the 1st quartile), and we can generate a histogram. To generate the standard deviation, we can use sd(), and to generate the interquartile range, we can use IQR(): sd(states2025$trumpMargin2024,na.rm=TRUE) ## [1] 0.2347816 IQR(states2025$trumpMargin2024,na.rm=TRUE) ## [1] 0.32 We can also use the code from section 3.2.7 to visually display the central tendancy of our variable, like this: ggplot(states2025, aes(x = trumpMargin2024)) + geom_histogram(fill = &quot;purple&quot;, colour = &quot;black&quot;)+ ggtitle(&quot;Histogram of Trump&#39;s margin of victory in US states, 2024&quot;)+ xlab(NULL) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This histogram shows us that this variable is relatively bell-shaped with one very low outlier (one state where Kamala Harris beat Donald Trump by a lot). What might that state be? In the next section, we will learn to answer this question. 3.4 A note on using the states2025 and world2025 When looking at the states2025 and world2025 dataset, there are times when we might want to know which values of a variable come from which states or countries. When we want to know this, the select and arrange commands from the tidyverse can be helpful. For example, this code can help us learn about which states Trump lost by the largest amount: states2025 %&gt;% dplyr::select(state, trumpMargin2024) %&gt;% #focusing in on a few variables arrange(trumpMargin2024) #sorting this variable from smallest to largest #to sort from largest to smallest, you would write #arrange(desc(trumpMargin2024)) When you execute this code, what do you learn about the states that Trump and Harris won by the largest amounts in 2024? 3.5 Review of this chapter’s commands Command Purpose Library class() Tells us how a specific vector is categorized. Is it a character vector, an ordered factor, etc.? Base R ggplot() Begins a ggplot graphic. Feel free to refer to the “20093 ggplot2 templates” script file to find templates for all graphs you will make using this workbook ggplot2 (tidyverse) mean() Calculates the mean of a variable. Use na.rm=TRUE if there is some missing data, like this: mean(dataframe$variable, na.rm=TRUE). Base R median() Calculates the median of an interval variable. Use na.rm=TRUE if there is some missing data, like this: median(dataframe$variable, na.rm=TRUE). Base R Median() Calculates the median of a ordinal or interval variable. Use na.rm=TRUE if there is some missing data, like this: Median(dataframe$variable, na.rm=TRUE). DescTools Mode() Calculates the mode of a variable. Use na.rm=TRUE if there is some missing data, like this: Mode(dataframe$variable, na.rm=TRUE). DescTools sd() Calculates the standard deviation of a variable. Use na.rm=TRUE if there is some missing data, like this: sd(dataframe$variable, na.rm=TRUE). Base R IQR() Calculates the interquartile range of a variable. Use na.rm=TRUE if there is some missing data, like this: IQR(dataframe$variable, na.rm=TRUE). Base R You might notice that I labeled this graph “Histogram of countries’ populations, 2019” You can see that the population variable is from 2019 in the World 2025 dataframe’s codebook, which you dowloaded in section 1.3.↩︎ "],["recoding.html", "Chapter 4 Recoding and comparing values of variables 4.1 Getting started with this chapter 4.2 Recoding ordinal and nominal variables 4.3 Reversing the order of ordinal variables 4.4 Making ordinal variables numeric 4.5 Dealing with variables coded 0/1 4.6 Simplifying interval variables 4.7 Cross-tabulation tables 4.8 Mean comparison tables 4.9 Review of this chapter’s commands", " Chapter 4 Recoding and comparing values of variables 4.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. We are going to install three new packages to use today.They are called “epiDisplay”, “Hmisc”, and “flextable.” Enter these three commands into your Console one by one: install.packages(&quot;Hmisc&quot;) install.packages(&quot;tigerstats&quot;) install.packages(&quot;flextable&quot;) Now, open a new script file and save it in your scripts folder as “chapter 4 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 4, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(epiDisplay) #the tab1 command helps us make nice frequency tables library(Hmisc) #the cut2 command helps us simplify interval variables library(tigerstats) #colPerc can also be useful with crosstabs library(flextable) #this makes tables that we can easily export into a word processor Now select all the text on this page, run it, and save it. 4.2 Recoding ordinal and nominal variables There are times that you will want to change the values of variables. Perhaps you are interested in one particular value of a key variable (such as married people, or high-income countries). Perhaps your variable has some values which you would like to treat as missing. Or, perhaps you will want to simplify a variable, or change a variable from character to numeric or vice versa. When doing so, you will have two powerful allies: the mutate() command and the recode() command, both available through the dplyr package that comes with tidyverse. To start, let’s take a look at the ideology variable in the anes2024 dataset. We can use the tab1() command that we learned last lesson to look at how the values are distributed, running this command tab1(anes2024$ideology) ## anes2024$ideology : ## Frequency %(NA+) %(NA-) ## very liberal 250 4.5 5.3 ## liberal 819 14.8 17.3 ## slightly liberal 543 9.8 11.5 ## moderate 1187 21.5 25.1 ## slightly conservative 594 10.8 12.6 ## conservative 1030 18.7 21.8 ## very conservative 309 5.6 6.5 ## NA&#39;s 789 14.3 0.0 ## Total 5521 100.0 100.0 Looking at this output, we can see that there are seven possible values (excluding the NAs), ranging from “very liberal” to “very conservative.” There are times when we might want all of this detail, and we should not throw away data. However, we will often just want to compare liberals, moderates, and conservatives. To simplify this variable, we will first tell R to make a new version of the ANES dataframe that will house our new variable. Then, we will use mutate and recode to give values for our new variable. Here is what the code will look like: anes2024&lt;-anes2024 %&gt;% mutate(ideology3=recode(ideology, &#39;very liberal&#39;=&quot;liberal&quot;, &#39;liberal&#39;=&quot;liberal&quot;, &#39;slightly liberal&#39;=&quot;liberal&quot;, &#39;slightly conservative&#39;=&quot;conservative&quot;, &#39;conservative&#39;=&quot;conservative&quot;, &#39;very conservative&#39;=&quot;conservative&quot;)) There is a lot going on in the above command, so let’s try to break it down: POST OPERATION DATAFRAME&lt;-PRE OPERATION DATAFRAME %&gt;% mutate(NEW VARIABLE NAME=recode(OLD VARIABLE NAME, ‘OLD VARIABLE VALUE #1’=&quot;NEW VARIABLE VALUE #1&quot;, ‘OLD VARIABLE VALUE #2’=&quot;NEW VARIABLE VALUE #2&quot;,…)) Using mutate and recode together, we are generating a new version of our dataframe that has a new variable whose values are based on another variable. I have an important piece of advice when using this kind of command: be sure to copy and paste variable names and values from your Console screen to your script file, because it is very easy to make a typographical error when transcribing variable names and values. Even if you are careful, there will be times when you make an error in your code and R sends you an error message. Most of the time, you will be able to quickly figure out the source of your error, but if you ever cannot, please feel free to ask your instructor! Sometimes, you will realize that your error modified the dataframe in ways that you don’t like. In that case, you can go back to the original dataframe. If you were using anes2024, you can use this command to restore that dataframe: load(&quot;Data/anes2024.Rda&quot;) Or, if that command doesn’t work for you, you can follow the instructions in section 1.6 to reload the original dataframe. To confirm that our recode worked, we can run “table” on both our old and our new variable, like this: table(anes2024$ideology) table(anes2024$ideology3) When run each of these commands, you can see that our recoding worked. You should have have 1187 moderates in each variable. And, if we add the numbers of very liberal, liberal, and slightly liberal respondents (which we can do in R!), we can see that 250+819+543=1612. The same is also true with the conservatives. What if we were interested in examining the horseshoe theory – that people on the extremely left and extreme right have some major things in common? In that case, we might want to generate a new variable which measures whether or not someone is an extremist. To do that, we could do another version of the mutate/recode command like we did above. But that would require us to type all seven values of our original variable into our code again, which takes a little while. Instead, we could also use the “.default” qualifier to tell R: assign this value to all other cases. Here is how that code would look: anes2024&lt;-anes2024 %&gt;% mutate(extremist=recode(ideology, &#39;very liberal&#39;=&quot;extremist&quot;, &#39;very conservative&#39;=&quot;extremist&quot;, .default=&quot;not extremist&quot;)) Now, when we run the command: table(anes2024$extremist) ## ## extremist not extremist ## 559 4173 we can see that we successfully created our new variable. 4.3 Reversing the order of ordinal variables Ordered factors in R have “levels” that are always displayed in the same order (it might be helpful to think of them as going from low to high). To see the levels of the ANES’s ideology variable, we can use this command: levels(anes2024$ideology) ## [1] &quot;very liberal&quot; &quot;liberal&quot; &quot;slightly liberal&quot; ## [4] &quot;moderate&quot; &quot;slightly conservative&quot; &quot;conservative&quot; ## [7] &quot;very conservative&quot; The output after this command shows that the variable is set up so that “very liberal” is the “lowest” value (it comes first in the list), and “very conservative” is the “highest” value (it comes last in the list). However, what if we were writing a paper that focused on liberals and liberalism? In that case, we might want an ideology variable which treated these values in the opposite order, so that “very liberal” is the highest value on our scale. Here is how we would create a new ordered factor called “liberalism” which is the ideology variable reversed: #first we create a vector called v which is the levels of our ideology variable v&lt;-levels(anes2024$ideology) #second we reverse the order of v, telling R to make the largest value the smallest and the smallest the largest v&lt;-v[length(v):1] #third, we use reorder to generate a new factor called liberalism with the opposite levels as our ideology variable anes2024$liberalism&lt;-reorder(anes2024$ideology,new.order=v) #finally, we can remove our vector v, since we don&#39;t need it anymore rm(v) To make sure that your new variable worked, you can enter table(anes2024$ideology) and table(anes2024$liberalism) into your console and see that both variables have 309 people that called themselves “very conservative” and 250 that called themselves “very liberal.” 4.4 Making ordinal variables numeric Sometimes you might want to make an ordered factors a number that you can analyze mathematically. So, for example, let’s imagine that we wanted to be able to think of the liberalism variable that we just generated as a number rather than the words “very conservative,” “conservative,” etc. R makes this very easy to do, with the code “as.numeric.” You can make a variable called liberalism.n like this: anes2024$liberalism.n&lt;-as.numeric(anes2024$liberalism) That code generates a new variable called liberalism.n with values from 1 (which we know from the levels of our liberalism variable means “very conservative”) to 7 (which means “very liberal”). This variable will be slightly easier to deal with in future chapters if the lowest item on the scale is 0, instead of 1. So, to make that happen, we can ask R to subtract one from every value of the liberalism.n variable like this: anes2024$liberalism.n&lt;-anes2024$liberalism.n-1 Now if you run table(anes2024$liberalism.n), you will see that your new variable has values ranging from 0 to 6, and that the number of cases with each value is the same as they are in the liberalism variable. 4.5 Dealing with variables coded 0/1 When looking at data in the real world, we will often find variables coded with only two possible values – 0 or 1. These variables are sometimes called dummy variables or binary variables. In general, these variables are used to mean the presence or absence of some characteristic. For example, either someone is married (1), or they are not (0). Either a country is a democracy (1) or it is not (0). On the anes2024 dataframe, I have coded the variable “female” in this way. If you refer to the codebook, you can see that 0 means male and 1 means female. It will be useful mathematically to have it coded this way in future labs. But it will be less useful when generating graphics, since the “1” and “0”s produced on the axis and in the legends will not be that informative. So, we may want to generate a “sex” variable that uses words instead of numbers. We can do so with this code: anes2024&lt;-anes2024 %&gt;% mutate(sex=recode(female, &#39;1&#39;=&quot;female&quot;,&#39;0&#39;=&quot;male&quot;)) We can check our results with this code (we could also use tab1 instead of table here).: table(anes2024$female) table(anes2024$sex) You should see that R reports the same number as 1s with the old “female” variable” as it does “female” with the new “sex” variable. and the same should be true of 0s and “male”. 4.6 Simplifying interval variables 4.6.1 Simplifying interval variables using cut2() Interval variables give us a lot of information, and it is sometimes too much! For example, we might want to generate a graph comparing age groups, rather than a graph of individuals at each age between 18 and 80. This is where the cut2() command from the Hmisc package can come in handy. Let’s say we want to create three roughly evenly distributed age groups. We can use cut2 like this: anes2024$age3&lt;-cut2(anes2024$age, g=3) The g=3 is telling R to divide age into three roughly equal groups and make them into a new variable called age3. If we wanted 4 groups, or 2 groups, or 5 groups, we could just change that number. Now let’s run table() on our new variable: table(anes2024$age3) ## ## [18,44) [44,65) [65,80] ## 1779 1772 1691 This output is telling us that our new variable has three values—[18,44), [42,65), and [65,80]. This notation means that the first group has 18-year-olds to those younger than 44 (that is what the “)” means), the second group has 42-year-olds to those younger than 65 (again, we learn this from the closed parenthesis), and the third group has 65-year-olds and older. We could use mutate(recode()) on those value names, like we did above, but here is an easier solution. The cut2 command generates an ordered factor, which is how R likes to treat ordinal variables. Let’s take a look at the levels of our new age variable: levels(anes2024$age3) ## [1] &quot;[18,44)&quot; &quot;[44,65)&quot; &quot;[65,80]&quot; This output means that [18,44) is the “low” level of the new age3 variable, [44,65) is the medium level, and [65,80] is the high level. To rename those variables, we can just make a list using the c() command that we learned in lab 1, like this: levels(anes2024$age3)&lt;-c(&quot;young&quot;, &quot;middle aged&quot;, &quot;old&quot;) When we run that command, R renames the levels of our age3 variable. When renaming the levels of a variable, be sure to list the new levels in an order that corresponds with the previous levels of the variable. If you typed “old” first, in the command below, you would be telling R to call people from 18-44 old, and you would also make “old” the lowest level of your age3 variable. To confirm that this worked, you can type table(anes2024$age3) and you should see your new labels. 4.6.2 Simplifying interval variables with case_when In section 4.6 we learned how to use the cut2 command to divide interval variables into groups of roughly the same size. But what if we are interested in separating interval variables into groups above or below specific values? For example. we might be interested in looking at people in their teens, in their 20s, in their 30s, etc. To do that, we can write the following: anes2024&lt;-anes2024 %&gt;% mutate(age_groups=case_when(age %in% 18:19 ~ &quot;teens&quot;, age %in% 20:29 ~ &quot;20s&quot;, age %in% 30:39 ~ &quot;30s&quot;, age %in% 40:49 ~ &quot;40s&quot;, age %in% 50:59 ~ &quot;50s&quot;, age %in% 60:69 ~ &quot;60s&quot;, age %in% 70:80 ~ &quot;over 70&quot;)) This code uses mutate, which we first saw in section 4.2, to create a new variable called age_groups. We set the values for our new variable with a new command that is available through the tidyverse package, case_when(). This command lets us create variable values based on ranges of numbers from our old variable (which we identify with the %in% and colons between the values). We can take a look at our new variable using table, as follows: table(anes2024$age_groups) ## ## 20s 30s 40s 50s 60s over 70 teens ## 500 841 874 802 1014 1162 49 You might notice that the values for our new variable are listed in a strange order, with 20s first and teens last. This is because our new variable is a character variable, and the values are thus listed in in alphabetical order. To fix this, we can make our new variable into an ordered factors and set the levels from youngest to oldest like this: anes2024$age_groups&lt;-ordered(anes2024$age_groups, levels=c(&quot;teens&quot;,&quot;20s&quot;,&quot;30s&quot;,&quot;40s&quot;,&quot;50s&quot;,&quot;60s&quot;,&quot;over 70&quot;)) Now when we enter table(anes2024$age_groups) into the Console we can see the variable values listed in a way that makes more sense. This will also be useful when making tables and graphs. 4.7 Cross-tabulation tables 4.7.1 Generating cross-tabs Up until now, we have focused on one single variable at a time. This is an essential component of social scientific analysis. However, most of the questions that we are interested in are about the relationship between two or more variables. The rest of the work that we will do in R from this point forward will be geared toward helping us to make observations and inferences about the relationship between more than one variable. One way that we can start to examine the relationship between more than one variable is by making cross-tabulation tables (often called crosstabs). These are essentially two frequency tables presented in a table. Crosstabs are most useful when looking at the relationship between twp ordinal or nominal variables. For example, let’s look at the relationship between education and income in the anes2024 dataframe. We can hypothesize that people with more education will make more income. To test this, we can first use the xtabs() command, like this: xtabs(~income+education, data=anes2024) ## education ## income less than high school high school some college ba ## under $9,999 81 137 131 68 ## $10,000 to $29,999 65 156 239 68 ## $30,000 to $59,999 57 271 387 205 ## $60,000 to $99,999 33 191 391 297 ## $100,000 to $249,999 19 131 423 548 ## $250,000 or more 1 22 70 110 ## education ## income graduate degree ## under $9,999 47 ## $10,000 to $29,999 25 ## $30,000 to $59,999 98 ## $60,000 to $99,999 188 ## $100,000 to $249,999 488 ## $250,000 or more 188 Let’s break down how we used this command: we put the dependent variable, income, first, and then the independent variable, education, second. Crosstabs generally have the independent variable in columns and the dependent variable in rows. This setup lets us do that. 4.7.2 Adding column totals to cross-tabs The output that our command generated is a basic crosstab. It tells us that 81 people with less than a high school education make under $9,999 per year, 65 people with less than a high school education make $10,000-29,999, etc. Now we might be tempted to look at this and say: “Graduate school is a waste of time! 488 people from our sample with graduate degrees make $100,000 to $250,000, while 548 people a BA make $100,000 to $250,000.” However, that would be a mistake! To understand why, we should look at the column totals from our crosstab, which we can produce with this command: addmargins(xtabs(~income+education, data=anes2024)) ## education ## income less than high school high school some college ba ## under $9,999 81 137 131 68 ## $10,000 to $29,999 65 156 239 68 ## $30,000 to $59,999 57 271 387 205 ## $60,000 to $99,999 33 191 391 297 ## $100,000 to $249,999 19 131 423 548 ## $250,000 or more 1 22 70 110 ## Sum 256 908 1641 1296 ## education ## income graduate degree Sum ## under $9,999 47 464 ## $10,000 to $29,999 25 553 ## $30,000 to $59,999 98 1018 ## $60,000 to $99,999 188 1100 ## $100,000 to $249,999 488 1609 ## $250,000 or more 188 391 ## Sum 1034 5135 4.7.3 Adding column percents to cross-tabs Take a look at the above column totals. Our anes2024 dataframe has 1,034 people with graduate degrees, while it has 1,296 people with BAs. So, returning to those two numbers from above, the 488 people with graduate degrees with household incomes of $100,000 to $250,000 make up 47.2% of all of graduate degree holders in our sample (because 488/1034=.472), while the 548 people with BAs with household incomes of $100,000 to $250,000 make up only 42.3% of all of the people with some college in our sample (because 528/1296=.423). In other words, there is a larger proportion of people with graduate degrees with household incomes of $100,000 to $250,000 than people with BAs with household incomes of $100,000 to $250,000. Thus, when interpreting crosstabs it is important to look at the column percents, and not just the raw numbers. To make R generate all of the column percents in a crosstab at the same time, we can use the colPerc() command from the tigerstats package that we installed for today. We can enter this command: colPerc(xtabs(~income+education, data=anes2024)) ## education ## income less than high school high school some college ba ## under $9,999 31.64 15.09 7.98 5.25 ## $10,000 to $29,999 25.39 17.18 14.56 5.25 ## $30,000 to $59,999 22.27 29.85 23.58 15.82 ## $60,000 to $99,999 12.89 21.04 23.83 22.92 ## $100,000 to $249,999 7.42 14.43 25.78 42.28 ## $250,000 or more 0.39 2.42 4.27 8.49 ## Total 100.00 100.00 100.00 100.00 ## education ## income graduate degree ## under $9,999 4.55 ## $10,000 to $29,999 2.42 ## $30,000 to $59,999 9.48 ## $60,000 to $99,999 18.18 ## $100,000 to $249,999 47.20 ## $250,000 or more 18.18 ## Total 100.00 Remember when interpreting this output that the numbers are now the column percents, not the raw numbers. If you take a look at this output, you will see that people with more education seem to make more money. If you look at the top income category row (for those with household incomes of $250,000 or more, the percent of thsoe at the lowest education level, “less than high school” in that income category is 0.39%, and as the education level increases this percent increase to 2.42%, 4.27%, 8.49%, and 18,18% for those with graduate degrees. 4.7.4 Exporting cross-tabs for your word processor If you have been following along, the cross-tabs that you have generated have shown up reasonably neatly in the Console of R-Studio. But often, you will want to export those tables into a format that you can easily bring into your word processor so that you can refer to it in papers that you are writing. To do this, you first want to give the cross tab you want to export a name as an object, like this: ctab1&lt;-colPerc(xtabs(~income+education, data=anes2024)) That command creates a new object called “ctab1.” If you paste ctab1 into your Console and hit enter, you will see that same crosstab that you made in section 4.7.3. Before exporting this crosstab, you have to make two changes to it in order for the export command that we will use to work. You need to change it to a dataframe, and you need to set the row names (which are currently the different income levels) as a column. To do both of those things, you can run this command: ctab1&lt;-as.data.frame(ctab1)%&gt;%rownames_to_column(&quot;income&quot;) Now your “ctab1” is formatted to be exported nicely (when you use this with other tables, just rename the text in quotes after “rownames_to_columns” to fit what you’d like to call your rownames). To export this, run this command: flextable(ctab1) %&gt;% save_as_docx(path=&quot;ctab1.docx&quot;) This has created a file called “ctab1.docx” in the main folder where you have your Scope and Methods labs (the folder that you created in section 1.2). You can open that file in Microsoft Word, or you can pull it into a Pages file or a Google Document. If you’d rather export your table into a rich text format file, you can use save_as_rtf instead of save_as_docx in the above command. 4.8 Mean comparison tables Crosstabs are useful when we have two nominal or ordinal variables. But what if we want to look at the relationship between a nominal or ordinal variable and an interval variable? In that case, a mean comparison table can be helpful. The tidyverse has some commands that we can use to generate such a table. To demonstrate, let’s look at our states2025 dataframe, and the relationship between the region variable and the govApproval2024 variable (which measures the proportion of the population that approved of a state’s governor in 2024). The first thing that we need to learn is the group_by() command. This tells R: “make groups based on the variable that we tell you.” In our case, we want to make groups based on our independent variable (which, for now, must be ordinal or nominal). Once we have done that, we can use the summarise() command to get the mean. For good measure, we can also use the summarise() command to calculate the standard deviation as well as the size of each group. Here is what that command looks like: states2025 %&gt;% group_by(region) %&gt;% summarise(mean=mean(govApproval2024), sd=sd(govApproval2024), n=n()) ## # A tibble: 4 × 4 ## region mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 midwest 0.348 0.0553 12 ## 2 northeast 0.388 0.0545 9 ## 3 south NA NA 17 ## 4 west 0.335 0.0380 13 We can see that, in the mean midwest state, .348 (or 34.8%) of the population approved of the governor, with a standard deviation of .0553, and .388 of the population of the mean northeast state (or 38.8% approved of the government) approved of the governor, with a standard deviation of .0545. The reason that the South is NA is because there is a Southern “state” without a governor – the District of Columbia not technically a state). To filter out states without data on that variable, we can add another line to our command: filter(!is.na(govApproval2024))%&gt;% (we used a version of this command in section 3.2.1). Our new command looks like this: states2025 %&gt;% filter(!is.na(govApproval2024)) %&gt;% group_by(region) %&gt;% summarise(mean=mean(govApproval2024), sd=sd(govApproval2024), n=n()) ## # A tibble: 4 × 4 ## region mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 midwest 0.348 0.0553 12 ## 2 northeast 0.388 0.0545 9 ## 3 south 0.347 0.0597 16 ## 4 west 0.335 0.0380 13 Now we can see that northeast states seem to have the highest approval of their governors, and also that there is no region where the majority of people approve of the governor in the average state. To export this table so that we can easily pull it into our word processor, we can do something similar to what we did in section 4.7.4. However, it is easier to export a mean comparison table because we don’t have to first tell R to treat it as a dataframe. You can export the file with the following code: mc1&lt;-states2025 %&gt;% filter(!is.na(govApproval2024)) %&gt;% group_by(region) %&gt;% summarise(mean=mean(govApproval2024), sd=sd(govApproval2024), n=n()) flextable(mc1) %&gt;% save_as_docx(path=&quot;mc1.docx&quot;) Now you should see a file called “mc1.docx” in the main folder where you have your Scope and Methods labs (the folder that you created in section 1.2). 4.9 Review of this chapter’s commands Command Purpose Library mutate(recode()) A set of commands to rename the values of variables. dplyr (tidyverse) cut2() Turn an interval variable into an ordinal variable. Remember to specify “g=” to tell R how many groups to split your variable into. Hmisc levels() You can use this command to see the levels of an ordered factor, or you can also use it to assign new levels to an ordered factor, like this: levels(df$variable&lt;-c(“new level1”,“new level2”,…) Base R reorder() You can use this command to change the order of levels in an ordered factor Base R as.numeric You can use this command to make a factor into a numeric variable, like this: df$NewVariable&lt;-as.numeric(df$Variable) Base R case_when() You can use this command along with mutate() and %in% to assign ranges of an interval variable to a new value of a new variable dplyr (tidyverse) ordered you can use this command to make a variable into an ordered factor Base R xtabs() Makes a crosstab table with raw values. Remember to define variables like this: xtabs(~DV+IV, data=DATAFRAME) Base R addmargins(xtabs()) Makes a crosstab table with row and column totals. Base R colPerc(xtabs()) Makes a crosstab table with column percents. Useful for making comparisons. tigerstats flextable You can use this with %&gt;% save_as_docx(path=“…”) to export your tables for word processors flextable group_by() A dplyr command that is useful when summarizing a dataframe according to a nominal or ordinal variable. Usually used with the “pipe” (%&gt;%). dplyr (tidyverse) summarise() A dplyr command that is useful when summarizing a dataframe according to a nominal or ordinal variable. Usually used with the “pipe” (%&gt;%). dplyr (tidyverse) "],["making-controlled-comparisons.html", "Chapter 5 Making controlled comparisons 5.1 Getting started with this chapter 5.2 Representing crosstabs with stacked bar graphs 5.3 Representing mean comparison tables with boxplots 5.4 Adding a control to a crosstab 5.5 Adding a control to a mean comparison table 5.6 Review of this chapter’s commands", " Chapter 5 Making controlled comparisons 5.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now, open a new script file and save it in your scripts folder as “chapter 5 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 5, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap library(tigerstats) #colPerc can also be useful with crosstabs library(flextable) #this makes tables that we can easily export into a word processor Now select all the text on this page, run it, and save it. 5.2 Representing crosstabs with stacked bar graphs In chapter 4 we learned how to generate crosstabs (and, crucially for interpretation, crosstabs with percents in columns). Crosstabs themselves have a lot of numbers on them, and can thus be difficult to interpret on their own. Many social scientists address this problem by generating graphs. A “stacked bar graph” is a really nice tool for visually depicting relationships in a bar graph. The code to generate stacked bar graphs is a little bit long and complex, so just feel free to copy and paste the below or the code from the “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3. Let’s return to the relationship that we were looking at in section 4.7, between education (our independent variable) and income (our dependent variable). Here is the code to generate a stacked bar graph of this relationship: #first, generate a dataframe called &quot;plotting.data&quot; based on grouped variables from #anes2024 plotting.data&lt;-anes2024 %&gt;% filter(!is.na(education)&amp;!is.na(income)) %&gt;% group_by(education, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) #second, make the graph ggplot(plotting.data, aes(x = education, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ #this line is in here because without it the x-axis labels overlap. You can remove it if that isn’t a problem scale_fill_viridis_d(name=&quot;income&quot;)+ #this color palate is made to be readable for colorblind people ggtitle(&quot;Education and income in the 2024 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) One thing to note in this code: it begins by creating a new dataframe called “plotting.data.” When you run it, that dataframe will show up in your RStudio environment. If you run it again with new variables, the new “plotting data” that you make will overwrite the old one. If it bothers you having that “plotting.data” object in your environment, you can always remove it with the command rm(plotting.data). Another thing to note: the “scale_fill_viridis_d()+” line of code asks R to use a color palette that is easy to read for people that are colorblind. There are many, many choices of color palette in R, and you are welcome to play around with colors, but it is also fine to just stick to this palette. If you’d rather have a black and white graph, there is code for that in the “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3. Looking at the graph, we can see that our X-axis is our independent variable, education, and our Y-axis is the column percentages of our dependent variable, income. Focusing on the yellow region, we can clearly see that each increase in education seems to correspond to an increase in the percent of people in the highest income earning category (with the possible exception of the movement between some college and associate degree). Focusing on the dark blue region, we can clearly see that, as level of education increases, the percent of people in the lowest income category decreases. In short, this graph has all of the same information that we got when looking at the crosstab that we generated with colPerc(xtabs(~income+edu, data=anes2024)) from our previous lab, but it is much easier to take in a bunch of that information at the same time. 5.3 Representing mean comparison tables with boxplots In addition to the histogram that we introduced in chapter 3, another useful way of representing the distribution of a single interval variable is with a boxplot. Here is the code for a boxplot of the variable (you can also find this code in the “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3): #make a boxplot of govapproval ggplot(states2025, aes(y = govApproval2024)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ scale_x_continuous(breaks = NULL) + ggtitle(&quot;Approval of governors in US states&quot;)+ xlab(NULL)+ ylab(&quot;Proportion of state residents\\napproving of governor&quot;)#the \\n forces a second line in the axis title A boxplot gives us some interesting information about an interval variable. The bottom of the box is the 1st quartile (25% of cases have that value or lower) and the top of the box represents the 3rd quartile (75% of cases have that value or lower). The horizontal line represents the median value, and the vertical lines coming out of the box extend to the first quartile minus 1.5 multiplied by the interquartile range and the third quartile plus 1.5 multiplied by the interquartile range. If there were any outlier points above or below this line, they would be identified by individual points on the graph. The Here is a labeled version of another boxplot for your reference:4 So, in other words, we can learn a lot about a variable from a single graph! Boxplots are also a useful way to look at the properties of an interval variable across several values of a nominal or ordinal variable, which is what we were doing with our mean comparison table in chapter 4. Let’s take a graphical look at the relationship between the region that a state is in and approval of the governor. We can set up that kind of graph with the following code: #make boxplots of regions and govapproval ggplot(states2025, aes(x = region, y = govApproval2024)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ ggtitle(&quot;Region and gubernatorial approval in US states&quot;)+ xlab(&quot;Region&quot;)+ ylab(&quot;Proportion of state residents that\\napprove of their governor, 2024&quot;) This graph shows us a lot of cool things. We can see that there is less variation among the states in the West than other reasons, and that altough the West has the lowest median approval the south appears to have the states with the lowest approval overall. 5.4 Adding a control to a crosstab So far, we have learned to look at the relationship between pairs of variables. However, in real social scientific analysis we will often be interested in more than that. We will be interested in other factors that could be the true cause of variation in our dependent variable. If we look at the relationship between our independent and dependent variable at different values of a control variable and there no longer seems to be a relationship, then we can say that the correlation that we observe between our independent and dependent variable is spurious. If the relationship between the independent and dependent variable is roughly the same at different values of a control, we call the relationship between the independent and dependent variables, controlling for the third, additive. If the relationship between the independent and dependent variables is different at different values of a control, we call that relationship an interaction. Let’s return to the relationship that we looked at earlier, between education and income. Here is the crosstab that we generated in chapter 4: ## education ## income less than high school high school some college ba ## under $9,999 31.64 15.09 7.98 5.25 ## $10,000 to $29,999 25.39 17.18 14.56 5.25 ## $30,000 to $59,999 22.27 29.85 23.58 15.82 ## $60,000 to $99,999 12.89 21.04 23.83 22.92 ## $100,000 to $249,999 7.42 14.43 25.78 42.28 ## $250,000 or more 0.39 2.42 4.27 8.49 ## Total 100.00 100.00 100.00 100.00 ## education ## income graduate degree ## under $9,999 4.55 ## $10,000 to $29,999 2.42 ## $30,000 to $59,999 9.48 ## $60,000 to $99,999 18.18 ## $100,000 to $249,999 47.20 ## $250,000 or more 18.18 ## Total 100.00 Let’s look at the row representing the people who make $250,000 or more per year. As we move from the lowest level of education to the highest, we can see that the percent of people in the highest income category moves from .39% to 18.18%. We can actually subtract .39 from 18.18 to say that the effect size is roughly 17.79%. In other words, 17.79% more people with the highest degree of education are in the highest income category than are people in the lowest education category. Do we think that this result would be roughly the same for men and women? To answer this question, we can use a neat tool that we have actually played with a bit in previous chapters: the “filter” command that is a part of the dplyr package (and thus works with pipes). The ANES dataset has a variable called female, where 0 means “men” and 1 means “women.” Let’s have R make that crosstab again, but only for men, and then again, but only for women. To do that, we can use this code: #men only colPerc(xtabs(~income+education, data=anes2024 %&gt;% filter(female==0))) ## education ## income less than high school high school some college ba ## under $9,999 27.05 12.31 6.29 4.19 ## $10,000 to $29,999 18.03 16.14 10.16 4.35 ## $30,000 to $59,999 28.69 30.79 22.03 14.35 ## $60,000 to $99,999 16.39 22.08 25.89 20.81 ## $100,000 to $249,999 9.02 16.77 30.76 46.94 ## $250,000 or more 0.82 1.91 4.86 9.35 ## Total 100.00 100.00 100.00 100.00 ## education ## income graduate degree ## under $9,999 4.09 ## $10,000 to $29,999 1.23 ## $30,000 to $59,999 9.41 ## $60,000 to $99,999 14.93 ## $100,000 to $249,999 48.88 ## $250,000 or more 21.47 ## Total 100.00 A quick note: if we wanted to use filter with a variable whose values are defined by text, such as our income variable, we must put the text in quotation marks. Here, we do not have to use quotation marks because female is defined numerically. If we do the same comparison that we did above, we can see that as men more from the lowest to the highest education category, the percent in the highest income category increases from .82% to 21.47%. The total effect size is about 20.65%, which is a bit higher than the 17.79% overall effect size that we witnessed above. To save this crosstab as in a word file that we can easily pull into various word processors, we can use this code, based on what we did in Section 4.7.4. ctab1men&lt;-colPerc(xtabs(~income+education, data=anes2024 %&gt;% filter(sex==0))) ctab1men&lt;-as.data.frame(ctab1men)%&gt;%rownames_to_column(&quot;income&quot;) flextable(ctab1men) %&gt;% save_as_docx(path=&quot;ctab1men.docx&quot;) Now let’s look at women only: #women only colPerc(xtabs(~income+education, data=anes2024 %&gt;% filter(female==1))) ## education ## income less than high school high school some college ba ## under $9,999 35.34 17.89 9.25 6.23 ## $10,000 to $29,999 32.33 18.35 17.85 5.93 ## $30,000 to $59,999 16.54 28.90 24.76 17.21 ## $60,000 to $99,999 9.77 19.95 22.32 24.78 ## $100,000 to $249,999 6.02 11.93 22.00 38.13 ## $250,000 or more 0.00 2.98 3.83 7.72 ## Total 100.00 100.00 100.00 100.00 ## education ## income graduate degree ## under $9,999 4.97 ## $10,000 to $29,999 3.50 ## $30,000 to $59,999 9.39 ## $60,000 to $99,999 21.18 ## $100,000 to $249,999 45.86 ## $250,000 or more 15.10 ## Total 100.00 Applying the same analysis that we did above, we can see that as women’s education level increases from the lowest to the highest level, the percent of people in the highest income bracket moves from 0% to 15.1%; a total effect size of 15.1% (compared with 20.65% for men). In other words, this data suggests that both men and women are economically rewarded for more education, but that men seem to be rewarded a bit more. If we were to write up these results, at this point we would have to use our own judgment to decide whether the effect size differs enough to call the relationship between education and income controlling for gender an interaction, or whether it makes more sense to call it additive. We can use more advanced statistical tools to make that determination with more rigor, but for now, we will rely on our judgment. We can also save this table as a Word document, using the following code: ctab1women&lt;-colPerc(xtabs(~income+education, data=anes2024 %&gt;% filter(female==1))) ctab1women&lt;-as.data.frame(ctab1women)%&gt;%rownames_to_column(&quot;income&quot;) flextable(ctab1women) %&gt;% save_as_docx(path=&quot;ctab1women.docx&quot;) There is not a single tool to graph relationships between three ordinal or nominal variables in an easy to interpret way. Instead, we can use the filter() command from above to make two graphs, and then use insert table in our word processor to nearly display them side by side. Here is the code for the graph for only men (with the line “filter(female==0) %&gt;% #filtering so that we only look at men” added): plotting.data&lt;-anes2024 %&gt;% filter(!is.na(income)&amp;!is.na(education)) %&gt;% filter(female==0) %&gt;% #filtering so that we only look at men group_by(education, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ggplot(plotting.data, aes(x = education, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d()+ ggtitle(&quot;Education and income for men in the 2024 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) And here is the code for the graph for only women: plotting.data&lt;-anes2024 %&gt;% filter(!is.na(income)&amp;!is.na(education)) %&gt;% filter(female==1) %&gt;% #filtering so that we only look at women group_by(education, income) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ggplot(plotting.data, aes(x = education, y = freq, fill=income)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d()+ ggtitle(&quot;Education and income for women in the 2024 ANES&quot;)+ xlab(&quot;Education&quot;)+ ylab(NULL) 5.5 Adding a control to a mean comparison table The ANES survey has a number of “feeling thermometer” questions, where respondents were asked how warmly or coldly they felt toward institutions, individuals, and groups of people. These questions all range from 0 (meaning that the respondent feel very negatively toward the subject), to 100 (meaning that the respondent feels very positively about the subject). For this section, I am going to treat the feeling thermometer about the police as my dependent variable. For my independent variable, I am going to look at whether or not a person watches NCIS, a TV drama that, according to the Internet Movie Database, “Follows the Major Case Response Team (MCRT) from the Naval Criminal Investigative Service (NCIS), as they get to the bottom of criminal cases connected to Navy and Marine Corps personnel.” Do people that watch that show, which portrays the Naval Police as its protagonists, like the police more than people that don’t watch that show? The ncis variable is currently coded as as a dummy variable, with those who don’t watch the show getting a 0 and those who watch the show getting a 1. To make our output easier to read, I am going to generate a new version of that variable with the values “watches NCIS” and “doesn’t watch”, using the same technique that I used in section 4.5. Note that I call the new variable ncis2, so that I preserve the original ncis variable instead of writing over it: anes2024&lt;-anes2024 %&gt;% mutate(ncis2=recode(ncis, &#39;1&#39;=&quot;watches NCIS&quot;,&#39;0&#39;=&quot;doesn&#39;t watch&quot;)) If you runtable(anes2024$ncis2), you will see 4,753 people in our sample don’t regularly watch NCIS, and 523 do regularly watch NCIS. Now, let’s make a mean comparison table to see how the mean police feeling thermometer score differs between NCIS watcher and non-watchers: anes2024 %&gt;% filter(!is.na(ft_police)&amp;!is.na(ncis2)) %&gt;% group_by(ncis2) %&gt;% summarise(mean=mean(ft_police), sd=sd(ft_police), n=n()) ## # A tibble: 2 × 4 ## ncis2 mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 doesn&#39;t watch 69.0 24.5 4226 ## 2 watches NCIS 76.1 21.1 474 This tells us that people that watch NCIS give the police a mean score of 76.1 on average, and people that don’t watch the show give the police a mean score of 69. The standard deviations are both pretty large, which suggests that there is a lot of variation in feelings about the police among both NCIS watchers and non-watchers. To graph this relationship, we can use this code: ggplot(anes2024 %&gt;% filter(!is.na(ncis2)) %&gt;% filter(!is.na(ft_police)), aes(x = ncis2, y = ft_police)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;purple&quot;, alpha=0.2)+ ggtitle(&quot;ANES 2024, Feelings about police by NCIS viewership&quot;)+ xlab(NULL)+ #I use null here because the value labels for my NCIS variable explain what is going on to readers, so an additional axis label is not necessary ylab(&quot;Feelings about police\\n(100 is warm and 0 is cold)&quot;) This graph shows us that the feeling thermometer scores for the police seem to have ranged from 0 to 100 for both NCIS viewers and non-viewers. However, the median score for non-NCIS watchers appears to be right around the first quartile for NCIS watchers. This graph does suggest that, overall, NCIS watchers seem to like the police more than non-watchers. When doing this analysis, I wondered whether what is really going might have to do with ideology. Maybe conservatives are more likely to both watch NCIS and like the police than are liberals. To test this hypothesis, I took a look at the ideology3 variable that I generated in section 4.2.5 Now let’s add our control variable to our mean comparison table. It is easier to add a control variable into a mean comparison table than it was to add a control variable into our cross tab. To do so, we can use the following code: anes2024 %&gt;% #first I filter out the NAs for all 3 variables filter(!is.na(ft_police)&amp;!is.na(ideology3)&amp;!is.na(ncis2)) %&gt;% #then I group by the control first, and then the IV group_by(ideology3, ncis2) %&gt;% #then I summarise in the same way that I do with a two-variable table summarise(mean=mean(ft_police), sd=sd(ft_police), n=n()) ## # A tibble: 6 × 5 ## # Groups: ideology3 [3] ## ideology3 ncis2 mean sd n ## &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 liberal doesn&#39;t watch 58.4 25.3 1334 ## 2 liberal watches NCIS 67.9 23.1 103 ## 3 moderate doesn&#39;t watch 67.8 22.8 906 ## 4 moderate watches NCIS 74.8 20.4 104 ## 5 conservative doesn&#39;t watch 79.9 18.9 1472 ## 6 conservative watches NCIS 83.3 17.1 181 The comments explain what is happening in the code. It is almost the same as the code for a two variable mean comparison table, but I group by the control (first) and then the independent variable, rather than just grouping by the independent variable, as I did in the two-variable code. Let’s look at this output. First, focus on the first two rows. They tell us that liberals who watch NCIS feel 9.5 points more warmly than liberals that don’t watch NCIS (because 67.9-58.4=9.5). But among moderates, the effect size is smaller (7), and among conservatives, the effect size is even smaller still (3.4). Thus, we can say that the relationship between NCIS watching and feelings about the police is different for respondents with different ideological backgrounds. We thus call this an interactive relationship. We can also notice that liberals who watch NCIS feel almost exactly the same about the police than do moderates who do not watch the show, and moderates who watch NCIS feel 5.1 points more coldly toward the police than do conservatives who do not watch the show. In other words, ideology also appears to be related to feeling about the police. We can save this three variable mean comparison table like this: mc2&lt;-anes2024 %&gt;% filter(!is.na(ft_police)&amp;!is.na(ideology3)&amp;!is.na(ncis2)) %&gt;% group_by(ideology3, ncis2) %&gt;% summarise(mean=mean(ft_police), sd=sd(ft_police), n=n()) flextable(mc2)%&gt;% save_as_docx(path=&quot;mc2.docx&quot;) To graphically represent this relationship, we can use this code (which you can also find in the “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3): ggplot(anes2024 %&gt;% filter(!is.na(ideology3) &amp; (!is.na(ncis2))), aes(x = ideology3, y = ft_police, fill=ncis2)) + geom_boxplot()+ ggtitle(&quot;ANES 2024, Feelings about police by ideology\\nand NCIS viewership&quot;)+ xlab(NULL)+ ylab(&quot;Feelings about the police\\n(100 is warm and 0 is cold)&quot;)+ scale_fill_discrete(name = &quot;NCIS&quot;) This boxplot clearly shows how the relationship between NCIS viewership and feelings about the police is stronger for liberals than it was for conservatives. This is a really nice example of an interaction, and it suggests that NCIS viewerhip may have had different meanings for liberals, moderates, and conservatives. 5.6 Review of this chapter’s commands Command Purpose Library filter(VARIABLE==VALUE) %&gt;% A way to filter out cases that you are not interested in. If the value you are interested in is coded as text (as is the case with our nominal and ordinal variables), be sure to put it in quotation marks. dplyr (tidyverse) This graph is generated with the states2010 dataframe that I used in previous versions of the workbook. The states2025 dataframe that we are using for this workbook also as a “percent evangelical” variable, but is data from 2020, while the data in this graph is from 2010. This graph also uses a different measure of percent evangelical from the 2010 data.↩︎ If you did not run the code to generate that variable at the time, you should generate the variable ideology3 using the code in section 4.2 before continuing with thise section.↩︎ "],["inferences.html", "Chapter 6 Making inferences from sample means 6.1 Getting started with this chapter 6.2 Constructing a confidence interval around a mean 6.3 Constructing a confidence interval around a proportion 6.4 Constructing confidence intervals around several proportions at once 6.5 T-test of independent means 6.6 Caution about “statistical significance” 6.7 T-test of independent proportions 6.8 Review of this chapter’s commands", " Chapter 6 Making inferences from sample means 6.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now, open a new script file and save it in your scripts folder as “chapter 6 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 6, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap library(flextable)#this makes tables that we can easily export into a word processor Now select all the text on this page, run it, and save it. 6.2 Constructing a confidence interval around a mean The fundamental task of much statistical analysis is to make mathematically informed inferences about populations from samples. We can see this most clearly when looking at data like the ANES, which is a sample of Americans from which we want to make inferences about all Americans. But even when looking at our states or world dataset, in which our dataframe includes data about all states in the US and (almost) all countries in the world, it is still helpful to think about making inferences about populations. In this case, we might think of the population as states or countries that we cannot observe because they are hypothetical, they have not yet come into existence (because they exist in the future) or they existed in the past. In other words, we are studying our group of states or countries to make inferences about hypothetical states or countries, states or countries in the future, states or countries in the past, or state or countries that we cannot observe for some other reason. One inference that we often want to make about a population is the mean of some variable. Given the mean that we observe in sample, what is the range of outcomes for the population mean that we might reasonably expect to observe? This is the formula for the confidence interval around a mean: \\[CI = \\bar{x} ± t_{critical}{\\frac{s}{\\sqrt{n}}}\\] The critical value for t will vary with the sample size. “S” is the sample standard deviation, and “n” is the sample size. You might recall from class that \\[{\\frac{s}{\\sqrt{n}}}\\] is the standard error of the sample mean. Let’s imagine that we want to estimate the mean amount that Americans owe in student loans. First we can ask R to calculate the mean of our sample, using the mean() command that we learned in Chapter 3. If we were to enter mean(anes2024$studentLoans), we would get NA” as our output, because there are some cases with missing data. To address this, we must ask R to remove the NAs for the purposes of this operation with “na.rm=TRUE”, like this: mean(anes2024$studentLoans, na.rm=TRUE) ## [1] 11262.54 This tells us that the mean of our sample is 11262.54. Or, in other words, the average person in owes $11,262.54 in student loans. Based on this, what can we say about the population mean? To answer this question, we can ask R to construct a confidence interval around this mean, with this command: t.test(studentLoans~1, data=anes2024) ## ## One Sample t-test ## ## data: studentLoans ## t = 15.477, df = 3945, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 9835.878 12689.193 ## sample estimates: ## mean of x ## 11262.54 Before we look at the output, let’s look at the command. We are asking R to construct a t-test of the mean of only one variable (which explains the ~1), and we are telling R to find that variable in the anes2024 dataframe. Looking at the output, after the title, “One Sample t-test” the next three lines are not particularly useful. The fourth and fifth lines, however, are quite helpful. They tell us that the 95% confidence interval that we can put around our sample mean goes from 9835.878 to 12689.193. In other words, there is a 95% chance that the true population mean student loan debt is between $9,835.88 and $12,689.19. If we wanted to be even more confident than 95%–say, for example, we wanted to be 99% confident that our interval contains the population mean—-we could include a line in our command setting the confidence interval like this: t.test(studentLoans~1, data=anes2024, conf.level=.99) ## ## One Sample t-test ## ## data: studentLoans ## t = 15.477, df = 3945, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 9387.257 13137.814 ## sample estimates: ## mean of x ## 11262.54 When you run this command, notice that if we want to be more confident that our interval contains the true population mean, the margin between our lower and upper estimate gets larger. Similarly, if we set the confidence interval to 90% (using conf.level=.9), or 80% (using conf.level=.8), the interval gets smaller. 6.3 Constructing a confidence interval around a proportion For nominal and ordinal variables, it does not make sense to calculate a mean, but we still often want to make inferences about a population from a sample. In fact, these are one of the most commonly reported kinds of inferences that we who follow politics hear: 54.2% of people support this policy, 32.1% of people support that politician, etc. To ask R to calculate a confidence interval around a proportion, we first to answer two questions: how many cases got the value that we are interested in, and how many cases are there total? So, for example, let’s say that we want to use our anes2024 dataset to estimate the proportion of Americans that are married. We can first run the table command, like this: table(anes2024$marital) ## ## divorced married never married separated widowed ## 771 2745 1242 91 414 When we run this command, we see that there are 2,745 married people in our sample. Unfortunately, the table command itself does not tell us the total number of cases in our sample, but to get that, we can ask R to sum all the values that it shows in the table command like this: sum(table(anes2024$marital)) ## [1] 5263 When we run this command, we get the total number of cases for which R has data on the marital question: 5,263. This is the sum of all of the numbers that we saw above, when we ran the table() command. Since the table() command automatically excludes the NAs, this sum also excludes NAs. So, now we know that there were 2,745 married people in our dataset out of the 5,263 people for whom we have data. We could do have R do some simple math and calculate a proportion by entering 2745/5263 into the Console. The output tells us that 0.5215656—about 52.2%—of individuals in our sample are married. Based on this finding about our sample, what can we say about our population? To help us generalize, we can use those two numbers (2,745 married people out of 5,263 total people) along with the prop.test() command to help generate a confidence interval around that proportion. When using the prop.test() command, in the parenthesis we first put the number of cases with the value that we are interested in, then a comma, then the total number of cases. So, to estimate the proportion of Americans that are married, we would use prop.test() like this: prop.test(2745,5263) ## ## 1-sample proportions test with continuity correction ## ## data: 2745 out of 5263 ## X-squared = 9.7047, df = 1, p-value = 0.001838 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.5079640 0.5351356 ## sample estimates: ## p ## 0.5215656 Just like the output from the t-test, the first few lines are testing a hypothesis that we are likely not interested in. In this case, the default is to test the hypothesis that the population proportion is actually .5. If you want to set that hypothesis to a different value, you can add p=the value you are interested in after a comma. So, for example, if you had a hypothesis that 70% of Americans are married, you could test that hypothesis with this command: prop.test(2745,5263, p=.7) But the most important part of this output begins with the fourth line. This shows that the 95% confidence interval around the estimate that the proportion of Americans that are married goes from 0.5079640 to 0.5351356. Or, put another way, based on our sample, we can infer that 95% of the time, a sample like ours would be drawn from a population where between 50.8% and 53.5% of the population was married. If we want to change the values of our confidence interval, to be 99% confident (or 90% confident, or 80% confident), we can add a comma and then conf.level= and then the level that we are interested in, expressed as a proportion. So, for example, if we wanted to be 99% confident that our interval contains the true population proportion, we could alter the command like this: prop.test(2745,5263, conf.level=.99) Note that the interval gets wider as the confidence level goes up. 6.4 Constructing confidence intervals around several proportions at once In the previous section, we learned how to construct a confidence interval around a single proportion at a time. However, what if we want to calculate confidence intervals around all values of a nominal or ordinal variable at the same time? To do that, you can use the following code. Just replace anes2024 with the dataframe that you are interested in analyzing, and replace the two instances of marital (in the second and third lines of the code) with the name of the variable that you are interested in. You can keep everything else the same: anes2024 %&gt;% filter(!is.na(marital)) %&gt;% group_by(marital) %&gt;% summarise(group.n=n()) %&gt;% mutate(total.n=sum(group.n)) %&gt;% mutate(proportion=group.n/total.n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[1]) %&gt;% mutate(upper_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[2]) ## # A tibble: 5 × 6 ## # Rowwise: ## marital group.n total.n proportion lower_ci upper_ci ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 divorced 771 5263 0.146 0.137 0.156 ## 2 married 2745 5263 0.522 0.508 0.535 ## 3 never married 1242 5263 0.236 0.225 0.248 ## 4 separated 91 5263 0.0173 0.0140 0.0213 ## 5 widowed 414 5263 0.0787 0.0716 0.0863 Reading down this output, we can see that a proportion of .146 (or 14.6%) of our sample is divorced. The next two columns tell us that there is a 95% chance that the true population proportion of divorced people is between 13.7% and 15.6%. We can see the married values on the second line, with a familiar confidence interval, and each additional line clearly displays the confidence interval around the proportion that we are interested in. And as we continue to read down, we can see the remaining proportions with their associated confidence intervals. To save this table of confidence intervals in a format that we can easily pull into a word processor, we can use the flextable technique that we learned in Section 4.7.4, which will create a file in our project folder called “ci_table.docx”: ci_table&lt;-anes2024 %&gt;% filter(!is.na(marital)) %&gt;% group_by(marital) %&gt;% summarise(group.n=n()) %&gt;% mutate(total.n=sum(group.n)) %&gt;% mutate(proportion=group.n/total.n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[1]) %&gt;% mutate(upper_ci = prop.test(group.n, total.n, conf.level=0.95)$conf.int[2]) flextable(ci_table) %&gt;% save_as_docx(path=&quot;ci_table.docx&quot;) 6.5 T-test of independent means We often find ourselves in a situation where we want to compare the means of two different groups in our sample, and make inferences about the population. So, for example, we might want to compare men and women, Democrats and Republicans, voters and non-voters, etc. For the purposes of this example, we will be comparing democracies and non-democracies in our world dataframe. Before we can start with this comparison, though, we have to do a bit of housekeeping. The democracy variable that we have in the world dataframe is an interval variable based on the Polity Score, with values ranging from -10, which is a complete autocracy, to 10, a complete democracy. This score is often simplified, so that -10 to -6 is an autocracy, -5 to 5 is an “anocracy” (a mixed authority regime, neither complete autocracy nor complete democracy), and 6 to 10 means democracy. To simplify the variable in this way, we can use the case_when command from section 4.6.2: world2025&lt;-world2025 %&gt;% mutate(polity2=case_when(polity %in% -10:-6 ~ &quot;autocracy&quot;, polity %in% -5:5 ~ &quot;anocracy&quot;, polity %in% 6:10 ~ &quot;democracy&quot;)) world2025$polity2&lt;-ordered(world2025$polity2, levels=c(&quot;autocracy&quot;,&quot;anocracy&quot;,&quot;democracy&quot;)) The world2025 dataframe also has a variable that measures educational inequality, called edInequality, which can range (at least in theory) from 0 (perfect equality) to 100 (perfect inequality). In our data, however, we can use this script (from section 3.4) to see the countries in the world with the fewest educational inequalities world2025 %&gt;% dplyr::select(country,edInequality) %&gt;% arrange(edInequality) Can you run that same script with arrange(desc) to find the countries in the world with the most educational inequality? Now, using the commands that we learned in chapter 4, we can first examine what the mean differences are between democracies, anocracies, and autocracies on their level of educational inequality, with this command: world2025 %&gt;% filter(!is.na(polity2)) %&gt;% filter(!is.na(edInequality)) %&gt;% group_by(polity2) %&gt;% summarise(mean=mean(edInequality), sd=sd(edInequality), n=n()) ## # A tibble: 3 × 4 ## polity2 mean sd n ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 autocracy 25.9 11.5 11 ## 2 anocracy 40.2 19.4 34 ## 3 democracy 24.7 14.7 79 This output suggests that anocracies have the most educational inequality, followed by autocracies, and then democracies have the least educational inequality (although democracies and autocracies are close). Are these differences due to random sampling error, or do they reflect a population-level difference between democracies, anocracies, and autocracies? In other words, are these differences statistically significant? Before conducting a significance test, it is important to specify our null and research hypotheses. In this case, our null hypothesis is that there is no relationship between regime type and level of educational inequality, and our research hypothesis is that democracies have less educational inequality than autocracies. To test our hypotheses, we can use the same t.test() command from above, but we have to give it more information first. Here is the command that we can use: t.test(edInequality ~ polity2, data=world2025 %&gt;% filter(polity2==&quot;democracy&quot;|polity2==&quot;autocracy&quot;)) ## ## Welch Two Sample t-test ## ## data: edInequality by polity2 ## t = 0.31549, df = 14.931, p-value = 0.7568 ## alternative hypothesis: true difference in means between group autocracy and group democracy is not equal to 0 ## 95 percent confidence interval: ## -6.995828 9.425515 ## sample estimates: ## mean in group autocracy mean in group democracy ## 25.89282 24.67797 Notice how the first thing in the parentheses after t.test is the dependent variable (i.e. the variable that we are calculating the mean of), followed by a ~ and then our independent variable (polity2). One thing that is a little tricky is that R does not like to do a t-test when the independent variable has more than two values. To address this, I have used the dplyr filter command to tell R to only look at cases of polity2 coded “democracy” or “autocracy” (i.e. I am filtering out the NAs and partial democracies). The “|” between the two conditions (it is the character above the “\" on your keyboard) means”or.” Now take a look at the output. There are a few things to note here. First of all, we could see that the reported p-value is 0.7568. That tells us that, if we accept the research hypothesis that democracies have more equality than non-democracies, there is a probability of .7568, or a 75.68% chance, that we are making Type I error (rejecting a true null hypothesis). Since the critical value for p is generally set to .05 (meaning we are not willing to take more than a 5% risk of Type I error), we would fail to reject the null hypothesis. We can also see that we would fail to reject the null hypothesis by looking at the confidence interval that R reports. Since this is a t-test of two samples, the confidence interval is actually of the difference between the two means. In other words, that confidence interval is telling us that there is a 95% chance that the true population difference in educational inequality between democracies and autocracies is between -6.995 and 9.4255. Since that interval includes 0, we cannot rule out the possibility that there is no difference at all between our two means in the population. We can also graph these results. As usual, we will put our independent variable (regime type) on the X-axis, and our dependent variable, educational inequality, on the Y axis. For today’s new feature, we will also add 95% confidence intervals to our graph. This gives us the ability to see whether the differences between all of the bars on our graph are statistically significant. And, while an independent samples t-test can only be between the means of two groups at a time, this graph will let you look at more than two groups at a time. Below is the code: plotting.data&lt;-world2025 %&gt;% filter(!is.na(polity2)) %&gt;% filter(!is.na(edInequality)) %&gt;% group_by(polity2) %&gt;% summarise( n=n(), mean=mean(edInequality), sd=sd(edInequality)) %&gt;% mutate(se=sd/sqrt(n)) %&gt;% mutate(ci=se * qt((1-0.05)/2 + .5, n-1)) ggplot(plotting.data) + geom_bar(aes(x=polity2, y=mean), stat=&quot;identity&quot;, fill=&quot;purple&quot;, alpha=0.5) + geom_errorbar( aes(x=polity2, ymin=mean-ci, ymax=mean+ci), width=0.4, colour=&quot;black&quot;, alpha=0.9, linewidth=1.5) + ggtitle(&quot;Educational inequality by regime type&quot;, subtitle=&quot;Error bars represent 95% confidence intervals&quot;)+ xlab(NULL)+ ylab(&quot;Educational inequality, 2010 (higher\\nnumbers mean more inequality)&quot;) Here we can see that, although the differences between some of bars are quite large, some of the error barsoverlap, which means that we can’t rule out the possibility that differences between means are due to random sampling error. Whenever the error bars between two totals overlap, the differences are not statistically significant. So for example, the difference between anocracy and democracy is statistically significant, but the difference between anocracy and autocracy and the difference between autocracy and democracy are not statistically significant. 6.6 Caution about “statistical significance” The idea of “statistical significance” is often misinterpreted. Saying that a relationship between variables has statistical significance only means that we would be unlikely to observe a relationship between variables such as the one we are observing in a sample drawn from a population where there is no relationship between those variables. In other words “statistically significant” means “likely not due to random sampling error.” Saying that a relationship is between two variables is “statistically significant” does not mean: That the relationship between those variables is strong. That the relationship between those variables is substantively significant (that it matters in the real world) That the relationship is causal (that change in one of the variables definitely causes change in the other) Conversely, with the statistical tests that we introduce in this workbook, if you observe a relationship that is not statistically significant, you cannot conclude (based on that alone) that there is no relationship between your two variables. Let’s take another look at the mean comparison table of the relationship between regime type and educational inequality: ## # A tibble: 3 × 4 ## polity2 mean sd n ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 autocracy 25.9 11.5 11 ## 2 anocracy 40.2 19.4 34 ## 3 democracy 24.7 14.7 79 If you look at the leftmost column, titled “n,” you will notice that there are only 11 autocracies and 34 anocracies in our world2025 dataframe. Those relatively small number of cases could be the reason that the difference in educational inequality between autocracies and anocracies are not statistically significant. Based on what we observe in this data, we simply don’t have enough information to determine whether or not there a meaningful difference between the educational inequality of anocracies and autocracies. 6.7 T-test of independent proportions There are times when we want to compare two or more proportions to examine whether or not the differences are due to random sampling error or some genuine correlation at the population level. Unfortunately, R does not make it easy to run a t-test comparing two proportions. However, we can use dplyr in a way similar to what we did in section 6.3 to generate some helpful output. Before we begin, we have to know three things: What is the nominal or ordinal independent variable that we are interested in examining? What is the nominal or ordinal dependent variable that we are interested in examining? For our dependent variable, what is the value that we want to focus on? For the following example, I am interested in the relationship between the belief that America is better than most countries in the world (natPride) and support for military aid to Ukraine (armUkraine). My hypothesis is that those that believe that America is a great country will also believe that America should want to spread that greatness by supporting other democracies that are fighting against foreign invasions. To generate a table that displays that information, I will use the following code. Please note that this is very similar to the code that we used in section 6.4. When using this code to generate your own table, you can replace anes2024 with the dataframe that you want to look at, “armUkraine” with your dependent variable, “natPride” with your independent variable, and armUkraine==\"favor strongly\" with DV==\"the value that you are interested in\". Here is the code: anes2024 %&gt;% filter(!is.na(natPride)) %&gt;% filter(!is.na(armUkraine)) %&gt;% group_by(natPride) %&gt;% summarise( n=n(), numerat=sum(armUkraine==&quot;favor strongly&quot;)) %&gt;% mutate(proportion=numerat/n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[1]) %&gt;% rowwise() %&gt;% mutate(upper_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[2]) ## # A tibble: 5 × 6 ## # Rowwise: ## natPride n numerat proportion lower_ci upper_ci ## &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a lot better 1277 387 0.303 0.278 0.329 ## 2 somewhat better 1160 341 0.294 0.268 0.321 ## 3 the same 1119 299 0.267 0.242 0.294 ## 4 somewhat worse 701 166 0.237 0.206 0.270 ## 5 a lot worse 406 88 0.217 0.178 0.261 This output has a bunch of great information. The first row, “a lot better”, for example, tells us that a total of 1277 people in the ANES study in 2024 thought that the the US is “a lot better” than most countries. And of those 1277, the second number tells us that 387 of them strongly favor military aide to Ukraine. The third number tells us that 30.3% of people that think America is a lot better than other countries strongly support military aid to Ukraine. The lower_ci and upper_ci columns tell us that there is a 95% chance that the true population proportion of those that think that American is much better than other countries strongly favor military aid to Ukraine is between .278 (27.8%) and .329 (32.9%). As we read down the rows, we see that some of the confidence intervals overlap with one another. When these confidence intervals overlap, we can say that the differences are not statistically significant (or that we cannot rule out the chance that the differences that we observed are due to random sampling error). So, for example, the confidence interval between the “a lot better” and “somewhat better” proportions that strongly favor military aid to Ukraine. This is easier to see on a graph. We can generate a graph with code similar to what we used in section 6.5: plotting.data&lt;-anes2024 %&gt;% filter(!is.na(natPride)) %&gt;% filter(!is.na(armUkraine)) %&gt;% group_by(natPride) %&gt;% summarise( n=n(), numerat=sum(armUkraine==&quot;favor strongly&quot;)) %&gt;% mutate(proportion=numerat/n) %&gt;% rowwise() %&gt;% mutate(lower_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[1]) %&gt;% rowwise() %&gt;% mutate(upper_ci = prop.test(numerat, n, conf.level=0.95)$conf.int[2]) ggplot(plotting.data) + geom_bar(aes(x=natPride, y=proportion), stat=&quot;identity&quot;, fill=&quot;purple&quot;, alpha=0.5) + geom_errorbar(aes(x=natPride, ymin=lower_ci, ymax=upper_ci), width=0.4, colour=&quot;black&quot;, alpha=0.9, linewidth=1.5) + ggtitle(&quot;Views on America&#39;s greatness and aid to Ukraine, 2024&quot;, subtitle=&quot;Error bars represent 95% confidence intervals&quot;)+ xlab(&quot;Is America better or worse than most countries?&quot;)+ ylab(&quot;Proportion that strongly favor military aid to Ukraine&quot;) This graph shows us that, while some of those differences are not statistically significant, there is clearly a statistically significant differences between views on arming Ukraine from those that think America is a lot better than most other countries compared with those who think that America is a lot worse. You might also notice that the confidence interval on the “a lot worse bar” is bigger than the confidence interval on the “a lot better” bar. That is because, if you look at the table that we generated at the beginning of this section, there are 1,277 people in our sample that think that America is a lot better than other countries, but only 406 people that think that America is a lot worse. Because we are basing our estimated proportion on a smaller sample when generating the confidence interval around the “a lot worse” bar, the confidence interval is larger (in other words, smaller samples make us less confident of our estimates, and larger samples make us more confident of our estimates). 6.8 Review of this chapter’s commands Command Purpose Library t.test() To calculate a t-test. With a one sample test, you put the variable that we are interested in ~1, and then a comma, and then data=“your dataframe.” For an an independent samples t-test, you put DV~IV in the parenthesis, and then a comma, and then data= “your dataframe”. Base R t.test(…, mu=?) A t-test of a single sample is testing the hypothesis that the population mean of the variable you are testing is 0. If you want to test another hypothesis (for example, that the average TCU student has a 17 purple shirts), you can set mu to be the number that you are hypothesizing. Base R t.test(…, conf.level=?) The default confidence interval for a t-test is .95. If you want to set it to something else, you can do it with the conf.level command. Base R sum(table()) This command adds up all of the values in a frequency table. It is useful when calculating proportions. Base R prop.test() This command calculates a confidence interval around a proportion. You must give R two numbers. The number of your cases with the value that you are interested in, followed by a comma, and then the total number of cases that you have data on for that variable (which you can calculate with sum(table()). For example, if you had a sample with 500 people and 200 said that bagels were their favorite baked good, you would write prop,test(200,500) to get a confidence interval around the proportion of people that like bagels best. Base R prop.test(…, p=?) The default hypothesis for an estimate of a population proportion is that the proportion is .5 (or 50%). If you want to change that, you can type p=and then the proportion that you are hypothesizing about. Base R prop.test(…, conf.level=?) The default confidence interval for a test of proportions is .95. If you want to set it to something else, you can do it with the conf.level command. Base R "],["chi-squared-etc.html", "Chapter 7 From chi-squared to somers’ d 7.1 Getting started with this chapter 7.2 Calculating chi-squared 7.3 Lambda and the PREs 7.4 Cramer’s V 7.5 Somers’ D 7.6 Review of this chapter’s commands", " Chapter 7 From chi-squared to somers’ d 7.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now, open a new script file and save it in your scripts folder as “chapter 7 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 7, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(Hmisc) #the cut2 command helps us simplify interval variables library(scales) #this is useful when the labels on our axes overlap library(tigerstats) #colPerc can also be useful with crosstabs library(DescTools) #this has tools to calculate Cramer&#39;s V, Lambda and Somers&#39; D Now select all the text on this page, run it, and save it. 7.2 Calculating chi-squared In chapter 6, we learned two tools to think about relationships between variables: a t-test of independent means and a t-test of independent proportions. These tests let us compare two groups and see whether their means or proportions differ in a statistically significant way. However, what if we want to compare more than two groups at the same time? And, in the case of proportions, what if we want to look at several different proportions at once? For example, the proportion of people that support, oppose and neither support nor oppose military aid to Ukraine? In other words, to return to a concept from chapter 4, what if we wanted to look at a whole cross tabulation table (a crosstab) and decide whether the values that we observe are likely due to random sampling error, or due to a correlation at the population level? The most basic way to do that is by calculating chi-squared. That statistic tells us whether the values of variables that we observe in a crosstab are so different from what we would expect to observe if there was no association between the variables at the population level that we can conclude that there likely is such a relationship. For this example, we will think about the relationship between the dominant religion of a country and percent of legislators that are women. We will think through that question with help from our world2025 dataframe, using the “religion” variable and the “femLeg” variable. Before we generate our crosstab, we first have to deal with an issue: the femLeg variable is interval. If we generate a crosstab with that variable as the DV, that crosstab will have one row for each value of that variable, which would give us 160 different rows.6 We don’t want that! Instead, let’s use the technique that we learned in section 4.6.1 and simplify that variable into a new variable called “femLeg3.” First, we will use the cut2 command to generate a variable with three groups: world2025$femLeg3&lt;-cut2(world2025$femLeg, g=3) Next, we can look at the levels of our new ordinal variable, like this: levels(world2025$femLeg3) ## [1] &quot;[ 0.0,20.4)&quot; &quot;[20.4,32.2)&quot; &quot;[32.2,61.2]&quot; Finally, we can rename those levels, like this: levels(world2025$femLeg3)&lt;-c(&quot;fewer&quot;,&quot;typical&quot;,&quot;many&quot;) Now, we can generate a crosstab, with this command (remember when generating a crosstab we put our dependent variable first): addmargins(xtabs(~femLeg3+religion, data=world2025)) ## religion ## femLeg3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath Sum ## fewer 9 1 0 26 4 5 7 6 58 ## typical 4 0 1 14 3 4 5 22 53 ## many 1 1 0 6 5 2 14 23 52 ## Sum 14 2 1 46 12 11 26 51 163 This crosstab gives us raw numbers, which are interesting to look at. It also foreshadows a potential limitation to our conclusions. Because we only have one predominantly Jewish state (Israel) and two predominantly Hindu states (Nepal and Mauritius) in our sample, we will be attempting to generalize about those two kinds of states from a tiny sample. We discuss that issue more below. To better interpret this crosstab, we need to look at column percents, which we can do with this command: colPerc(xtabs(~femLeg3+religion, data=world2025)) ## religion ## femLeg3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath ## fewer 64.29 50 0 56.52 33.33 45.45 26.92 11.76 ## typical 28.57 0 100 30.43 25.00 36.36 19.23 43.14 ## many 7.14 50 0 13.04 41.67 18.18 53.85 45.10 ## Total 100.00 100 100 100.00 100.00 100.00 100.00 100.00 Take a look at that table. You can see that there are some pretty noticeable differences in female representation across states with different major religions. For example, the religion with the largest percentage of states with many women in their legislatures is Protestantism, with 53.85%, followed by Roman Catholicism, with 45.1%. To help us visualize these differences, we can generate a stacked bar graph using the technique that we learned in section 5.2: plotting.data&lt;-world2025 %&gt;% filter(!is.na(religion)&amp;!is.na(femLeg3)) %&gt;% group_by(religion, femLeg3) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ggplot(plotting.data, aes(x = religion, y = freq, fill=femLeg3)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ scale_fill_viridis_d(name=&quot;Women in legislature&quot;)+ ggtitle(&quot;Countries&#39; major religion and women in legislature&quot;)+ xlab(NULL)+ ylab(NULL) Clearly, there are differences in the distribution of women in legislatures in countries with different major religions. But, are these differences statistically significant? To test that, we can ask R to calculate the chi-squared value that is associated with this relationship. We can use the chisq.test() command, and in the parenthesis we can put the command that generates crosstab that we want to run this test on. Note: be careful to use the chisq.test() command on the command for a crosstab with actual values and not percents, or the test will not be accurate. Here is how we would generate a chi-squared value for the relationship between dominant religion of a country and the percent of women in the legislature: chisq.test(xtabs(~femLeg3+religion, data=world2025)) ## Warning in chisq.test(xtabs(~femLeg3 + religion, data = world2025)): ## Chi-squared approximation may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: xtabs(~femLeg3 + religion, data = world2025) ## X-squared = 40.057, df = 14, p-value = 0.00025 Look at what R tells us in this output (the text just above this passage, beginning with ## Pearson's Chi-squared test). First, R reports that it will be giving us the results of a Pearson’s Chi-squared test. Second, it repeats the command that we entered. Third, it reports the value for chi-squared (which it writes as X-squared), tells us our degrees of freedom (calculated by (number of rows-1)*(number of columns-1)) and reports our p-value. Lower p values mean that the relationship that we are observing is more likely to be statistically significant (i.e. not due to random sampling error). In this case, our p-value is very low: .00025, which means that, according to the chi-squared test, this relationship is statistically significant (because .00025 is lower than .05, our conventional benchmark). However, look at warning at the top: R is warning us that this estimate may be wrong. Why? This has to do with the expected values of the cells in our crosstab. Let’s take another look at the crosstab that we generated earlier: ## religion ## femLeg3 Eastern Hindu Jewish Muslim Orthodox Other Protestant Roman Cath Sum ## fewer 9 1 0 26 4 5 7 6 58 ## typical 4 0 1 14 3 4 5 22 53 ## many 1 1 0 6 5 2 14 23 52 ## Sum 14 2 1 46 12 11 26 51 163 Look at top cell in the “Hindu” column: countries where Hinduism is dominant that have fewer women in their legislatures. There is only one country in that category. If there were no relationship at all between a country’s religion, we would expect that cell to be the row total*the column total/the total number of cases in the table. So, (58*2)/163, which is .7127. As a general rule, if some cells in a crosstab have an expected value of less than 5, value of the chi-squared test will be artificially high, and we will be more likely to make Type I error (rejecting a true null hypothesis). In this case, we can actually look at the expected values of all of our cells with this command: chisq.test(xtabs(~femLeg3+religion, data=world2025))$expected ## Warning in chisq.test(xtabs(~femLeg3 + religion, data = world2025)): ## Chi-squared approximation may be incorrect ## religion ## femLeg3 Eastern Hindu Jewish Muslim Orthodox Other Protestant ## fewer 4.981595 0.7116564 0.3558282 16.36810 4.269939 3.914110 9.251534 ## typical 4.552147 0.6503067 0.3251534 14.95706 3.901840 3.576687 8.453988 ## many 4.466258 0.6380368 0.3190184 14.67485 3.828221 3.509202 8.294479 ## religion ## femLeg3 Roman Cath ## fewer 18.14724 ## typical 16.58282 ## many 16.26994 Looking over this output, we can see that we have many cells with expected values below 5, and thus our estimated chi-squared is less helpful. 7.3 Lambda and the PREs One way to address the deficiency in chi-squared—that chi-squared is less accurate when the expected values of some of our cells is quite small—is with Lambda. Lambda is the first of several Proportional Reduction in Error (PRE) statistics that we will be learning about in class. PRE statistics produce a value between 0 and 1 or between 0 and |1| (in the case of PRE statistics that let us look at the direction of relationships). These statistics are useful in interpreting both the significance and strength of relationships. A PRE statistic with a value of 0 means that knowing the independent value has no impact on our ability to correctly predict the value of the dependent variable, while a PRE statistic with a value of 1 (or -1, in the case of a negative relationship) means that knowing our independent variable will allow us to perfectly predict the value of our dependent variable (these kinds of perfect relationships do not exist in actual political science). A PRE statistic of .35, for example, tells us that knowing our independent variable improves our ability to guess the dependent variable by a probability of .35 (or 35%) compared with how good our guess would be if we did not know the independent variable. Here is a general rule of thumb when interpreting PRE-statistics on social science data (this is based on Pollock and Edwards 2019, 225): Range of values Strength of relationship 0 and .1 Weak relationship 0 and -.1 Weak relationship .1 and .2 Moderate relationship -.1 and -.2 Moderate relationship .2 and .3 Moderately strong relationship -.2 and -.3 Moderately strong relationship Above .3 or below -.3 Strong relationship We can ask R to calculate Lambda in a way similar to the way that we had R calculate chi-square: Lambda(xtabs(~femLeg3+religion, data=world2025)) ## [1] 0.2119816 In other words, R is telling us that knowing the religion of a country improves the probability that we will correctly guess the level of economic equality in that country by about .21 (or 21%). That is a moderately strong relationship. However, that statistic alone does not tell us about statistical significance (or the chance that the relationship that we observe is due to random sampling error). To fix that, we can add “conf.level=.95” to our command from above (we could also set the confidence level to .99, or .9, or something else, but .95 is the standard in political science). This is our new command: Lambda(xtabs(~femLeg3+religion, data=world2025), conf.level=.95) ## lambda lwr.ci upr.ci ## 0.2119816 0.1281852 0.2957780 While this output does not directly report a significance level, unlike the t-tests and chi-squared tests that we ran earlier, the confidence interval that it reports is extremely helpful. This tells is that there is a 95% chance that the true value of Lambda is somewhere between .128 (which would be a moderate relationship) and .296 (which would be a moderately strong relationship). In other words, we are pretty confident that this relationship is moderate or moderately strong. If this reported interval had included 0, we would have concluded that the relationship was not statistically significant. 7.4 Cramer’s V Lambda sometimes underestimates the strength of a relationship when there is little variation between the overall mode of a sample and the modes of each of our groups. In that case, we can use Cramer’s V instead. Or, given how easy it is to ask R to calculate these statistics, we can actually ask R to calculate chi-squared, lambda, and Cramer’s V, and if they provide similar information, that is a good sign that there might be something to our hypothesis. Unlike Lambda, Cramer’s V is not a PRE statistic, which means that we can’t interpret it in terms of improved probability of guessing our dependent variable with knowledge of our independent variable. However, like Lambda, it reports a statistic between 0 and 1, and higher values mean that we have a stronger relationship. We can also use the same ” conf.level=.95” qualifier to get a confidence interval around our estimate, like this: CramerV(xtabs(~femLeg3+religion, data=world2025), conf.level=.95) ## Cramer V lwr.ci upr.ci ## 0.3505328 0.1606482 0.4060984 Seeing this confidence interval with a lower bound nowhere near zero gives us additional evidence that there is likely a relationship between religion and economic equality at the population level. 7.5 Somers’ D All of the measures of association that we have looked at so far have been useful for thinking about the relationship between two ordinal or nominal variables. But if we have two ordinal variables, we can also start to think about the direction of our relationship. In other words, we can go beyond asking whether knowing our independent variable helps us predict our dependent variable and ask whether an increase in our independent variable is associated with an increase (or decrease) in our dependent variable. This kind of question does not make sense when discussing nominal variables, such as marital status. In other words, it does not make sense to ask whether marital status “increasing” leads to income increasing, because there is no order in which the values for the marital status variable (divorced, married, never married, separated, and widowed) must be listed. For this example, let’s return to the relationship between ideas about whether America is better than other countries and support for aid to Ukraine that we looked at in section 6.7. My theory is that people that believe that America is better than other countries will tend to support military aid to Ukraine, because they will conceive of that kind of aid as a way to spread American values abroad (a democracy fighting off invasion from an autocracy). Before we can do our statistical tests of this theory, we can take a look the data in graphics and crosstabs to see whether the data seems to support our theory. We can use the following two commands to generate a crosstab and a table of the columns percentages (notice that I put the dependent variable, armUkraine, first in these commands): xtabs(~armUkraine+natPride, data=anes2024) colPerc(xtabs(~armUkraine+natPride, data=anes2024)) One tip before we move on: R is almost always willing to create an object so that you don’t have to keep typing the same things over and over. So, in this case, you can make your crosstab an object called xt like this: xt&lt;-xtabs(~armUkraine+natPride, data=anes2024) Then, to see what that table looks like, you can run this line of code: xt ## natPride ## armUkraine a lot better somewhat better the same somewhat worse ## favor strongly 387 341 299 166 ## favor 239 202 200 129 ## favor weakly 51 72 61 36 ## neither 276 297 327 187 ## oppose weakly 33 30 32 33 ## oppose 104 101 93 52 ## oppose strongly 187 117 107 98 ## natPride ## armUkraine a lot worse ## favor strongly 88 ## favor 67 ## favor weakly 16 ## neither 112 ## oppose weakly 9 ## oppose 34 ## oppose strongly 80 After that, to generate the table with column percents, you can go like this: colPerc(xt) ## natPride ## armUkraine a lot better somewhat better the same somewhat worse ## favor strongly 30.31 29.40 26.72 23.68 ## favor 18.72 17.41 17.87 18.40 ## favor weakly 3.99 6.21 5.45 5.14 ## neither 21.61 25.60 29.22 26.68 ## oppose weakly 2.58 2.59 2.86 4.71 ## oppose 8.14 8.71 8.31 7.42 ## oppose strongly 14.64 10.09 9.56 13.98 ## Total 100.00 100.00 100.00 100.00 ## natPride ## armUkraine a lot worse ## favor strongly 21.67 ## favor 16.50 ## favor weakly 3.94 ## neither 27.59 ## oppose weakly 2.22 ## oppose 8.37 ## oppose strongly 19.70 ## Total 100.00 Let’s look at the second table, with the percents. The first column in that table tells us that, among those who believe that the US is a lot better than other countries, 30.31% strongly favor military aid to Ukraine, and 14.64% oppose military aide to Ukraine strongly. However, the last column shows us thatm among those to think that the US is a lot worst than other countries, 21.67% strongly favor military aid to Ukraine, and 19.7% oppose. That seems like a piece of evidence for our theory! We have a number of choices for how to represent this relationship graphically. Since we generated a bar graph with error bars in of this relationship in section 6.7, for this section let’s generate stacked bar graph. plotting.data&lt;-anes2024 %&gt;% filter(!is.na(armUkraine)&amp;!is.na(natPride)) %&gt;% group_by(natPride, armUkraine) %&gt;% summarise(n=n()) %&gt;% mutate(freq = n / sum(n)) ggplot(plotting.data, aes(x = natPride, y = freq, fill=armUkraine)) + geom_col(position = &quot;fill&quot;) + scale_y_continuous(labels=percent)+ scale_x_discrete(guide = guide_axis(n.dodge=2))+ #this line is in here because without it the x-axis labels overlap. You can remove it if that isn’t a problem scale_fill_viridis_d(name=&quot;Arms to Ukraine&quot;)+ #this color palate is made to be readable for colorblind people ggtitle(&quot;National Pride and Aid to Ukraine, 2024&quot;)+ xlab(&quot;Is the US better than other countries?&quot;)+ ylab(NULL) Notice that the bars are going in the way our theory predicted – people that think America is better than other countries also seem to be more supportive of errors to Ukraine. But are these differences larger than we would expect to see from random sampling error? First, if you didn’t generate the crosstab object before, you can do that now like this: xt&lt;-xtabs(~armUkraine+natPride, data=anes2024) Now you can simply execute these three commands: chisq.test(xt) ## ## Pearson&#39;s Chi-squared test ## ## data: xt ## X-squared = 82.713, df = 24, p-value = 2.236e-08 Lambda(xt, conf.level = .95) ## lambda lwr.ci upr.ci ## 0.02142435 0.00690347 0.03594523 CramerV(xt, conf.level = .95) ## Cramer V lwr.ci upr.ci ## 0.06659213 0.04053637 0.07202800 These three tests all show a statistically significant relationship. The statistics that we just calculated let us discuss the strength (in the case of Lambda) and significance (in the case of Cramer’s V and chi-squared) of the relationship that we are interested in, but how about the direction? To examine the direction of a relationship between two ordinal variables, we can use a Somers’ D test. Somers D is a Proportional Reduction in Error (PRE) statistic like Lambda. Unlike Lambda, however, which ranges from 0 to 1, Somers’ D can range from 0 to 1 OR -1. A perfect negative relationship, where knowledge of the independent variable lets us guess the value of the dependent variable with absolute certainty, would generate a Somers’ D score of -1, while a perfect positive relationship would generate a Somers’ D score of 1. A Somers’ D score of 0 means that knowing the independent variable gives us no insight at all in predicting the dependent variable. Before we run Somers’ D, we need to see whether our hypothesis is predicting a positive or a negative relationship. What does it mean to “increase” or “decrease” on those two variables? For this, we can use the levels() command, like this: levels(anes2024$natPride) ## [1] &quot;a lot better&quot; &quot;somewhat better&quot; &quot;the same&quot; &quot;somewhat worse&quot; ## [5] &quot;a lot worse&quot; levels(anes2024$armUkraine) ## [1] &quot;favor strongly&quot; &quot;favor&quot; &quot;favor weakly&quot; &quot;neither&quot; ## [5] &quot;oppose weakly&quot; &quot;oppose&quot; &quot;oppose strongly&quot; The levels() command only works with ordered factors (which is how R treats ordinal variables), and the output lists the scores of the variable from low to high. So, the lowest value on natPride mean that the respondent thinks that the US is “a lot better” than other countries, and the highest means that the respondent thinks that the US is “a lot worse” than other countries. Increasing values means less national pride. With “armUkraine”, the lowest value means that the respondent “favors strongly” arming Ukraine, and the highest value means that the respondent “opposed strongly” arming Ukraine. So increasing values mean less support for arming Ukraine. Since our hypothesis is that Americans with more national pride will be more likely to support arming Ukraine, we hypothesize a positive relationship between those two variables. We run Somers’ D just like how we ran Cramer’s V, Lambda, and chi-squared, above: SomersDelta(xt, conf.level=.95) ## somers lwr.ci upr.ci ## 0.04664520 0.02246288 0.07082751 This is telling us that there is a .95 probability that Somer’s D at the population level is between .022 and .071. In other words, there is a positive, weak, and statistically significant relationship between national pride and views on arming Ukraine. The relationship is in the direction that our hypothesis predicted, and it is statistically significant, but the weakness of the relationship means that we would only reduce our error in guessing views on Ukraine a small amount (about 4.66%) if we knew how much national pride someone had. One note: Somers’ D is an asymmetric test, which means that it gives different results when you treat one of your two variables as independent than when you treat that same variable as dependent. So, be careful to list your dependent variable first when setting up your test. 7.6 Review of this chapter’s commands Command Purpose Library chisq.test() Runs a chi-squared test on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command, but you can also use it on a table that you generate with other commands or on a pair of variables that you define with a $. Base R Lambda() Runs a Cramer’s V test (a test of the relationship between two nominal or ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools CramerV() Runs a Lambda test (a PRE test of the relationship between two nominal or ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools SomersDelta() Runs a Somers’ D test (a PRE test of the relationship between two ordinal variables) on a pair of variables. In this class we use it on crosstabs that we generate with the xtabs() command. DescTools References Pollock, Philip H, and Barry C Edwards. 2019. The Essentials of Political Analysis. Book. 6th ed. Sage. To see where I got the number 160 from, you can use this code, which asks R to count the number of unique values of the femLeg variable length(unique(world2025$femLeg))↩︎ We can make R do this math for us by entering this command in the Console: (58*2)/163↩︎ "],["pearsons-r.html", "Chapter 8 Pearson’s r and linear regression 8.1 Getting started with this chapter 8.2 Pearson’s R 8.3 The scatterplot 8.4 Bivariate linear regression 8.5 Residuals 8.6 Review of this chapter’s commands", " Chapter 8 Pearson’s r and linear regression 8.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now type these two commands into your Console one by one: install.packages(&quot;ggrepel&quot;) install.packages(&quot;modelr&quot;) Now, open a new script file and save it in your scripts folder as “chapter 8 practice.” Copy and paste this onto the page (updating the text so that it is about you): #################################### # Your name # 20093 Chapter 8, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(ggrepel) library(modelr) Finally, select all the text on this page, run it, and then save it. 8.2 Pearson’s R What happens when we are interested in the relationship between two interval variables? Up into now, our solution has been to convert those variables into ordinal variables and use chi-squared, Lambda, Cramer’s V, and/or Somers’ D. However, when we do that, we are throwing out information. For example, if we convert an interval measure of respondent’s age into an ordinal variable coded “young”, “middle aged”, and “old”, we are throwing out the distinctions between, for example, 30 and 31-year-olds (assuming that they both fall into our young category) and between 79 and 80-year-olds (assuming that they both fall into our old category). Pearson’s R is a test that lets us look at the relationship between two interval variables. It produces a statistic that ranges from -1 to 1, with negative numbers indicating a negative relationship and positive numbers indicating a positive relationship. Although, unlike Lambda and Somers’ D, it is not a Proportional Reduction in Error (PRE) statistic, values farther from 0 indicate stronger relationships, and values closer to 0 indicate weaker relationships (with 0 meaning no relationship). R makes it very easy to calculate Pearson’s R. You can simply use the command cor.test(). Below, I test the relationship between two variables that I have found in the states2025 dataframe: trumpMargin2024, which is the proportion of a state that voted for Trump in 2024 minus the proportion of that state that voted for Harris in 2024, and reaganMargin1980, which is the proportion of a state that voted for Reagan in 1980 minus the proportion that voted for Carter. My hypothesis is that states that voted for the Republican candidate in 1980 (Reagan) are more likely to have voted for the Republican candidate in 2024 (Trump). In other words, I am hypothesizing that Pearson’s R will be positive. Here is the command: cor.test(states2025$trumpMargin2024, states2025$reaganMargin1980) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 4.6916, df = 49, p-value = 2.208e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3321163 0.7216109 ## sample estimates: ## cor ## 0.5567436 Pearson’s R is a symmetrical test, which means that it produces the same value regardless of which variable you treat as independent or dependent. So, it does not matter which order you enter the variables into a cor.test() command. Look at R’s Pearson’s R output. R calls this Pearson’s product-moment correlation, which is just another way of saying Pearson’s R. Look at the bottom number of this output: 0.5567436. That number is positive, which is consistent with my hypothesis. It also seems relatively far for zero, although as I noted above, that is more difficult to interpret with Pearson’s R because it is not a Proportional Reduction in Error statistic. The p-value noted at the top ofthe output is 2.208e-05, which is R’s shorthand for \\(2.208*10^{-5}\\), which is another way of saying 0.00002208. That is the probability that we would get a sample like this from a population where there was not correlation between our two variables. Since it is (much) lower than 0.05, we can conclude that this relationship is statistically significant. We can also conclude that by looking at the 95% confidence interval around our estimated Pearson’s R value. R reports a confidence interval of 0.332 to 0.722, which does not include 0. Thus, we are 95% confident that there is a positive correlation between our two variables at the population level. 8.3 The scatterplot To visualize a relationship between two interval variables, we can generate a scatterplot, one of my favorite graphs to generate. Here is the code that we can use to visualize the relationship between those two variables above. In general, consistent with other graphs you have made, remember to put your independent variable on the x-axis and your dependent variable on the y-axis (as always, you can find a template for this graph in the file “Strausz ggplot2 templates” which is in the “rscripts” folder that was in the zipped filed that you should have downloaded in section 1.3. ggplot(states2025, aes(x = reaganMargin1980, y = trumpMargin2024)) + geom_point(alpha=.3)+ #if you&#39;d rather leave off the dots and just #include the labels, you can exclude this line geom_text_repel(aes(label = state), size=3)+ #if you don’t have data that you want to #use to name your dots, leave this line #off. labs(x = &quot;Reagan&#39;s margin in 1980&quot;, y = &quot;Trumps&#39;s margin in 2024&quot;)+ ggtitle(&quot;The 1980 and 2024 presidential elections in US States&quot;) The neat thing about scatterplots is that they display the values of all of the cases of the two variables that you are interested in for all of your data. So, looking at this graph, we can see dots representing all 50 states as well as Washington D.C. We are also able to label our dots. This is something that is possible when you are comparing a relatively small number of units that you are able to identify in a way that makes sense to your readers. Labeling individual dots usually doesn’t make sense with survey data, such as what we find in the ANES dataframe, because there are many thousands of individuals who took that survey, and they took it anonymously (although we might want to color code the dots for additional variables not captured on the X and Y axes). The above graph alone tells a pretty clear story about the relationship between our two variables – it is easy to visualize a positive relationship going from the District of Columbia in the bottom left to Wyoming, Idaho, and Utah in the top right. However, what if we wanted to ask R to fit a line to this data? In other words, what if we wanted R to find the line that is closest to all of the points on this graph? To do that, we can add a single line to our code from above: ggplot(states2025, aes(x = reaganMargin1980, y = trumpMargin2024)) + geom_point(alpha=.3)+ #if you&#39;d rather leave off the dots and just #include the labels, you can exclude this line geom_smooth(method=&#39;lm&#39;, formula= y~x)+ #this is the line of code we add here geom_text_repel(aes(label = state), size=3)+ #if you don’t have data that you want to #use to name your dots, leave this line #off. labs(x = &quot;Reagan&#39;s margin in 1980&quot;, y = &quot;Trumps&#39;s margin in 2024&quot;)+ ggtitle(&quot;The 1980 and 2024 presidential elections in US States&quot;) The line on this graph is called a regression line, and it is the line that is as close as possible to all of the dots on this graph. The shaded region around it is a 95% confidence interval. Visualize drawing a line from the top left corner of that region to the bottom right corner. If that line that visualize was horizontal or sloping down, then we could not rule out the possibility that there was no relationship or a negative relationship between the two variables that we are looking at there. But looking at the above graph, even that imaginary line slopes upwards, which is good evidence for our hypothesis. 8.4 Bivariate linear regression As I mentioned in the last section, the line that goes through the graph that we just generated is called a regression line. Let’s now spend a few minutes reviewing some of what we know about lines from algebra. This is the formula for a line: Y=MX+B X is our independent variable, and Y is our dependent variable. M is the slope of our line. In other words, a one unit increase in X leads to an M unit increase in Y (or, if M is negative, a one unit increase in Y leads to an M unit decrease in Y). B is the Y intercept of our line. When X is zero, B will be the value of our line. When we ask R to draw a regression line on a scatterplot, we are asking R to to find the line that most closely fits all of the points in our scatterplot, and then generate a formula for that line. To generate that formula, R needs to find the slope and the intercept. To ask R to report what slope and intercept it found, we can use this command: #here we create an object called &quot;model.1&quot; which is our regression analysis model.1&lt;-lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) #now we ask R to display the results of that analysis model.1 ## ## Call: ## lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) ## ## Coefficients: ## (Intercept) reaganMargin1980 ## -0.03344 0.72737 In this code, we must put the dependent variable first, and then the independent variable second. The “lm” stands for “linear model” (because we are asking R to generate the formula for a line). This is the most basic form of regression, an extremely powerful family of statistical techniques that are used across the social and natural sciences. This most basic form of regression is called “Ordinary Least Squared,” or OLS regression, because calculating it involves using linear algebra to find the line with the minimum squared distance between each point in a scatterplot and that line. Looking at the output, R is giving us two numbers: they are R’s estimates for the slope and the intercept in the formula Y=MX+B. Recall that our independent variable is the reaganMargin1980, and our dependent variable is trumpMargin2024. So, we can rewrite the Y=MX+B formula with those variables, like this: trumpMargin2024=M(reaganMargin1980) + B And now we can replace M with the slope (which is right under reaganMargin1980, in our output) and B with the intercept (which is right number the word “intercept” in our output): trumpMargin2024=.72737(reaganMargin1980) - .03344 This formula represents the best prediction from our regression analysis for any given state’s Trump Margin in 2024, if we were given their Reagan Margin in 1980. If we look at the scatterplot above, we can see that there are many points above that line, and many points below that line, so it would be a mistake to assume that we can predict trumpMargin2024 from reaganMargin1980 with 100% accuracy, but the line represents our best possible prediction. More specifically, the line is making two predictions. First, it is telling us that for every one unit increase in our independent variable (Reagan’s 1980 margin of victory), we predict that Trump’s margin would increase by .72737. Second, it is telling us that in a state with a reaganMargin of 0 (meaning the Reagan and Carter vote in that state was identical), the line predicts a trumpMargin of -.03344 (meaning Harris would have won that state by .03344). Now we know, looking at the second graph in section 8.3, that this line doesn’t perfectly fit the data. Some points are right on the line, like Florida, but others are far above the line, like Colorado, or far below the line, like South Carolina. In order to get a sense of how well our regression actually fits the data, we can ask R to summarize our regression, like this: summary(model.1) ## ## Call: ## lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35722 -0.15308 0.00016 0.11850 0.48517 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03344 0.03276 -1.021 0.312 ## reaganMargin1980 0.72737 0.15504 4.692 2.21e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.197 on 49 degrees of freedom ## Multiple R-squared: 0.31, Adjusted R-squared: 0.2959 ## F-statistic: 22.01 on 1 and 49 DF, p-value: 2.208e-05 There is a lot in this output, but there are a few things that you should focus on. First, look at the estimated slope and intercept. Those are the same as above, when we just did the lm() command without summary(). Second, look at the column that says “Pr(&gt;|t|).” That is the significance level of the coefficient and the intercept. In other words, that is the probability that we would see a sample like this from a population where the intercept was 0 and where the slope was 0. The significance level of the intercept is less important, but let’s think about the significance level of the slope. If the slope were 0, then whatever happened to X, Y would not change. Let’s look at the formula to remember why: trumpMargin2024=M(reaganMargin1980) + B If our slope (M) were 0, then the estimated Trump margin would be B whatever the Reagan margin was. And thus, there would be no relationship between Reagan margin and Trump margin. So, if we can say that it is unlikely that the true slope of our line is 0 (because the p-value is below the level of risk of error that we are willing to take, usually set to .05 in political science), then we can conclude that there is likely to be a relationship between our two variables. Since the p-value of our slope is \\(2.21*10^{-5}\\), we can conclude that we are pretty unlikely to get this kind of data from a population with no relationship between our independent and dependent variables. Therefore, we can conclude that this relationship is statistically significant. There is one final aspect of this output that we should discuss: the \\(R^2\\) and adjusted \\(R^2\\) values. \\(R^2\\) is a Proportional Reduction in Error (PRE) measure of the strength of a regression relationship. It ranges from 0 to 1, and it tells us the proportion of variation in our dependent variable that is explained by our independent variable. Because \\(R^2\\) values are often a little bit inflated for some statistical reasons that we won’t get into in this chapter, it is generally better to use the adjusted \\(R^2\\) to interpret regression results. So, in this case, an adjusted \\(R^2\\) of .2959 tells us that variation in Reagan’s 1980 margin of victory can account for about 29.59% of variation in the Trump’s 2024 margin of victory. 8.5 Residuals Let’s take another look at the scatterplot of the relationship between the 1980 Reagan margin and the 2024 Trump margin: If you look at the points in relationship to the regression line, there are some points that seem to be right on the line, such as Rhode Island, some that are pretty significantly above the line, such as West Virginia, and some that are pretty significantly below the line, such as Vermont. Being above the line means that that state’s value on the dependent variable is larger than we would have expected, given the value on the independent variable, and being below the line means that the dependent variable is smaller than expected, given the dependent variable. So, in other words, West Virgina voted for Trump by a bigger margin than we would have predicted given its margin for Reagan (it actually voted against Reagan, and for Carter), and Vermont voted for Trump by a smaller margin than we would have predicted given its margin for Reagan (Vermont voted the opposite way from West Virginia in both elections, voting for Reagan in 1980 and Harris in 2024). The difference between the observed and predicted value for a given point in a regression analysis is known as the residual. When the residual is positive, that means that the observed value (the value of the dependent variable that we actually see in our data), is larger than the expected value (the value of the dependent variable that is actually on the regression line). R makes it very easy to calculate the residuals for all of our points. We can do that with the following code. This code will generate a version of our states2025 dataframe called model.1.data that has two new variables: pred: the predicted values of the the dependent variable for each value of the independent variable (remember that the independent variable is reaganMargin1980 and the dependent variable is trumpMargin2024). resid: the residual for each of our cases (the observed value of the dependent variable minus the “expected value” – the point on our regression line). model.1.data&lt;-states2025 %&gt;% add_predictions(model.1) %&gt;% add_residuals(model.1) To check R’s math, and help you understand the concept of residuals, we can ask R to calculate the residuals by ourselves using the expected values. First, we generate a new variable called resid.check which is just the observed value of the DV for each case minus the predicted value (the point on the line): model.1.data$resid.check&lt;-model.1.data$trumpMargin2024-model.1.data$pred Second, we can use the “identical()” command to confirm that our new resid.check variable is the same as the “resid” variable we generated above. identical(model.1.data$resid.check,model.1.data$resid) ## [1] TRUE The “TRUE” that we see after we ran the identical() command means that the residual that we generated by subtracting the observed value of our DV from the expected value of our DV is the same as the residual that we generated above with the add_residuals() command. In other words, the add_residuals() command worked. One thing that the residuals are useful for is identifying cases that our regression equation explains well and less well. To identify find both kinds of cases, we first want to calculate the absolute value of the residuals. This is because residuals are positive when points are above the regression line and negative when points are below the regression line, but we just want to see how far each point is from the regression line (whether above or below). Here is how we use the new command abs() to calculate the absolute value of our resid variable: model.1.data$abs.resid&lt;-abs(model.1.data$resid) That command generates a new variable called abs.resid which is the absolute value of our residual. Now we can use the technique from section 3.4 to find the states that our model does the least good job of predicting. This time we will add the command head() to the end of our sequence so that R will only show the first few cases, rather than all 50 states: model.1.data %&gt;% dplyr::select(state,abs.resid) %&gt;% arrange(desc(abs.resid)) %&gt;% head() ## # A tibble: 6 × 2 ## state abs.resid ## &lt;chr&gt; &lt;dbl&gt; ## 1 West Virginia 0.485 ## 2 District of Columbia 0.357 ## 3 Arkansas 0.335 ## 4 Alabama 0.329 ## 5 Tennessee 0.328 ## 6 Kentucky 0.328 Take a look at this output. It shows us the six states and territories in which the 1980 presidential election results did the worst job in predicting the 2024 results. Interestingly, these are all in the South (Washington DC is also in the South). Now let’s use the same technique to look at the states that our model does the best job in predicting: model.1.data %&gt;% dplyr::select(state,abs.resid) %&gt;% arrange(abs.resid)%&gt;% head() ## # A tibble: 6 × 2 ## state abs.resid ## &lt;chr&gt; &lt;dbl&gt; ## 1 Michigan 0.000162 ## 2 Pennsylvania 0.00120 ## 3 Wisconsin 0.00825 ## 4 Kansas 0.0155 ## 5 Minnesota 0.0198 ## 6 Rhode Island 0.0282 Look at these results. Michigan is the state in which the 1980 presidential election results does the best job in predicting the 2024 election, followed by Pennsylvania, Wisconsin, Kansas, Minnesota, and Rhode Island. 8.6 Review of this chapter’s commands Command Purpose Library cor.test() Generates a Pearson’s R statistic for the relationship between two variables. Variables can be listed in any order and should be written dataframe$variable. Base R lm(formula = DV ~ IV, data = DATAFRAME) Generates the slope and intercept for a regression line. Replace DV with the dependent variable you are interested in, the IV with the independent variable, and DATAFRAME with the dataframe that you are using. Base R summary(lm(…)) Generates much more data about a regression relationship than the above command. Replace the “…” with the IV, DV, and DATAFRAME the same way that you would with the above command. Base R add_predictions Adds predictions from regression analysis to a dataset. Must save the regression as an object and put that object in the parenthesis. Follows the dataset after a pipe modelr add_residuals Adds residuals from regression analysis to a dataset. Must save the regression as an object and put that object in the parenthesis. Follows the dataset after a pipe modelr identical() Indicates whether two vectors are identical. Put the vectors in the parenthesis, sepeated by a comma Base R abs() Calculates the absolute value of a scalar or vector Base R "],["regression.html", "Chapter 9 Multiple regression 9.1 Getting started with this chapter 9.2 Multiple regression 9.3 Residuals in multiple regression 9.4 Stargazer output 9.5 Dummy variables in multiple regression 9.6 Ordinal variables in regression analysis 9.7 Graphing multiple regression 9.8 Review of this chapter’s commands", " Chapter 9 Multiple regression 9.1 Getting started with this chapter To get started in today’s chapter, open the project that you made in lab 1. If you forgot how to do this, see the instructions in section 2.2. Now type install.packages(\"stargazer\") into the Console.8. This new package will help us label our graphs in a way that is easier to read. Next, open a new script file and save it in your scripts folder as “chapter 9 practice.” Write this on the page: #################################### # Your name # 20093 Chapter 9, Practice exercises # Date started : Date last modified #################################### #libraries------------------------------------------ library(tidyverse) library(ggrepel) library(modelr) library(stargazer) Now select all the text on this page, run it, and then save it. 9.2 Multiple regression In the last chapter, we learned how to do bivariate ordinary least squared (OLS) regression. This is an extremely powerful tool for statistical inference because it both provides a proportional reduction in error estimate (\\(R^2\\)), which tells us exactly how much of the variation in our dependent variable is explained by variation in our independent variable, and it provides us with a coefficient estimate, which tells us exactly what we expect to happen to our dependent variable with every one unit increase in our independent variable. But there is one more reason that regression is a tool beloved by scientists. It provides an easy way to control for many different independent variables at the same time. Recall that in Chapter 8 we discussed how when R is running a regression it is estimating the values of M and B in the formula Y=MX+B. Another way that we can write that same formula is like this: \\[Y=β_0+ β_1X_1\\] In this version of the formula, we call the intercept \\(β_0\\) (β is the Greek letter Beta) instead of B, and we call the slope \\(β_1\\) instead of M. When conducting bivariate regression, we are asking R to give us the values of \\(β_0\\) (the intercept, or B) and \\(β_1\\) (the slope, or M) that best fits our data. When we want to conduct multiple regression, we can use the same principle, but we just have to add more independent variables and βs, like this: \\[Y=β_0+ β_1X_1 + β_2X_2 + β_3X_3+…+ β_nX_n\\] Multiple regression results include an estimate for all of those βs. The estimates are called “partial regression coefficients,” which is generally shortened to “coefficients.” And they are very powerful! In the above equation, R’s estimated value for \\(β_1\\) tells us what we expect to happen to our dependent variable, Y, when we increase \\(X_1\\) by one unit, and when we hold \\(X_2\\), \\(X_3\\), and all other independent variables in our equation constant. Thus, if someone were to say “Hey, I think that the relationship that you observe between \\(X_1\\) and Y is spurious; variation in \\(X_2\\), which happens to be correlated with \\(X_1\\), is the actual cause of variation in Y!” You can reply, “in this multiple regression, I controlled for \\(X_2\\) (in other words, I looked at the relationship between \\(X_1\\) and Y at similar values of \\(X_2\\)) and I still found a meaningful relationship between \\(X_1\\) and Y. Thus, I can rule out spuriousness!” Let’s try an example, returning to our regression analysis from Chapter 8, where we looked at reaganMargin1980 as our dependent variable trumpMargin2024 as our dependent variable. To test this analysis, we ran the following regression: model1&lt;-lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) #now we ask R to display the results of that analysis model1 ## ## Call: ## lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) ## ## Coefficients: ## (Intercept) reaganMargin1980 ## -0.03344 0.72737 This output helped us write the equation for the relationship between reaganMargin1980 and trumpMargin2024: trumpMargin2024 = -.03344 + -0.72737 (reaganMargin1980) So in other words, R is estimating that a in state where Reagan and Carter in tied in 1980 (and thus Reagan’s margin was 0), Trump would lose by a proportion of .03344 (or about 3.3%), and that, if a state’s reaganMargin increased by 1 (moving from a score of 0, where Reagan and Carter tied, to a 1, which means that Reagan won 100% of the vote), Trump’s margin of victory would increase by .72737, or about 72.7%. To determine whether these estimates are statistically significant, we summarized our regression model, like this: summary(model1) ## ## Call: ## lm(formula = trumpMargin2024 ~ reaganMargin1980, data = states2025) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35722 -0.15308 0.00016 0.11850 0.48517 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03344 0.03276 -1.021 0.312 ## reaganMargin1980 0.72737 0.15504 4.692 2.21e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.197 on 49 degrees of freedom ## Multiple R-squared: 0.31, Adjusted R-squared: 0.2959 ## F-statistic: 22.01 on 1 and 49 DF, p-value: 2.208e-05 This output shows us that the estimates of the slope and intercept are both statistically significant, and that the adjusted \\(R_2\\) is .2959, which means that variation Reagan’s margin in 1980 explain about 29.59% of variation in Trump’s margin in 2024. That is pretty good! We can learn a lot about how states voted in 2024 by observing how they voted in 1980. However, we also know that there is still a lot of variation that is unexplained. Thinking back to our analysis of residuals in section 8.5, we know that, while our regression analysis does a pretty good job of explaining Michigan, it does a much less good job of explaining West Virginia. Moreover, what if there is a variable that is correlated with reaganMargin1980 but which is the true cause of trumpMargin2024? If that were the case, and if we add that new independent variable to our regression analysis, then the relationship between reaganMargin1980 and trumpMargin2024 would no longer be statistically significant, and we would say that the relationship between reaganMargin and trumpMargin that we had observed is spurious. One additional variable which might help explain Trump’s margin of victory in 2024 is the percentage of a state’s population that is evangelical, since evangelical Christians are an increasingly important constituency of the Republican party. When we control for percent evangelical, does reaganMargin1980 still help explain trumpMargin2024? To help us answer this question, we need to have R estimate \\(β_1\\) and \\(β_2\\) in this formula: trumpMargin2024=\\(β_0\\) + \\(β_1\\)reaganMargin1980 + \\(β_2\\)evanPerc2020 We can do that with this command: model2&lt;-lm(formula = trumpMargin2024 ~ reaganMargin1980+evanPerc2020, data = states2025) summary(model2) ## ## Call: ## lm(formula = trumpMargin2024 ~ reaganMargin1980 + evanPerc2020, ## data = states2025) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.29391 -0.07966 0.00576 0.07360 0.50405 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.274902 0.041992 -6.546 3.65e-08 *** ## reaganMargin1980 0.876417 0.112853 7.766 4.98e-10 *** ## evanPerc2020 0.013624 0.001967 6.926 9.55e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1408 on 48 degrees of freedom ## Multiple R-squared: 0.6549, Adjusted R-squared: 0.6405 ## F-statistic: 45.54 on 2 and 48 DF, p-value: 8.155e-12 Looking at the Pr(&gt;|t|) column in this output, we can see that the intercept and both coefficient estimates are statistically significant, because the numbers are much, much lower than .05. The value of adjusted \\(R_2\\) is .6405, which means that variation in reaganMargin and percent evangelical explains about 64% of variation in trumpMargin2024. trumpMargin2024 = -.2749 + .8764reaganMargin1980 + .0136evanPerc2020 This equation is telling us that, based on our data, R predicts that in a state where Reagan and Carter tied in 1980 (creating a reaganMargin1980 of 0),with 0 evangelical Christians, Trump would have lost to Harris by .2749, or 27.49%. However, holding the percent of evangelicals in that state constant, if we increase Reagan’s margin of victory in 1980 from 0 to 1 (or from 0% to 100%), then we expect Trump’s margin of victory in 2024 to increase by .8764, or 87.64%. Moreover, holding Reagan’s margin in 1980 constant, an 1% increase of the percentage of evangelical Christians in a state is associated with a .0136 increase in trumpMargin2024 (or a 1.36% increase in Trump’s margin of victory). In other words we know that the partial relationship between Reagan margin and Trump margin, controlling for evangelical population, is not spurious. 9.3 Residuals in multiple regression We can also apply the same principles that we used in section 8.5 to see which of our cases are really well explained by our model (with residuals that are close to 0) and which are less well explained by our model: model2.data&lt;-states2025 %&gt;% add_predictions(model2) %&gt;% add_residuals(model2) model2.data$abs.resid&lt;-abs(model2.data$resid) Now let’s look at the states with the residuals that are the farthest from zero: model2.data %&gt;% dplyr::select(state,abs.resid) %&gt;% arrange(desc(abs.resid)) %&gt;% head() ## # A tibble: 6 × 2 ## state abs.resid ## &lt;chr&gt; &lt;dbl&gt; ## 1 West Virginia 0.504 ## 2 Washington 0.294 ## 3 Wyoming 0.276 ## 4 District of Columbia 0.232 ## 5 Colorado 0.205 ## 6 California 0.196 Just like the bivariate regression, West Virginia remains the state with the residual that is the farthest from 0 (the state that our regression has the most trouble explaining). DC also remains in our top six, but other than those two, we can see that, with our multiple regression the other four states with the residuals farthest from zero are not in the South (Washington, Wyoming, Colorado, and California). This might suggest that the percent evangelical was important driver of Trump’s margin of victory in the South in 2024, and that was not captured by our first regression. If you run the same command in without desc(), you will see that Michigan, which was the case that our bivariate regression explained the best, is no longer in the top six best explained cases of our multivariate regression.9 9.4 Stargazer output The output that we generated in the last section is pretty difficult to read, and would not look good in a paper. But, we have a nice solution – the stargazer package that we installed above! Once we have installed and loaded that package, we can invoke it like this: stargazer(model1, model2, type=&quot;html&quot;, out=&quot;lab9regressions.html&quot;, column.labels = c(&quot;Model 1&quot;, &quot;Model 2&quot;), single.row=FALSE, notes.append = FALSE, header=FALSE) This is telling R to make a table with regression results from both of our regressions and to save it as a file called lab9regressions.html. If you want to change the name, you can just change what you write in the quotes after “out=”. Just don’t type any periods before .html, and keep the .html at the end. If you have more than two models that you want to run, you can just add them, but you have to add as many labels as you have models after column.labels. When you run this command, a bunch of gibberish will pop up in your consol. Don’t worry about that. Just look in the main folder where you have your Scope and Methods labs project stored. There, you should see an html file called “lab9regressions.html” (unless you changed the name to something else). Open that file – it should open in your web browser. Highlight all of the text, copy it (using Ctrl-C on a Windows machine or Command-C on a Mac), and paste it into a word processor document. You should see something like this: Dependent variable: trumpMargin2024 Model 1 Model 2 (1) (2) reaganMargin1980 0.727*** 0.876*** (0.155) (0.113) evanPerc2020 0.014*** (0.002) Constant -0.033 -0.275*** (0.033) (0.042) Observations 51 51 R2 0.310 0.655 Adjusted R2 0.296 0.640 Residual Std. Error 0.197 (df = 49) 0.141 (df = 48) F Statistic 22.011*** (df = 1; 49) 45.540*** (df = 2; 48) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 This is an extremely useful table, very much like what you see when you read many academic journal articles that use regression analysis. At the top, you see a title that identifies the dependent variable for all regressions in the table – trumpMargin2024. The column on the left provides the variable names. Once you have pasted that text into your word processor, You should go in and change variable names and the title to something that will be easy for your readers to understand. So, for example, instead of evanPerc2020, you might write “Percent evangelical, 2020” and instead of reaganMargin1980, you might write “Reagan margin, 1980”. The second column is the results of the first regression that we ran, with only reaganMargin1980 as the independent variable (you can tell that the regression did not include the age variable because the spot where the estimated coefficient for evanPerc2020 would go is blank). The “constant” is the intercept (they are the same thing). The third column is the results of the second regression analysis. If you look at the Adjusted \\(R^2\\) row, you can see how the Adjusted \\(R^2\\) changes as you add and subtract variables to the regression equation. When Adjusted \\(R^2\\) decreases as you add more independent variables, that means that whatever increase in explanatory power that you get from the new variables is more than offset by the loss in degrees of freedom that you take by adding more variables. 9.5 Dummy variables in multiple regression For the next set of examples, we are going to use the variable ft_sccs in ANES as our dependent variable. This variable is a feeling thermometer on the the US Supreme Court, which ranges from 0 (most negative feelings) and 100 (most positive feelings). We often want to include nominal variables in regression analysis. For example, we want to know whether things like gender, religion, and race might cause variation in our dependent variable. However, since the values in a nominal variable can be listed in any order, it does not make sense to talk about “increasing” the value of a nominal independent variable. We get around this by converting a nominal independent variable into a series of dummy variables (variables coded 0, for when an attribute is absent, and 1, for when it is present). So, for example, the houseKnowledge variable in anes2024 is coded so that respondents that correctly identified the Republicans as the party that controlled the House of Representatives got a 1, and respondents that gave an incorrect answer got a 0. If we want to use that as an independent variable for regression analysis, we are asking R to estimate \\(β_0\\) and \\(β_1\\) in this equation: ft_ussc=\\(β_0\\)+ \\(β_1\\)houseKnowledge How would we interpret the results of this regression? For people wrong about the party that controls the House, the predicted value of ft_ussc would be \\(β_0\\) (because \\(β_1\\) multiplied by 0 equals 0). For people who know which party controls the House, the estimated value of ft_ussc would be \\(β_0\\) + \\(β_1\\) (because \\(β_1\\) multiplied by 1 equals \\(β_1\\)). Let’s run this regression using this command: model3&lt;-lm(ft_ussc~houseKnowledge, data=anes2024) summary(model3) ## ## Call: ## lm(formula = ft_ussc ~ houseKnowledge, data = anes2024) ## ## Residuals: ## Min 1Q Median 3Q Max ## -50.240 -17.953 2.047 19.760 52.047 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.2400 0.6675 75.265 &lt; 2e-16 *** ## houseKnowledge -2.2873 0.8244 -2.774 0.00555 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.52 on 4582 degrees of freedom ## (937 observations deleted due to missingness) ## Multiple R-squared: 0.001677, Adjusted R-squared: 0.001459 ## F-statistic: 7.697 on 1 and 4582 DF, p-value: 0.005553 This output tells us a few important things. First, it helps us fill out the equation from above, like this: ft_ussc=50.24 + -2.29houseKnowledge In other words, R is estimating that the average person who does not know which party controls the House gives the Supreme Court a 50.24, and that the average person who does know which party controls the House gives the Supreme court a 50.24-2.29, which is 47.95. And these estimates are both statistically significant. Second, the adjusted \\(R^2\\) value of this equation tells us that variation in House knowledge alone only explains about 0.146% of variation in feelings on the Supreme Court. Thus, there are likely a lot of major causes that are missing from this model. An important note: Let’s take a minute to generate a dummy variable for those who gave the wrong answer to House question,like this: anes2024&lt;-anes2024 %&gt;% mutate(nonKnowledgeable=recode(houseKnowledge,&#39;1&#39;=0,&#39;0&#39;=1)) This variable is the inverse of our houseKnowledge variable. While the houseKnowledge variable is coded 1 for those who gave the correct answer to the question which party controls the House, the nonKnowledgeable variable is coded 1 for respondents that gave the incorrect answer. If we include that variable in the regression model above, to estimate this equation: ft_ussc=\\(β_0\\)+ \\(β_1\\)houseKnowledge+\\(β_2\\)nonKnowledgeable we are giving R an impossible task. That it because no case will have a value of 0 for both our houseKnowledge and nonKnowledgable variables. If we ignore this impossibility, and force R to do it anyway, R does this: lm(ft_ussc~houseKnowledge+nonKnowledgeable, data=anes2024) ## ## Call: ## lm(formula = ft_ussc ~ houseKnowledge + nonKnowledgeable, data = anes2024) ## ## Coefficients: ## (Intercept) houseKnowledge nonKnowledgeable ## 50.240 -2.287 NA R essentially says “no thank you.” You will see that the coefficient for nonKnowledgeable is NA. That is because R noticed that the houseKnowledge and nonKnowledgeable variables are redundant with one another and left the nonKnowledgeable variable out. Aside from that coefficient, the rest of the output is the same as when we ran the regression with only the houseKnowledge variable. The houseKnowledge variable is a nominal variable with only two values: either each respondent either answered correctly or didn’t. But it is still important to avoid including redundant variables in regression even when we are dealing with nominal independent variables with more than two values, such as race. If we enter table(anes2024$race), we can see that the race variable in ANES has six values: Asian or Pacific Islander, Black, Hispanic, Multiple races, Native American, and White. Let’s see what happens when we include that variable in a regression analysis with ft_ussc as our dependent variable, like this: model4&lt;-lm(ft_ussc~race, data=anes2024) summary(model4) ## ## Call: ## lm(formula = ft_ussc ~ race, data = anes2024) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.125 -19.639 0.361 20.361 56.217 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 52.125 2.043 25.512 &lt; 2e-16 *** ## raceBlack -8.342 2.411 -3.460 0.000545 *** ## raceHispanic -4.833 2.364 -2.044 0.040980 * ## raceMultiple races -4.096 2.873 -1.426 0.153976 ## raceNative American -7.085 5.677 -1.248 0.212077 ## raceWhite -2.486 2.091 -1.189 0.234511 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.48 on 4814 degrees of freedom ## (701 observations deleted due to missingness) ## Multiple R-squared: 0.004912, Adjusted R-squared: 0.003878 ## F-statistic: 4.752 on 5 and 4814 DF, p-value: 0.0002467 What R has done here is converted our race variable into six dummy variables: one for each value in the original race variable, and estimated the intercept and coefficient for this equation: ft_ussc = \\(β_0\\) + \\(β_1\\)Black + \\(β_2\\)Hispanic + \\(β_3\\)MultipleRaces + \\(β_4\\)NativeAmerican + \\(β_5\\)White Notice that our original race variable had six categories, but there are only five dummy variables in the model. That is because if R included the sixth value—Asian or Pacific Islander—then R would have put itself in a position where it was trying to estimate ft_ussc for a person would cannot exist in our data – someone with 0s on all 0 possible values for race. Thus, R chooses one category to be the “reference category,” and leaves that one out. In the case of the race variable in ANES, R chose to leave out the first category in alphabetical order, which is “Asian or Pacific Islander.” Thus, the intercept that R estimated in the above regression (52.13) is the predicted value of feelings about the Supreme Court for someone who identified themselves as Asian or Pacific Islander (because their values on all of the other possible race dummy variables are 0). -8.342 is the expected differences between someone who is an Asian or Pacific Islander and someone who is Black; in other words, Blacks seem to have a lower estimation of the Supreme Court, on average, than Asians or Pacific Islanders. And the rest of the coefficients estimate the differences between the groups that they identify and Asian Pacific Islanders. R chose “Asian or Pacific Islander” as the reference category because it comes first in alphabetical order when looking at the values for the race variable. This would be useful analysis if we were writing a paper about the public opinion of Asians and Pacific Islanders. However, given that whites are the majority in our sample, we might want to compare the various ethnic minority groups in this sample with whites. To do that, we would have to first make the race variable into a factor,10 like this: anes2024$race.factor&lt;-as.factor(anes2024$race) Next, we have to relevel the variable, telling R what we want the reference category to be, like this: anes2024$race.factor&lt;-relevel(anes2024$race.factor, ref=&quot;White&quot;) This will compare all the racial groups in this variable to whites. So, now, let’s run a regression with ft_ussc as our dependent variable and race.factor as our independent variable, like this: model5&lt;-lm(ft_ussc~race.factor, data=anes2024) summary(model5) ## ## Call: ## lm(formula = ft_ussc ~ race.factor, data = anes2024) ## ## Residuals: ## Min 1Q Median 3Q Max ## -52.125 -19.639 0.361 20.361 56.217 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.6386 0.4457 111.381 &lt; 2e-16 *** ## race.factorAsian or Pacific Islander 2.4864 2.0912 1.189 0.2345 ## race.factorBlack -5.8559 1.3554 -4.320 1.59e-05 *** ## race.factorHispanic -2.3463 1.2699 -1.848 0.0647 . ## race.factorMultiple races -1.6096 2.0679 -0.778 0.4364 ## race.factorNative American -4.5986 5.3152 -0.865 0.3870 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.48 on 4814 degrees of freedom ## (701 observations deleted due to missingness) ## Multiple R-squared: 0.004912, Adjusted R-squared: 0.003878 ## F-statistic: 4.752 on 5 and 4814 DF, p-value: 0.0002467 Now R is using the dummy variable for “white” as our reference category. So, interpreting this, we can see that R predicts that the average white person will give the Supreme Court a 49.64, the average Black person’s score will be 5.86 points lower than that, the average Hispanic person’s score will be 2.35 points lower than the average white person, etc. Interestingly, we can also see that Blacks are the only racial group that differs from Whites in their assessment of the Supreme Court in a statistically significant way. 9.6 Ordinal variables in regression analysis If we put an ordinal variable into our regression analysis as an ordered factor (the way that R likes to treat ordinal variables), R will treat the possible values as a series of dummy variables, leave one out, and produce in the output in a way that is pretty confusing.11 One way around this is to convert an ordinal variable into a numeric variable, like we did in section 4.4. Let’s try this with the outrage variable, testing the hypothesis that people that are more prone to outrage are also more likely to be outraged about the US Supreme Court, and thus produce a lower ft_ussc. First, we have to check the way that the values are ordered, using this command: levels(anes2024$outrage). If you enter that into your console, you will see that the variable is coded so that the lowest value is “not at all” outraged, and the highest value is “extremely” outraged. So when we convert this to numeric, it will still make sense to keep the values in this order. Now let’s convert the variable to numeric and rename it: anes2024$outrage2&lt;-as.numeric(anes2024$outrage) Now, if you run table(anes2024$outrage2), you will see that our new variable is scaled from 1 to 5. However, it will be a little easier for us if we scale this variable from 0 to 4. We can accomplish that with this command: anes2024$outrage2&lt;-anes2024$outrage2-1 Now let’s try our regression analysis with our new variable: model6&lt;-lm(ft_ussc~outrage2, data=anes2024) summary(model6) ## ## Call: ## lm(formula = ft_ussc ~ outrage2, data = anes2024) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.17 -18.36 1.64 19.24 56.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.5532 0.7590 57.385 &lt; 2e-16 *** ## outrage2 2.4032 0.2953 8.137 5.14e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.32 on 4658 degrees of freedom ## (861 observations deleted due to missingness) ## Multiple R-squared: 0.01402, Adjusted R-squared: 0.0138 ## F-statistic: 66.21 on 1 and 4658 DF, p-value: 5.142e-16 Interestingly, these results show that, on average, someone with an outrage score of 0 (meaning they were not at all outraged) gave the Supreme Court a 43.55, but a one point increase in the outrage scale is associated with a 2.4 point increase in positive feelings toward the Supreme Court. Perhaps outraged people trust the Supreme Court to address their sources of outrage? 9.7 Graphing multiple regression When you ask R to calculate a bivariate OLS regression, you are actually asking R to estimate the formula for a line. This is relatively easy to represent on a two-dimensional scatterplot. You can have R plot all of the points that it is using to estimate the line, as well as the line itself (like we did in Chapter 8). When you add a second independent variable, you are asking R to estimate the formula for a plane, which becomes harder to graph, and when you add a third, you are asking R to estimate the formula for a three-dimensional object, which is even harder to graph. Adding four or more independent variable makes it impossible to graph the whole relationship about which you are asking R to theorize. However, there are still ways to represent part of the relationship. For example, let’s say that we have asked R to run this regression: summary(lm(ft_ussc~ft_liberals+race.factor, data=anes2024)) When we do this, we are asking R to estimate the intercept and coefficients in this equation: ft_ussc=\\(β_0\\)+ \\(β_1\\)ft_liberals + \\(β_2\\)AsianPacificIslander + \\(β_3\\)Black + \\(β_4\\)Hispanic + \\(β_5\\)MultipleRaces + \\(β_6\\)NativeAmerican with “White” as the reference category for race. One way that we can graphically represent this relationship is by using facets. In other words, we can ask R to make a different scatterplot for every value of our nominal race variable, using the following code. ggplot(data=anes2024 %&gt;% filter(!is.na(race.factor)), aes(x=ft_liberals, y=ft_ussc)) + geom_point(size=2, alpha=.5, color=&quot;purple&quot;) + theme_bw()+ #add axis labels labs(title = &quot;Feelings about liberals &amp; Supreme Court&quot;, subtitle=&quot;ANES 2024&quot;, x=&quot;Feelings about liberals&quot;, y=&quot;Feeling about Supreme Court&quot;, color=NULL)+ #add regression line geom_smooth(method=&#39;lm&#39;, color=&quot;BLACK&quot;)+ #make different graphs for every value of a nominal or ordinal variable facet_wrap(~race.factor) This graph shows us the relationship between feelings about liberals and feelings about the US Supreme court at all values of our race variable. Notice how much more substantially negative the relationship is among whites than it is among other racial groups. 9.8 Review of this chapter’s commands Command Purpose Library stargazer() Use this code to generate a beautiful table displaying the results of one or more regressions. stargazer as.factor() Changes a character variable to a factor. Necessary when you want to set a different reference category when including a nominal variable in regression analysis. Base R relevel(, ref=““) Changes the reference category of a factor. Necessary when you want to set a different reference category when including a nominal variable in regression analysis. Base R References Hlavac, Marek. 2022. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Social Policy Institute. https://CRAN.R-project.org/package=stargazer. Stargazer is designed by Hlavac (2022)↩︎ To test this, you can use this command: model2.data %&gt;% dplyr::select(state,abs.resid) %&gt;% arrange(abs.resid) %&gt;% head()↩︎ Because race is a nominal variable, we are making it into a non-ordered factor. If it were an ordinal variable, we would want to make it an ordered factor.↩︎ if you don’t believe me, enter lm(ft_ussc~outrage,data=anes2024) into the Console.↩︎ "],["wild.html", "Chapter 10 R in the wild 10.1 Today’s lab as a reference 10.2 Importing data into R 10.3 Cleaning data in R 10.4 Online R resources 10.5 The last command: how to get help 10.6 The Strausz method for improving in R", " Chapter 10 R in the wild 10.1 Today’s lab as a reference Unlike the previous chapters, I am not going to ask you to start a practice document and follow along. Just read the chapter and know that this a resource that you have available as you become more advanced in your R skills and need more advanced guidance. 10.2 Importing data into R 10.2.1 Importing CSV files For this workbook, you have been able to access our three dataframes—world2025, states2025, and ane2024—as .Rda files. In other words, these files were already formatted for use with R. However, if you keep using R, you will regularly run into data that has not been formatted for use with R. For example, when I wanted to generate the ANES dataframe, I went to the ANES website: https://electionstudies.org/data-center/2024-time-series-study/. I had to create a (free) log in to download data, and once I did that, I was taken to a page that gave me a few different choices of formats to download the data in, and I chose to download the csv version, which is often a good choice. “CSV” stands for comma separated values, and it is a simple form of data storage that many different software packages can understand. I next created a folder on my computer called “CreatingANES2024.”12 I then moved the cvs file that I had downloaded (which downloaded as anes_timeseries_2024_csv_20250430.cvs) into that folder. After that, I made a new R project which I set to be located in that CreatingANES2024 folder. Next, I imported the data with this command: anes2024&lt;-read.csv(&quot;data/anes_timeseries_2024_csv_20250430.csv&quot;) That dataset is huge: 1,289 variables! I read over the codebook (which I had also downloaded from the same webpage) and chose the variables that I wanted to keep for this class. I used this command to make a simplified version of that dataset (you need to have the tidyverse package loaded to use this command). anes2024&lt;-anes2024 %&gt;% select(V241004, V241106x, V241108, V241118, V241119, V241120, V241177, V241211, V241229, V241230, V241234, V241245, V241248, V241258, V241287x, V241290x, V241308x, V241319x, V241324, V241325, V241327, V241338x, V241366x, V241385x, V241400x, V241403x, V241421, V241422, V241440, V241458x, V241459, V241465x, V241470, V241497, V241501x, V241521, V241525, V241528, V241537, V241550, V241552, V241553, V241567x, V241569, V241583, V241601r, V241612, V241614, V242025, V242096x, V242095x, V242125, V242126, V242134, V242135, V242136, V242137, V242138, V242139, V242140, V242141, V242142, V242143, V242144, V242145, V242146, V242147, V242148, V242149, V242150, V242151, V242152, V242153, V242154, V242155, V242156, V242266x, V242311, V242321, V242325, V242420, V242422, V242423, V241227x) That command looks like a lot, but all it is doing is telling R to write over the anes2024 dataset with a new version of the anes2024 dataset with fewer variables (instead of the 1,289 variables in the original dataframe). 10.2.2 Importing Excel files Sometimes you will find an Excel file that you want to import. There is a very useful package that you can install to help with that called “readxl.” Once you have installed it, you can use the “read_excel” command to bring Excel files into R. I build a previous version of the world2025 dataset in the following way. After I downloaded the Excel version of the “DEMOCRACY CROSS-NATIONAL DATA, RELEASE 4.0 FALL 2015” from Dr. Pippa Norris’s webpage, https://www.pippanorris.com/data. I read it into R with this command: library(readxl) world &lt;- read_excel(&quot;data/Democracy Cross-National Data V4.1 09092015.xlsx&quot;) 10.2.3 Importing other files When dealing with data that is not formatted as a Rda, Excel, or CSV file, the package “foreign” is often helpful. Here is an example of how I have used the read.spss command from the package “foreign” command to read in an SPSS-generated .sav file: df &lt;- read.spss(&quot;data/previous semester data.sav&quot;, use.value.label=TRUE, to.data.frame=TRUE) 10.3 Cleaning data in R One you have imported data, there are a number of things that you want to do to do before you do analysis. It always helps to glimpse() your new dataframe and look at the codebook, if you can find one. When I was first looking at the ANES data for this lab workbook, I noticed that many of the variables had long names that were difficult to interpret, like this: V201005. I renamed them with this tidyverse command (note that the real command was longer because it included all the variables in the dataframe): anes2024&lt;-anes2024 %&gt;% rename(attention=V241004, v2020=V241106x, v2016=V241108, hope=V241118, fear=V241119, outrage=V241120, ideology=V241177) You might find that are ordinal or nominal variables that are coded as numeric which you want to recode as factors, using the as.factor command, which you can do like this: df$variable&lt;-as.factor(df.variable) You can also use the codebook and/or the table() command to figure out how data is coded. Often missing data is coded as 8, 9, 99, or negative numbers. You want to make sure to recode that as NAs or R will treat those values as numbers and mess up your analysis. I sometimes used the tidyverse command mutate(na_if()) to do that. Here is how I accomplished that with the age variable in ANES: anes2024&lt;-anes2024 %&gt;% mutate(age=na_if(age, -2)) Sometimes I find it is easier to use base R to create NAs and to change numbers. This is how I created the “female” variable in ANES (which was originally set up to make women 2 and men 1): anes2024$female[anes2024$female&lt;0]&lt;-NA anes2024$female[anes2024$female==1]&lt;-0 anes2024$female[anes2024$female==2]&lt;-1 10.4 Online R resources There are excellent free resources for using R online. Here are some of my favorites: https://stackoverflow.com/ This is a community of people that ask and answer questions about lots of different software packages including R. You can search all of the questions for free as a non-member, or you can join (also free) and ask and answer questions. I have figured out how to do many things in R thanks to the community at this site. https://bookdown.org/ndphillips/YaRrr/ This is a very fun, free, online introduction to R, called YaRrr! The Pirate’s Guide to R, by Nathaniel D. Phillips. Actually, Dr. Phillips claims that he discovered the book and translated it from “pirate-speak” into English. https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-1/ This is part one of a four-part series of blog posts on “Data Wrangling,” by Suzan Baert. All four parts are very helpful in learning to manage data in R. https://r-graphics.org/ This is the R Graphics Cookbook, by Winston Chang. It is an extremely useful resource when using ggplot. It shows you how to make many, many kinds of graphs, and also how to modify the graphs that you have already made. https://www.apreshill.com/project/ohsu-dataviz/ This is a really nicely done, free, asynchronous online class called ” Principles &amp; Practice of Data Visualization” taught by Alison Hill. This class teaches you about data visualization in R with ggplot. https://r4ds.had.co.nz/ This textbook, R for Data Science, by Hadley Wickham and Garrett Grolemund is a terrific resource, available for free online. https://isabella-b.com/blog/ggplot2-theme-elements-reference/ This is a really helpful guide to theme elements in ggplot2 by Isabella Benabaye. https://henrywang.nl/ggplot2-theme-elements-demonstration/ This is a nice ggplot2 theme elements demonstration by Henry Wang. http://mattwaite.github.io/sports/ This textbook, Sports Data Analysis and Visualization by Matt Waite is a great resource for using R to analyze sports. https://bookdown.org/yihui/bookdown/ I used this textbook, bookdown: Authoring Books and Technical Documents with R Markdown, by Yihui Xie, to learn how to put this workbook online. 10.5 The last command: how to get help As you continue with R, you will often find yourself in a situation where you want to know what a command is for, or the specifics of how to use it. When you are in this situation, you can type a “?” followed by that command, like this: ?table R will pull up documentation on the command. R’s documentation is sometimes hard to read (and reading it takes practice) but it can be a good place to start, before looking at the online resources in the previous section. If the command is from a package that you don’t have installed at that time, you can type two question marks, and R will look through all of the packages in its online libraries, like this: ??cut2 10.6 The Strausz method for improving in R After taking this class and working through this workbook, you may decide that you want to keep using and improving at R for the reasons that I laid out in section 1.1: because R is free, powerful, professional-grade software and skills in R will really impress potential employers (and your friends and family!). Below is the method that I used to learn R. Every day, I set a timer for 15 minutes, and spent that time doing something in R. I began with though Nathaniel D. Phillips’ Pirate’s Guide and then I worked my way through the other resources in section 10.4. I also spent that time figuring out how to use R to solve problems in my own research; I made a vow to myself that from now on, I would do all analysis for my own research in R. You can also use that 15 minutes to do anything in R. Maybe you want to use R to analyze your favorite sport. Maybe you want to use R to learn about knitting patterns, or to keep track of data regarding your other hobbies or interests. There is even a package available called sourrr that is useful when generating recipes for sourdough bread (although I have found my favorite sourdough bread recipes in these books). Maybe you want to use R to gather and analyze data relating to games that you play or debates that you have with your friends. Or, maybe you want to use R to help you with research projects for other classes. Just choose something to do in those 15 minutes, and do it. If you do that for a while, you will surprise yourself with how much R you can do. You can find all of the files that I used to generate anes2024 here.↩︎ "],["references.html", "References", " References Hlavac, Marek. 2022. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Social Policy Institute. https://CRAN.R-project.org/package=stargazer. Pollock, Philip H, and Barry C Edwards. 2019. The Essentials of Political Analysis. Book. 6th ed. Sage. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
